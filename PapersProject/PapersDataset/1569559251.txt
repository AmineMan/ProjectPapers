Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Tue May  1 09:49:53 2012
ModDate:        Tue Jun 19 12:54:39 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      495490 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569559251

The Finite Field Multi-Way Relay Channel with
Correlated Sources: Beyond Three Users
Lawrence Ong† , Roy Timo‡ , Sarah J. Johnson†
†

School of Electrical Engineering and Computer Science, The University of Newcastle, Australia
‡
Institute for Telecommunications Research, University of South Australia, Australia
Email: lawrence.ong@cantab.net, roy.timo@unisa.edu.au, sarah.johnson@newcastle.edu.au

X0

0 (relay)

Abstract—The multi-way relay channel (MWRC) models cooperative communication networks in which many users exchange
messages via a relay. In this paper, we consider the ﬁnite ﬁeld
MWRC with correlated messages. The problem is to ﬁnd all
achievable rates, deﬁned as the number of channel uses required
per reliable exchange of message tuple. For the case of three users,
we have previously established that for a special class of source
distributions, the set of all achievable rates can be found [Ong
et al., ISIT 2010]. The class is speciﬁed by an almost balanced
conditional mutual information (ABCMI) condition. In this paper,
we ﬁrst generalize the ABCMI condition to the case of more than
three users. We then show that if the sources satisfy the ABCMI
condition, then the set of all achievable rates is found and can
be attained using a separate source-channel coding architecture.

Y0
N0

X1
W1

1

X2
ˆ
(W j,1 )

W2

Y1

2

XL
ˆ
(W j,2 )

Y2
N1

I. I NTRODUCTION

WL
···

L

ˆ
(W j,L )

YL
NL

N2

Fig. 1. The ﬁnite ﬁeld MWRC in which L users (nodes 1, 2, . . . , L) exchange
correlated messages through a relay (node 0). The uplink channel is marked
with solid lines and the downlink channel with dotted lines.

This paper investigates multi-way relay channels (MWRCs)
where multiple users exchange correlated data via a relay. More
speciﬁcally, each user is to send its data to all other users. There
is no direct link among the users, and hence the users ﬁrst
transmit to a relay, which processes its received information
and transmits back to the users (refer to Fig. 1). The purpose
of this paper is to ﬁnd the set of all achievable rates, which
are deﬁned as the number of channel uses required to reliably
(in the usual Shannon sense) exchange each message tuple.
The joint source-channel coding problem in Fig. 1 includes,
as special cases, the source coding work of Wyner et al. [1]
and the channel capacity work of Ong et al. [2]. Separately,
the Shannon limits of source coding [1] (through noiseless
channel) and of channel coding [2] (with independent sources)
are well established. However, these limits have not yet been
discovered for noisy channels with correlated sources in general.
For three users, Ong et al. [3] gave sufﬁcient conditions
for reliable communication using the separate source-channel
coding paradigm. The key result of [3] was to show that these
sufﬁcient conditions are also necessary for a special class of
source distributions, hence giving the set of all achievable rates.
The class was characterized by an almost balanced conditional
mutual information (ABCMI) condition. This paper extends
the ABCMI concept to more than three users, and shows that
if the sources have ABCMI, then the set of all achievable rates
can be found. While the ABCMI condition for the three-user
case is expressed in terms of the standard Shannon information
measure, we will use I-Measure [4, Ch. 3] for more then three
users—using Shannon’s measure is possible but the expressions

would be much more complicated.
Though the ABCMI condition limits the class of sources for
which the set of all achievable rates is found, this paper provides
the following insights: (i) As achievability is derived based on
a separate source-channel coding architecture, we show that
source-channel separation is optimal for ﬁnite ﬁeld MWRC
with sources having ABCMI. (ii) Since ABCMI are constraints
on the sources, the results in this paper are potentially useful for
other channel models (not restricted to the ﬁnite ﬁeld model).
(iii) This paper highlights the usefulness of the I-Measure as a
complement to the standard Shannon information measure.
II. M AIN R ESULTS
A. The Network Model
1) Sources: Consider m independent and identically
distributed (i.i.d.) drawings of a tuple of correlated
discrete ﬁnite random variables (W1 , W2 , . . . , WL ), i.e.,
{(W1 [t], W2 [t], . . . , WL [t])}m . The message of user i is given
t=1
by W i = (Wi [1], Wi [2], . . . , Wi [m]).
2) Channel: Each channel use of the ﬁnite ﬁeld MWRC
consists of an uplink and L downlinks, characterized by
Uplink: Y0 = X1 ⊕ X2 ⊕ · · · ⊕ XL ⊕ NL

Downlinks:

Yi = X0 ⊕ Ni ,

(1)

for all i ∈ {1, 2, . . . , L}, (2)

where X , Y , N , for all ∈ {0, 1, . . . , L}, each take values in
a ﬁnite ﬁeld F, ⊕ denotes addition over F, X is the channel

1

input from node , Y is the channel output received by node
, and N is the receiver noise at node . The noise N is
arbitrarily distributed, but is i.i.d. for each channel use. We
have used the subscript i to denote a user and the subscript
to denote a node (which can be a user or the relay).
3) Block Codes (joint source-channel codes with feedback):
Consider block codes for which the users exchange m message
tuples in n channel uses. [Encoding:] The t-th transmitted
channel symbol of node
is a function of its message
and the (t − 1) symbols it previously observed on the
downlink: X [t] = f [t](W , Y [1], Y [2], . . . , Y [t − 1]), for
all ∈ {0, 1, . . . , L} and for all t ∈ {1, 2, . . . , n}. As the relay
has no message, we set W 0 = ∅. [Decoding:] The messages
decoded by user i are a function of its message and the symbols
ˆ
it observed on the downlink: (W j,i : ∀j ∈ {1, . . . , L} \ {i}) =
hi (W i , Yi [1], Yi [2], . . . , Yi [n]), for all i ∈ {1, 2, . . . , L}.
4) Achievable Rate: Let Pe denote the probability that
ˆ
W j,i = W i for any i = j. The rate (or bandwidth expansion
factor) of the code is the ratio of channel symbols to source
symbols, κ = n/m. The rate κ is said to be achievable if the
following is true: for any > 0, there exists a block code, with
n and m sufﬁciently large and n/m = κ, such that Pe < .

Remark 1: Theorem 1 characterizes a class of sources (on
any ﬁnite ﬁeld MWRC) for which (i) the set of all achievable
rates is known, and (ii) source-channel separation holds.
Remark 2: Slepian-Wolf type constraints of the form (4)
appear often in multi-terminal information theory. Since
Theorem 2 applies directly to such constraints, it might be
useful beyond its application here to the ﬁnite ﬁeld MWRC.
Remark 3: To prove Theorem 2, we need to select L nonnegative numbers that satisfy (2L − 2) equations.

B. Statement of Main Results

˜
˜
for any non-empty S ⊆ {1, 2, . . . , L}, where WS = i∈S Wi
and WS {Wi : i ∈ S}.
L
The atoms of FL are sets of the form i=1 Ui , where Ui
c
L
˜
˜
can either be Wi or Wi . There are 2 atoms in FL , and we
denote the atoms by
˜
˜
a(K)
Wi −
Wj ,
(6)

III. D EFINITION OF ABCMI
A. The I-Measure
Consider L jointly distributed random variables (W1 , W2 ,
. . . , WL ). The Shannon measures of these random variables can
be efﬁciently characterized via set operations and the I-measure.
For each random variable Wi , we deﬁne a (corresponding) set
˜
˜
Wi . Let FL be the ﬁeld generated by {Wi } using the usual
set operations union ∪, intersection ∩, complement c , and
˜
difference −. The relationship between Wi and Wi is described
∗
by the I-Measure µ on FL , deﬁned as [4, Ch. 3]
˜
µ∗ (WS ) = H(WS ),
(5)

Theorem 1: Consider an L-user ﬁnite ﬁeld MWRC with
correlated sources. If the sources (W1 , W2 , . . . , WL ) have
almost balanced conditional mutual information (ABCMI),
then κ is achievable if and only if
H(W{1,2,...,L}\{i} |Wi )
.
(3)
κ≥
max
i∈{1,2,...,L} log2 |F| − max{H(N0 ), H(Ni )}
The ABCMI condition used in Theorem 1 is rather technical
and best deﬁned using the I-measure [4, Ch. 3]. For this reason,
we specify this condition later in Section III-B after giving
a brief review of the I-measure in Section III-A. For now, it
sufﬁces to note that it is a non-trivial constraint placed on the
joint distribution of (W1 , W2 , . . . , WL ).
The achievability (if assertion) of Theorem 1 is proved
using a separate source-channel coding architecture, which
involves intersecting a certain Slepian-Wolf source-coding
region with the ﬁnite ﬁeld MWRC capacity region [2]. The
particular source-coding region of interest is the classic SlepianWolf region [5] with the total sum-rate constraint omitted;
speciﬁcally, it is the set of all source-coding rate tuples
(r1 , r2 , . . . , rL ) such that
i∈S

ri ≥ H(WS |W{1,2,...,L}\S )

j∈Kc

i∈K

c

for all K ⊆ {1, 2, . . . , L} where K
{1, 2, . . . , L} \ K. Note
that each atom corresponds to a unique K ⊆ {1, 2, . . . , L}. For
the atom in (6), we call |K| the weight of the atom.
Remark 4: The I-measure of the atoms corresponds to the
conditional mutual information of the variables. More speciﬁcally, µ∗ (a(K)) is the mutual information among the variables
{Wi : i ∈ K} conditioning on {Wj : j ∈ Kc }. For example, if
L = 4, then µ∗ (a(1, 2)) = I(W1 ; W2 |W3 , W4 ), where I(·) is
Shannon’s measure of conditional mutual information.
B. Almost Balanced Conditional Mutual Information
For each K ∈ {1, 2, . . . , L − 1}, we deﬁne
µK

(4)

µK

holds for all strict subsets S ⊂ {1, 2, . . . , L}. The next theorem
will be a critical step in the proof of Theorem 1.
Theorem 2: If L arbitrarily correlated random variables
(W1 , W2 , . . . , WL ) have ABCMI, then we can ﬁnd a nonnegative real tuple (r1 , r2 , . . . , rL ) such that
[C1] the inequality (4) holds for all subsets S ⊂ {1, 2, . . . , L}
for which 1 ≤ |S| ≤ L − 2,
[C2] the inequality (4) holds with equality for all subsets S ⊂
{1, 2, . . . , L} for which |S| = L − 1.

max

µ∗ (a(K))

(7)

min

µ∗ (a(K)),

(8)

K⊆{1,2,...,L}
s.t. |K|=K
K⊆{1,2,...,L}
s.t. |K|=K

i.e., atoms of weight K with the largest and the smallest
measures respectively. With this, we deﬁne the ABCMI
condition for L random variables:
Deﬁnition 1: (W1 , W2 , . . . , WL ) are said to have ABCMI
if the following conditions hold:
1
µL−1 ≤ µL−1 1 +
L−2

2

and
1
β(L, S, K)
min
,
K − 1 S∈{K,K+1,...,L−2} α(L, S, K)
for all K ∈ {2, 3, . . . , L − 2}, where
L−1
S
α(L, S, K) S
− (S − K)
(9)
K
K
L−1
S
β(L, S, K) S
− (L − 1)
,
(10)
K
K
1+

i∈S
i ∈ {1, 2, . . . , L}
Fig. 2. Each row represents the contributions from a unique atom, and ri
is the summation of all cells, i.e., {Ji (·)}, in the i-th column. The hashed
region shows the contributions from all atoms with weight K to all ri ’s for i
in some set S.

We now evaluate the LHS of (12) for some ﬁxed i. Consider
some atom a(K) where i ∈ K. We evaluate the contributions
/
from this atom to r −i
{rj : j ∈ {1, 2, . . . , L} \ {i}}, i.e.,
one speciﬁc row in Fig. 2 less the cell Ji (K):
L−|K| ∗
• |K| of the (L − 1) cells each contribute L−1 µ (a(K)).
• The remaining (L − 1 − |K|) cells each contribute
− |K|−1 µ∗ (a(K)).
L−1
So, summing the contributions from a(K) to r −i , we have

IV. P ROOF OF T HEOREM 2
For the rest of this paper, we are interested in atoms only
with weight between one and (L − 1) inclusive. So, we deﬁne
K
{K ⊂ {1, 2, . . . , L} : 1 ≤ |K| ≤ L − 1} and refer to
A {a(K) : ∀K ∈ K} as the set of all such atoms.
We propose to select (r1 , r2 , . . . , rL ) in terms of the IMeasure:
(11)

Ji (K),

Jj (K)

K∈K

j∈{1,2,...,L}\{i}

where
Ji (K) =

L − |K|
|K| − 1 ∗
− (L − 1 − |K|)
µ (a(K)) (14a)
L−1
L−1
= µ∗ (a(K)).
(14b)
= |K|

+ L−|K| µ∗ (a(K)), if i ∈ K
L−1
− |K|−1 µ∗ (a(K)), otherwise, i.e., if i ∈ Kc .
L−1

Each ri is chosen as the sum of the weighted (by a
coefﬁcient + L−|K| or − |K|−1 ) I-measure of all atoms in A.
L−1
L−1
The assignments of ri ’s are depicted in Fig. 2. We term Ji (K)
the contribution from the atom a(K) to ri . Each contribution
is represented by a cell in Fig. 2, and each ri by a column.
We now show that conditions C1 and C2 in Theorem 2 hold
when the ri ’s are chosen as per (11).

Consider some atom a(K ) where i ∈ K . The contributions
from this atom to r −i are as follows (again, one speciﬁc row
in Fig. 2 less the cell Ji (K )):
• (|K | − 1) of the (L − 1) cells each contribute
L−|K | ∗
L−1 µ (a(K )).
• The remaining (L − |K |) cells each contribute
− |K |−1 µ∗ (a(K )).
L−1
So, summing the contributions from a(K ) to r −i , we have

A. For |S| = L − 1:

We ﬁrst show that for any i ∈ {1, 2, . . . , L}, we have
j∈{1,2,...,L}\{i}

rj = H(W{1,2,...,L}\{i} |Wi ).

Jj (K )
j∈{1,2,...,L}\{i}

(12)

L − |K |
|K | − 1 ∗
− (L − |K |)
µ (a(K )) = 0.
L−1
L−1
Combining the above results, we have, for a ﬁxed i,

= (|K | − 1)

Since ri ’s are deﬁned in terms of I-Measure, we link the
measure of atoms to the entropies of the corresponding random
variables:
˜
˜
H(WS |WS c ) = µ∗ (WS − WS c )
(13a)
∗

=

|K| = K

ri

S
S!
where K
K!(S−K)! .
Remark 5: The ABCMI condition requires that all atoms of
the same weight (except for those with weight equal to zero,
one, or L) have about the same I-measure.
Remark 6: For any L, S, and K, such that 2 ≤ K ≤ S ≤
L − 2, it can be shown that α(L, S, K) ≥ β(L, S, K) ≥ 0.
Remark 7: For L = 3, we have µ2 ≤ 2µ2 , i.e., we recover
the ABCMI condition for the three-user case [3].

ri =

a(K)
All atoms
with weight K

Ji (K)
K∈K

µK ≤ µK

µ (a(K)),

rj
j∈{1,2,...,L}\{i}



(13b)

=

non-empty K⊆S

where (13a) follows from [4, eqn. (3.43)], and (13b) is obtained
˜
˜
by counting all the atoms in the set (WS − WS c ).
This
means
the
RHS
of
(12)
equals
µ∗ (a(K)).
non-empty K⊆{1,2,...,L}\{i}

j∈{1,2,...,L}\{i}

µ∗ (a(K))

=
K∈K
s.t. i∈K
/

3





K∈K
s.t. i∈K
/


Jj (K) +
K ∈K
s.t. i∈K


Jj (K )


(15a)
(15b)

µ∗ (a(K))

(15c) [O6] The remaining (S − K) cells each contribute
− K−1 µ∗ (a(K)).
L−1
= H(W{1,2,...,L}\{i} |Wi ).
(15d) Summing the contributions from this active atom to r S ,
K −1 ∗
L−K
B. For 1 ≤ |S| ≤ L − 2:
− (S − K)
µ (a(K))
Ji (K) = K
L−1
L−1
i∈S
Consider some S ⊂ {1, 2, . . . , L} where 1 ≤ |S| ≤ L − 2.
(L − S − 1)(K − 1) ∗
We now show that if the ABCMI is satisﬁed, then
= 1+
µ (a(K)).
L−1
c)
ri ≥ H(WS |WS
(16a)
=

non-empty K⊆{1,2,...,L}\{i}

i∈S

∗

=

µ (a(K)).

(16b)

non-empty K⊆S

Deﬁne S = |S| and r S {ri : i ∈ S}. The LHS of (16a)
is the sum of contributions from all atoms in A to all ri ∈ r S .
We will divide all atoms in A according to their weight K: (i)
K = 1, (ii) 2 ≤ K ≤ S, and (iii) K ≥ S + 1. So, we have
S

ri =
i∈S

Ji (K) +
i∈S

Ji (K)
i∈S K=2

K∈K
s.t. |K|=1

K∈K
s.t. |K|=K

L−1

+

Ji (K).
i∈S K=S+1

S
For any ﬁxed S, there are K active atoms with weight K
(different ways of choosing K ⊆ S), and observations O5 and
O6 are true for each active atom. Combining O3–O6, we can
further categorize the contributions from all atoms with weight
K to r S :
L−1
[O7] Out of the S · K−1 contributions with coefﬁcient L−K ,
L−1
S
K · K of them are from active atoms.
L−1
[O8] Out of the S · L−K−1 contributions with coefﬁcient
S
− K−1 , (S − K) K of them are from active atoms.
L−1
Now, summing the contributions from all (active and inactive)
atoms with weight K to r S , we have

Ji (K) =

(17)
i∈S

K∈K
s.t. |K|=K

1) Atoms with weight K = 1: For atoms with weight one,
µ (a(K)), if K = {i}
0,
otherwise.
∗

Ji (K) =

K⊆S
s.t. |K|=K

(18)

+

Summing the contributions from all atoms with weight one to
all ri ∈ r S ,
µ∗ (a(K)).

Ji (K) =

i∈S

K∈K
s.t. |K|=1

1+

=

(19)

≥

K⊆S
s.t. |K|=K

Ji (K)
i∈S

K S
s.t. |K|=K

(L − S − 1)(K − 1) ∗
µ (a(K))
L−1
Ji (K)

i∈S

K⊆S
s.t. |K|=1

Ji (K) +
i∈S

K∈K
s.t. |K|=K

K S
s.t. |K|=K

µ∗ (a(K)) +
K⊆S

S (L − S − 1)(K − 1)
µK
K
L−1

s.t. |K|=K
2) Atoms with weight 2 ≤ K ≤ S: We ﬁx K. Consider the
L−1
S
L−K
contributions from all atoms with weight K to a particular ri ,
+ S·
−K ·
µ
K −1
K
L−1 K
i ∈ S (one column in the hashed region in Fig. 2). There are
L−1
L−1
S
−(K − 1)
[O1] K−1 contributions with coefﬁcient L−K [from atoms
L−1
+ S·
− (S − K) ·
µK
L−1
L−K −1
K
L−1
where i ∈ K; there are K−1 ways to select the other
(21a)
(K − 1) elements in K from {1, 2, . . . , L} \ {i}], and
L−1
[O2] L−K−1 contributions with coefﬁcient − K−1 [from
K −1
L−1
=
µ∗ (a(K)) +
η,
(21b)
L−1
atoms where i ∈ Kc ; there are L−K−1 ways to select the
L−1
K⊆S
other (L−K −1) elements in Kc from {1, 2, . . . , L}\{i}].
s.t. |K|=K
L
L
There are K atoms with weight K. We can check that K = where
L−1
L−1
β(L, S, K)
K−1 + L−K−1 .
η
α(L, S, K) +
µK − α(L, S, K)µK ,
Since observations O1 and O2 are true for each ri ∈ r S , we
K −1
have the following contributions from all atoms with weight where α(L, S, K) is deﬁned in (9) and β(L, S, K) in (10). If
K to r S (the hashed region in Fig. 2): There are
the ABCMI condition is satisﬁed, then η ≥ 0.
L−1
[O3] S · K−1 contributions with coefﬁcient L−K , and
3) Atoms with weight S + 1 ≤ K ≤ L − 1: Consider the
L−1
L−1
[O4] S · L−K−1 contributions with coefﬁcient − K−1 .
contributions from all atoms with weight K to ri , for some i.
L−1
For an atom a(K), we say that the atom is active if K ⊆ S; From observations O1 and O2, we know that there are
L−1
L−K
otherwise, i.e., K S, the atoms is said to be inactive,
•
K−1 contributions with coefﬁcient L−1 , and
L−1
K−1
Consider the contributions from a particular active atom
•
L−K−1 contributions with coefﬁcient − L−1 .
a(K) to r S (one row in the hashed region in Fig. 2). We have
Summing these contributions, we have for any i that
the following contributions from this atom to r S :
Ji (K)
[O5] Since K ⊆ S, K cells in the hashed row each contribute
K∈K
L−K ∗
s.t. |K|=K
L−1 µ (a(K)).

4

L−1 L−K
L−1
−(K − 1)
3) Achievable Rates: We propose the following sepaµ +
µK
K −1 L−1 K
L−K −1
L−1
rate source-channel coding scheme. Fix mri = nRi . Let
(L − 2)!
(D1 , . . . , DL ), where each Di is i.i.d. and uniformly distributed
(22)
=
KµK − (K − 1)µK ≥ 0.
on {1, . . . , 2mri }. We call Di a dither. The dithers are made
(L − K − 1)!K!
Since α(L, S, K) ≥ β(L, S, K) ≥ 0, the ABCMI condition known to all users. User i performs source coding to compresses
mri
1
K
implies that µK ≤ 1 + K−1 µK = K−1 µK for all K ∈ its message Wi to an index Mi ∈ {1, . . . , 2 } and
mri
computes Mi = Mi + Di mod 2 . The random variables
{2, 3, . . . , L − 1}. Hence, the inequality in (22) follows.
(M1 , . . . , ML ) are independent, with Mi being uniformly
4) Combining the contributions of A to r S : Substituting distributed on {1, . . . , 2nRi }. The nodes then perform channel
(19), (21b), and (22) into (17), if the sources have ABCMI, coding with M as inputs. If (23) is satisﬁed, then each
i
then
user i can recover {Mj : ∀j ∈ {1, . . . , L} \ {i}}, from
S
which it can obtain {Mj : ∀j ∈ {1, . . . , L} \ {i}}. If (4) is
ri ≥
µ∗ (a(K)) +
µ∗ (a(K))
satisﬁed, then each user i can recover all other users’ messages
i∈S
K=2
K⊆S
K⊆S
{W j : ∀j ∈ {1, . . . , L} \ {i}}, using the indices Mj ’s and its
s.t. |K|=1
s.t. |K|=K
own message W i . This means the rate κ = n/m is achievable.
= H(WS |WS c ).
Using the above coding scheme, we have the following:
This completes the proof of Theorem 2.
Lemma 4: Consider a ﬁnite ﬁeld MWRC with correlated
sources. If there exist a tuple (r1 , r2 , . . . , rL ) and a positive
V. P ROOF OF T HEOREM 1
real number κ such that (4) is satisﬁed for all non-empty
strict subsets S ⊂ {1, 2, . . . , L}, and (23) is satisﬁed for all
A. Necessary Conditions
i ∈ {1, 2, . . . , L} with Ri = ri /κ, then the rate κ is achievable.
Lemma 1: Consider a ﬁnite ﬁeld MWRC with correlated C. Proof of Theorem 1 (Necessary and Sufﬁcient Conditions)
sources. A rate κ is achievable only if (3) holds for all i ∈
The “only if” part follows directly from Lemma 1 (regardless
{1, 2, . . . , L}.
of whether the sources have ABCMI). We now prove the “if”
Lemma 1 can be proved by generalizing the converse
part. Suppose that the sources have ABCMI. From Theorem 2,
theorem [6, Appx. A] for L = 3 to arbitrary L. The details
there exists a tuple (r1 , . . . , rL ) such that conditions C1 and
are omitted.
C2 are satisﬁed. These conditions imply that (4) is satisﬁed for
all non-empty strict subsets S ⊂ {1, 2, . . . , L}. Let Ri = ri /κ.
B. Sufﬁcient Conditions
H(W{1,2,...,L} |Wi )
Condition C2 further implies κ =
. So, if
i∈{1,2,...,L}\{i} Rj
Consider a separate source-channel coding architecture.
(3) is true, then (23) is satisﬁed for all i ∈ {1, 2, . . . , L}. From
1) Source Coding Region: We have the following source Lemma 4, κ is achievable.
.
coding result for correlated sources:
Remark 8: For a ﬁxed source correlation structure and
Lemma 2: Consider L correlated sources as deﬁned in a ﬁnite ﬁeld MWRC, one can check if the L-dimensional
Section II-A1. Each user i encodes its message W i to an polytope deﬁned by the source coding region and that by the
index Mi ∈ {1, 2, . . . , 2mri }, for all i. It reveals its index to channel coding region (scaled by κ) intersect when κ equals
all other users. Using these indices and its own message, each the RHS of (3). If the regions intersect, then we have the set of
user i can then decode the messages of all other users, i.e., all achievable κ for this particular source-channel combination.
{W j : ∀j ∈ {1, 2, . . . , L} \ {i}}, if (4) is satisﬁed for all Theorem 1 characterizes a class of such sources.
non-empty strict subsets S ⊂ {1, 2, . . . , L}.
R EFERENCES
The above result is obtained by combining the results for
[1] A. D. Wyner, J. K. Wolf, and F. M. J. Willems, “Communicating via a
(i) source coding for correlated sources [7, Thm. 2], and (ii)
processing broadcast satellite,” IEEE Trans. Inf. Theory, vol. 48, no. 6,
the three-user noiseless MWRC [1, Sec. II.B.1]. Note that the
pp. 1243–1249, June 2002.
relay does not participate in the source code, in contrast to the [2] L. Ong, S. J. Johnson, and C. M. Kellett, “The capacity region of multiway
relay channels over ﬁnite ﬁelds with full data exchange,” IEEE Trans. Inf.
setup of [1]. Instead, the relay participates in the channel code.
Theory, vol. 57, no. 5, pp. 3016–3031, May 2011.
2) Channel Coding Region: We have the following channel [3] L. Ong, R. Timo, G. Lechner, S. J. Johnson, and C. M. Kellett, “The ﬁnite
ﬁeld multi-way relay channel with correlated sources: The three-user case,”
coding result for the ﬁnite ﬁeld MWRC [2]:
in Proc. IEEE Int. Symp. on Inf. Theory (ISIT), St Petersburg, Russia,
Lemma 3: Consider the ﬁnite ﬁeld MWRC deﬁned in (1)–
July 31–Aug. 5 2011, pp. 2238–2242.
(2). Let the message of each user i be Mi , which is i.i.d. and [4] R. W. Yeung, Information Theory and Network Coding, Springer, 2008.
uniformly distributed on {1, . . . , 2nRi }. Using n uplink and [5] T. S. Han, “Slepian-Wolf-Cover theorem for networks of channels,” Inf.
and Control, vol. 47, no. 1, pp. 67–83, Oct. 1980.
downlink channel uses, each user i can reliably decode the [6] L. Ong, R. Timo, G. Lechner, S. J. Johnson, and C. M. Kellett,
“The three-user ﬁnite ﬁeld multi-way relay channel with correlated
message of all other users {Mj : ∀j ∈ {1, 2, . . . , L} \ {i}} if
≥

j∈{1,2,...,L}\{i}

sources,” submitted to IEEE Trans. Inf. Theory, 2011. [Online]. Available:
http://arxiv.org/abs/1201.1684v1
[7] T. M. Cover, “A proof of the data compression theorem of Slepian and
Wolf for ergodic sources,” IEEE Trans. Inf. Theory, vol. IT-21, no. 2, pp.
226–228, Mar. 1975.

Rj ≤ log2 |F| − max{H(N0 ), H(Ni )}, (23)

for all i ∈ {1, 2, . . . , L}.

5


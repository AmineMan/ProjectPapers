Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon Apr 23 12:30:29 2012
ModDate:        Tue Jun 19 12:56:28 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      422336 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564613

Jar Decoding: LDPC Coding Theorems for Binary
Input Memoryless Channels
En-Hui Yang and Jin Meng
[5] is a powerful tool to analyze LDPC codes under BP
decoding (which is output-centric, as the iterative decoding
procedure starts with a channel output and ends upon ﬁnding a
codeword); it provides a good approximation of performance
(concerning bit error probability) of LDPC codes and also
facilitates the optimization of degree distributions. In general,
however, rigorous analysis in this approach is computationally
prohibitive, and its result is hard to comprehend without any
approximation. In addition, most of analysis is also asymptotic.
Recently, jar decoding was proposed as an alternative decoding rule [6]. Under jar decoding, the decoder ﬁrst forms a
set of suitable size, called a jar, consisting of sequences from
the channel input alphabet considered to be closely related
to the channel output, and then takes any codeword from the
jar as the estimate of the transmitted codeword. In contrast
with MAP and ML decoding, jar decoding is channel output
sequence centric. It is ﬂexible in the sense that it can handle
both the word error probability and bit error probability;
it is also powerful in the sense that new general coding
theorems (with respect to channel statistics) can be established.
Extracting the portion related to LDPC codes from its full
version [6], this paper aims to demonstrate the capability of
jar decoding to handle bit error probability. It is shown that
compared to ML or BP decoding, jar decoding makes LDPC
code performance analysis much easier; under jar decoding,
general LDPC coding theorems can be established. Speciﬁcally, consider any binary input memoryless channel (BIMC)
X → Y where X is assumed to be uniformly distributed
throughout the paper. Then given any bit error probability, nonasymptotic bounds on the achievable rate of LDPC codes are
derived, which show that for any valid variable and check node
degree distribution L(z) and R(z), LDPC ensembles with
variable and check node degree distributions L(z k ) and R(z k )
(where each degree in L(z) and R(z) is multiplied by k)
can approach I(X; Y ), with diminishing bit error probability,
as k increases. As a by-product, it is also proved that for
any ﬁxed variable node degree distribution, the optimal check
node degree distribution of a LDPC code under jar decoding is
right concentrated. Last but not the least, as jar decoding and
BP decoding shares an interesting similarity—both are output
sequence centric—simulation further reveals that BP decoding
can be considered as one of many ways to pick up a codeword
from the jar for LDPC codes when it succeeds in outputting
a codeword.
The rest of the paper is organized as follows. Section II
reviews the concept of jar decoding. LDPC coding theorems

Abstract—Recently, a new decoding rule called jar decoding
was proposed, under which the decoder ﬁrst forms a set of
suitable size, called a jar, consisting of sequences from the
channel input alphabet considered to be closely related to y n ,
and then takes any codeword from the jar as the estimate of
the transmitted codeword. In this paper, we show that under jar
decoding, the analysis of low density parity check (LDPC) codes
is much easier compared to maximum a posteriori (MAP) or
maximum likelihood (ML) and Belief Propagation (BP) decoding,
and new general LDPC coding theorems can be established.
Speciﬁcally, it is proved that LDPC codes can approach the
mutual information, with diminishing bit error probability, of any
binary input memoryless channel with uniform input distribution
when the average variable node degree is large. Moreover,
simulation shows an interesting connection between jar decoding
and BP decoding, i.e., BP decoding can be regarded as one of
many ways to pick up a codeword from the jar for LDPC codes
when it succeeds in outputting a codeword.
Index Terms—Belief propagation decoding, channel capacity,
jar decoding, low-density parity-check codes, maximum a posteriori (MAP) decoding, non-asymptotic coding theorems, nonasymptotic equipartition properties.

I. I NTRODUCTION
So far, the theoretic performance analysis of low density
parity check (LDPC) codes has been carried out in two distinct
approaches. Originating from Gallager’s milestone paper [1],
one approach is to exploit the best possible performance of
LDPC codes with respect to word error probability under
maximum a posteriori (MAP), maximum likelihood (ML) or
minimum distance (MD) decoding. The other approach, as
demonstrated in Urbanke and Richardson’s award-winning
paper [2], is to investigate the performance of LDPC codes
with respect to bit error probability under belief propagation
(BP) decoding.
Both approaches have their strengths and weaknesses. Under ML decoding (which is codebook-centric as it involves
the optimization of likelihood of codewords over the entire codebook), coding theorems on LDPC ensembles can
be derived rigorously for memoryless binary-input outputsymmetric (MBIOS) channels [1], [3] [4]. Based on Hamming
weight distributions (or spectrums) of LDPC ensembles, the
analysis in this direction, for which symmetry of channel is
essential, is concerned mainly with word error probability
and is asymptotic. On the other hand, density evolution [2]
This work was supported in part by the Natural Sciences and Engineering
Research Council of Canada under Grant RGPIN203035-11, and by the
Canada Research Chairs Program.
En-Hui Yang and Jin Meng are with the Dept. of Electrical and Computer
Engineering, University of Waterloo, Waterloo, Ontario N2L 3G1, Canada.
Email: ehyang@uwaterloo.ca, j4meng@uwaterloo.ca

1

under jar decoding are established in Section III. Simulation
results are reported in Section IV to conﬁrm that for LDPC
codes, BP decoding can be viewed as one of many ways to
pick up a codeword from the jar when it succeeds in outputting
a codeword. Finally, conclusions are drawn in Section V.

III. LDPC C ODING T HEOREMS WITH JAR D ECODING
In this section, the capacity of LDPC codes under jar
decoding with bit error probability for BSC and BIMC will
be analyzed, and certain interesting results will be reported
(including mutual information-approaching LDPC codes and
optimality of check node concentration degree distribution).
To facilitate the following discussion, we use the standard
notion of tanner graph [7] of linear codes and the normalized
variable and check degree distributions from a node perspective of a parity check matrix Hm×n (and its tanner graph)
L
R
[5] denoted by L(z) = i=1 Li z li and R(z) = j=1 Rj z rj ,
where Li and Rj represent the percentages of variable and
check nodes with degrees li and rj respectively ∗ .
Given m, n, and (normalized) variable and check degree
distributions L(z) and R(z) satisfying nL (1) = mR (1), let

II. JAR D ECODING
Consider a BIMC {p(y|x) : x ∈ X , y ∈ Y}, where X =
{0, 1} is the channel input alphabet, and Y is the channel
output alphabet, which is arbitrary and could be discrete or
continuous. As such, for any x ∈ X , p(y|x) is a probability
mass function (pmf) over Y if Y is discrete, and a probability
density function (pdf) over Y if Y is the real line.
For any set S, let S n denote the set of all sequences of
length n drawn from S.
Deﬁnition 1 (Jar Decoding). Given any channel with input
alphabet X and output alphabet Y, which may not be necessarily memoryless, any code of block length n for the channel,
and any channel output sequence y n ∈ Y n , jar decoding ﬁrst
forms a set of suitable size (called a jar and denoted by J(y n ))
consisting of sequences xn ∈ X n considered to be closely
related to y n through the channel, and then picks any codeword
(if any) from the jar J(y n ) as the estimate of the transmitted
codeword.

¯
∆
∆
¯ =L (1), r =R (1), l = m ,
l
¯
r
¯
n

and Hm,n,L(z),R(z) denote the collection of all m × n parity
check matrices with normalized variable and check degree
distributions L(z) and R(z). Then an LDPC code of designed
rate (1 − m/n) ln 2 (in nats) is said to be randomly generated
from the ensemble Cm,n,L(z),R(z) with degree distributions
L(z) and R(z) if its parity check matrix Hm×n is uniformly picked from Hm,n,L(z),R(z) . Denote the designed rate
(1 − m/n) ln 2 as R(Cm,n,L(z),R(z) ). The encoding procedure
of Cm,n,L(z),R(z) is assumed to be systematic so that the
original information bits are visible in each codeword.
To establish our LDPC coding theorems, the probability
Pr {Hm×n xn = 0m } is investigated ﬁrst, which depends on
the support set of xn , i.e., the positions of non-zero elements
in xn . Let κ(xn ) represent the support set of xn , and we
write κ(xn ) simply as κ whenever xn is generic or can be
determined from context. Let Hκ
m×|κ| be the matrix consisting of those columns of Hm×n with indices in κ. The
degree polynomial of κ, denoted by Lκ (z), is deﬁned by
∆
L
Lκ (z) = i=1 Lκ z li where Lκ n is the number of columns
i
i
∆
L
with degree li within Hκ
. And deﬁne ¯κ = i=1 Lκ li .
l
i
m×|κ|
Then the following lemma is proved in [6] (a similar result
has been proved in [8]); its proof is omitted here due to the
page limitation.

Example 1 (Hamming Jar): Consider the binary symmetric
channel (BSC) with cross-over probability 0 < p < 0.5. No
matter what the code of block length n used over the BSC is,
the jar J(y n ) for each y n ∈ {0, 1}n can be formed as
J(y n ) =

xn ∈ X n :

1
wt(y n − xn ) ≤ p + δ
n

(2.1)

where wt(z n ) denotes the Hamming weight of z n , i.e., the
number of nonzero entries in z n , and δ is a real number. For
the obvious reasons, the jar deﬁned in (2.1) will be referred
to as a Hamming jar. The size of J(y n ) is upper bounded by
enH(p+δ) whenever p + δ < 0.5.
Example 2 (BIMC Jar): Consider now an arbitrary BIMC,
where Y is the BIMC output in response to the uniform input
random variable X. For any xn ∈ X n and y n ∈ Y n , let
n

p(y n |xn ) =

(3.1)

p(yi |xi ) .
i=1

Lemma 1. Let L(z) and R(z) be normalized variable and
check node degree distributions from a node perspective with
∆
minimum variable node degree l1 ≥ 1. Let g(τ, k) =(1+τ )k +
(1−τ )k for any τ and k. Suppose Hm×n (m ≤ n) is uniformly
picked from ensemble Hm,n,L(z),R(z) . Then for any xn = 0
with its support set κ,

Then given y n ,
p(xn |y n ) =

p(y n |xn )
+ p(yi |1)]

n
i=1 [p(yi |0)

is a pmf over X n . In this case, the jar J(y n ) for y n can be
formed as
1
J(y n ) = xn ∈ X n : − ln p(xn |y n ) ≤ H(X|Y ) + δ
n
(2.2)
where δ is a real number. Once again, one can verify that
|J(y n )| ≤ en(H(X|Y )+δ)

Pr {Hm×n xn = 0m } ≤
ln(nˆκ )
l
exp nP ¯ R(z), ¯κ +
l,
l
2

(2.3)

R

ri
i=1

∗ readers who are unfamiliar with those terminology are directed to references provided

n

for any y .

2

1
+ ln n¯κ 1 −
l
2

¯κ
l
¯
l

whenever

+ O(1)

+2 p+

where

¯κ ¯ ¯κ
1
ˆκ = min l , l − l
l
+
2
n
and for any ¯ ξ ∈ (0, ¯ and R(z), P ¯ R(z), ξ is deﬁned as
l,
l]
l,

¯
l
r
¯

for ξ ∈ 0, ¯ −
l

R
j=1

−

¯
l
r
¯

∆

B(xn , ) = z n :
(3.3)

∆

=−∞

(3.4)

where W n is the noise vector. Then

for ξ ∈ ¯ −
l
Rj π(rj ), ¯ with the convention that
l
−∞
e
= 0, and where for any integer r
0
1

Pb (Cm,n,L(z),R(z) )
1
ˆ
wt(X n − X n )
= E
n
ˆ
≤ Pr X n ∈ B(X n , ) +

if r is even
otherwise.

By investigating the properties of P (¯ R(z), ξ), it can be
l,
further shown [6] that
n

≤

Pr {Hm×n z = 0 }
2

+

¯
ln nl
2
2

R
i=1

Pr{∃xn ∈ J(Y n ) ∩ B(X n , ), X n ∈ J(Y n ),
Hm×n xn = 0m } + Pr {X n ∈ J(Y n )} + . (3.11)
/

m

¯
1
nl
¯
≤ enP (l,R(z),l1 )+ 2 ln 4

To continue, on one hand, we have
ri +O(1)

Pr {X n ∈ J(Y n )}
/

( )

=
whenever
( )

−nΓm,n,L(z),R(z) −2 ln n+O(1)

e
≤

Γm,n,L(z),R(z)

1
n
n wt(z )

(3.5)
≤ Pr

≤ 1 − where

∆
= −P ¯ R(z), l1
l,

−

1
n¯
l
ln
2n
2

1
ˆ
=E
wt(X n − X n )
n

ln n
n

≤ n−2

ri
i=1

(3.12)

where 1) is due to Hoeffding’s inequality. On the other hand,
we have
Pr{∃xn ∈ J(Y n ) ∩ B(X n , ), X n ∈ J(Y n ), Hm×n xn = 0m }
=

Pr {∃xn ∈ J(X n + W n ) ∩ B(X n , ),
X n ∈ J(X n + W n ), Hm×n (xn − X n ) = 0m }

=

Pr {∃xn , xn − X n ∈ J(W n ) ∩ B(0n , ),
0n ∈ J(W n ), Hm×n (xn − X n ) = 0m }

=

(3.7)

Pr{∃z n ∈ J(W n ) ∩ B(0n , ),
0n ∈ J(W n ), Hm×n z n = 0m }

ˆ
where X n denotes the estimated codeword by jar decoding, and the expectation is with respect to the transmitted
random codeword, the BSC, and the random LDPC code
ln n
Cm,n,L(z),R(z) itself. Selecting δ =
n in the Hamming
jar (2.1), we then have the following theorem.

Pr {W n = wn }

=
wn ∈J(0n )

× Pr{∃z n ∈ J(wn ) ∩ B(0n , ), Hm×n z n = 0m }
2)

Pr {W n = wn }

≤
wn ∈J(0n )

Theorem 1. For any variable and check node degree distributions L(z) and R(z), and for any block length n,
Pb Cm,n,L(z),R(z) ≤ + O n−2

1
wt(W n ) > p +
n

1)

R

1
n¯2
l
2 ln n
−
ln
−
.
(3.6)
2n
4
n
We ﬁrst establish our non-asymptotic LDPC coding result
for the BSC with capacity CBSC . By assuming the encoding
procedure to be systematic, the original information bits are
visible in the transmitted codeword X n , and we can measure
the bit error probability by
Pb Cm,n,L(z),R(z)

1
wt(z n − xn ) >
n

for any xn ∈ X n and 0 ≤ < 1. Let X n be the transmitted
codeword, and Y n the output of the BSC in response to X n ,
i.e.,
Y n = Xn + W n

R
j=1

π(r) =

m
( )
ln 2 − Γm,n,R(z),L(z)
n
1−p
ln n
ln
.
(3.10)
p
n

Proof: Let B(xn , ) be a subset of X n , deﬁned as

Rj π(rj ) , and

P ¯ R(z), ξ
l,

(3.9)

R(Cm,n,L(z),R(z) ) ≤ CBSC −

¯ R
g(τ, ri )
¯ ξ/¯ − ξ ln τ + l
Ri ln
= − lH
l
r i=1
¯
2
(3.2)
in which τ is the solution to
¯ R
l
g(τ, ri − 1) ¯
Ri ri
=l−ξ
r i=1
¯
g(τ, ri )

<1

and

∆

P ¯ R(z), ξ
l,

ln n
n

( )

−nΓ

−2 ln n+O(1)

m,n,L(z),R(z)
× |J(wn )|e
√ ln n
( )
nH p+
−nΓm,n,L(z),R(z) −2 ln n+O(1)
n
≤ e
e

(3.8)

3

≤
=

e

1−p
p

n H(p)+(ln

O n

−2

)

√ ln n
n

( )

e

−nΓm,n,L(z),R(z) −2 ln n+O(1)

[whenever (3.10) holds]

whenever
H( ) ≤ γn (X|Y ) −

(3.13)

where the inequality 2) is due to (3.5), z n ∈ B(0n , ), and the
fact that wn ∈ J(0n ) and z n ∈ J(wn ) imply
1
wt(z n ) ≤
n
<

m
( )
ln 2 − Γm,n,L(z),R(z)
n
4 ln n
− σH (X|Y )
.
(3.17)
n
Remark. To show the existence of satisfying (3.16), it can
be veriﬁed that γn (X|Y ) > 0, by observing that
R(Cm,n,L(z),R(z) ) ≤ I(X; Y ) −

whenever (3.9) holds. Then (3.8) is proved by combining
(3.11), (3.12) and (3.13). This completes the proof of Theorem 1.
A tighter bound can be obtained in the following form, i.e.

n(p+δ)+1≤t≤n

1
1
ln E pλ (X|Y ) − ln E pλ (−X|Y )
2
2
is a concave function of λ, gn (0) = 0, and

pt (1 − p)n−t

dgn (λ)
dλ

z n ∈J(wn )

λ=0

n

n

Pb (Cm,n,L(z),R(z) )
≤
δ
n(p+ ln 2 )+1≤t≤n

z n ∈V (wn )

ε(wn )
V (w )

x∈X

sup −λ H(X|Y ) + σH (X|Y )
λ≥0

4 ln n
n ,

4 ln n
n

1
1
− ln E pλ (X|Y ) − ln E pλ (−X|Y )
2
2

)

|{i : wi = ε}|

∆

= {v n ∈ X n : vi = 0 if wi = 0} .

R(z) = (1 + r − r) z
¯
¯

Theorem 2. For any variable and check node degree distributions L(z) and R(z), any block length n, and any ∈ (0, 0.5),
Pb (Cm,n,L(z),R(z) ) ≤ + O(n

∆

=

Theorem 3. Given the variable node degree distribution L(z)
and the rate of code R = 1− m , the optimal R(z) is the check
n
node concentrated distribution, i.e.

where −x is the complement of x, i.e. the module-2 addition
of x and 1. Then we have the following theorem.

−2

wt(z )
Pr{Hm×n z n = 0m }
n

From Theorems 1 and 2, it can be clearly seen that there is
a constant gap between the rate of LDPC and I(X; Y ):
m
( )
ln 2 − Γm,n,R(z),L(z) .
(3.19)
n
Several interesting results arise from the study of this gap with
respect to the degree distributions L(z) and R(z). First of all,
let us consider the optimal R(z) which can minimize (3.19)
given L(z).

p(y)p(x|y)[− ln p(x|y) − H(X|Y )]2 dy

∆

)

where
n

=

n

(3.18)

2
σH (X|Y )

γn (X|Y )

n

pε(w ) (1 − p)n−ε(w

×

m

specify J(Y n ) as the BIMC jar with δ = σH (X|Y )
and further deﬁne

pt (1 − p)n−t

δ
wn ∈{0,ε}n :ε(wn )≤n(p+ ln 2 )
n

In other words, we randomly choose a coset code of Hm×n
for use over the BIMC. To present our LDPC coding theorem
for the BIMC, let X and Y be the uniform random variable
and corresponding channel output respectively, deﬁne
∆

n
t

+

{x ∈ X : Hm×n x = S }.

=

4 ln n
n

whenever I(X; Y ) > 0 and n is large.
Similarly, for the BEC with erasure probability p, a tighter
upper bound can be obtained as follows:

wt(z n )
Pr{Hm×n z n = 0m } (3.14)
n

where tightening (3.11) kills and Pr{Hm×n z n = 0m } is
bounded by Lemma 1. Some symmetry can be exploited to
evaluate this bound in future research.
Now let us extend Theorem 1 to an arbitrary BIMC
{p(y|x) : x ∈ X , y ∈ Y}. Towards this, we modify
Cm,n,L(z),R(z) in the following way: Hm×n and S m are
uniformly picked from Hm,n,L(z),R(z) and X m respectively,
and the codebook consists of
n

1
p(X|Y )
E ln
− σH (X|Y )
2
p(−X|Y )
> 0
=

wn ∈J(0n )

×

4 ln n
n

−

pwt(w) (1 − p)n−wt(w)

+

= −λ H(X|Y ) + σH (X|Y )

gn (λ)

Pb (Cm,n,L(z),R(z) )
n
t

(3.16)

and

1
1
wt(wn ) + wt(z n − wn )
n
n
ln n
<1−
2 p+
n

≤

2 ln n
n

where
r=
¯

(3.15)

4

r
¯

+ (¯ − r )z
r
¯

¯
n¯
l
.
l=
m
1−R

r
¯

(3.20)

decoding was run for 1000 blocks. In our simulation, we
observed that BP decoding always failed whenever
1
wt(Y n − X n ) > 0.098 = p + 0.008
n
and sometimes succeeded and sometimes failed when
1
wt(Y n − X n ) < 0.098 = p + 0.008 .
n
The second testing channel we selected is the binary input
additive white Gaussian channel with variance of noise σ =
0.875 and channel capacity 0.575 (in bits). The codeword was
modulated to {−1, +1}. In our simulation, we observed that
BP decoding always failed whenever
1
− ln p(X n |Y n ) > 0.322 = H(X|Y ) + 0.028
n
and sometimes succeeded and sometimes failed when
1
− ln p(X n |Y n ) < 0.322 .
n
Both simulations conﬁrm that BP decoding can be regarded
as one of many ways to pick up a codeword from a jar when
it succeeds in outputting a codeword.

The next result shows that LDPC codes can achieve asymptotically I(X; Y ) of any BIMC with diminishing bit error
probability when large degrees are used.
Theorem 4. Given any variable and check node degree
distributions L(z) and R(z),
1
Pb (Cm,n,L(zk ),R(zk ) ) ≤ √ + O(n−2 )
2 k
whenever
2 ln n
1
√
≤ γn (X|Y ) −
H
n
2 k
and

(3.21)

(3.22)

R(Cm,n,L(zk ),R(zk ) )
≤ I(X; Y ) − O e−

l 1 r1
¯
l

√

k+ 1 ln k
2

+

ln nk
+
n

ln n
n

.

(3.23)
Remark 1. In Theorem 4, k is not related to n, and can remain
a constant as n approaches inﬁnity. Therefore, the parity check
matrix of the code can be always sparse, although large k is
needed for the rate of the code to approach I(X; Y ).

V. C ONCLUSION
In this paper, LDPC codes are investigated under jar decoding. In comparison with old decoding rules (MAP or ML
and BP decoding), jar decoding really makes LDPC coding
analysis simpler and easier. Speciﬁcally, we have shown that
LDPC codes can achieve, with diminishing bit error probability, the mutual information of any BIMC with uniform
input distribution as their average node degrees increase, and
given any variable node degree distribution, the check node
concentration distribution is optimal under jar decoding. Moreover, simulation conﬁrms the interesting connection between
BP decoding and jar decoding, i.e. from the perspective of
jar decoding, BP decoding is one of many ways to pick up a
codeword in the jar when it succeeds in outputting a codeword.
Therefore, with jar decoding, we believe that there is ample
room to design effective codes and jar decoding algorithms.

The proofs of Theorem 2, 3 and 4 are in [6] and omitted
due to the length of this paper. Also, the proof of Theorem 4
is similar to that of Theorem 4 in [8].
IV. S IMULATION R ESULTS
In this section, we demonstrate, by simulation, that for
LDPC codes, BP decoding can be regarded as one of many
ways to pick up a codeword from a jar when it succeeds in
outputting a codeword.
In our simulation, we ﬁrst selected a LDPC code with block
length n = 8000, coding rate 1 − m = 0.5 (in bits), variable
n
node degree distribution
L(z)

=

0.457827z 2 + 0.323775z 3 + 0.0214226z 4
+ 0.0592851z 6 + 0.0389015z 7 + 0.0248109z 8
+ 0.00884569z 9 + 0.0176697z 19 + 0.0474625z 20

R EFERENCES

and check node concentration degree distribution R(z), and
then randomly chose its coset code

[1] R. G. Gallager, Low-Density Parity-Check Codes. Cambridge, MA: MIT
Press, 1963.
[2] T. J. Richardson and R. L. Urbanke, “The capacity of low-density paritycheck codes under message-passing decoding,” IEEE. Trans. Inform.
Theory, vol. 2, pp. 599–618, Feb. 2001.
[3] G. Miller and D. Burshtein, “Bounds on the maximum-likelihood decoding error probability of low-density parity-check codes,” IEEE Trans. Inf.
Theory, vol. 47, no. 7, Nov. 2001.
[4] I. Sason and R. Urbanke, “Parity-check density versus performance of
binary linear block codes over memoryless symmetric channels,” IEEE
Trans. Inf. Theory, vol. 49, no. 7, pp. 1611–1635, July 2003.
[5] T. Richardson and R. Urbanke, Modern Coding Theory. Cambridge
University Press, 2008.
[6] E.-H. Yang and J. Meng, “Jar decoding: Basic concepts and nonasymptotic capacity achieving coding theorems for channels with discrete
inputs,” submitted to IEEE Trans. on Inform. Theory, 2011.
[7] R. M. Tanner, “A recursive approach to low complexity codes,” IEEE
Trans. Inform. Theory, vol. 27, pp. 533–547, 1981.
[8] J. Meng and E.-H. Yang, “Interactive encoding and decoding based on binary ldpc codes with syndrome accumulation,” submitted to IEEE Trans.
on Inform. Theory, 2011. Available online: http://arxiv.org/abs/1201.5167.

{xn : Hm×n xn = S m }
for use over our testing channel. Let X n denote the transmitted
codeword and Y n denote the channel output.
At the decoder, the standard BP decoding algorithm was
used. Simply speaking, messages are passed and modiﬁed in
certain manner between check and variable nodes in the tanner
graph, and eventually the decoding output is the hard decision
on each variable node with channel statistics ln Pr{Xi =0|Yi }
Pr{Xi =1|Yi }
and messages passed to it. The algorithm kept running until
it either found a codeword xn i.e. Hm×n xn = S m or the
ˆ
ˆ
upper bound on the number iteration (N ) was reached (in our
simulation N = 100).
The ﬁrst testing channel we selected is the BSC with
crossover probability 0.09 and capacity 0.564 (in bits). BP

5


Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu Dec 29 09:00:12 2011
ModDate:        Tue Jun 19 12:54:12 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      344924 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569551535

On Optimum Strategies for Minimizing the
Exponential Moments of a Loss Function
Neri Merhav
Department of Electrical Engineering
Technion – Israel Institute of Technology
Technion City, Haifa 32000, Israel
Email: merhav@ee.technion.ac.il

works along this line, especially in contexts related to buffer
overﬂow in data compression [10], [11], [22], exponential
moments related to guessing [1], large deviations properties
of parameter/signal estimators, [4], [12] and more.
How can we harness the existing knowledge on optimum
strategies in the ﬁrst moment sense in our quest for minimizing
exponential moments? We ﬁrst furnish sufﬁcient conditions
that the optimum strategy in the exponential moment sense
can be found in terms of the optimum strategy in the ﬁrst
moment sense. The main message of this expository paper is
that the combination of these sufﬁcient conditions serves as a
tool for solving concrete problems, and it gives a a new insight
into these problems. In some applications, these sufﬁcient
conditions for optimality yield an equation in s, whose solution
is the desired optimum strategy. In other applications this may
not be the case yet the optimality conditions may still be useful
as one may have an intuitive guess concerning the optimum
strategy, and then the optimality conditions can be used to
prove optimality. A few application examples of this are given
in Section 3.
We next study the asymptotic regime (Section 4). Consider
the case where X is a random vector, X = (X1 , . . . , Xn ), governed by a product–form probability distribution, and (X, s)
grows linearly with n for a given empirical distribution of
X. In this case, the exponential moments typically behave
like exponential functions of n. If we can then select a
strategy s that somehow “adapts” to the empirical distribution
of (X1 , . . . , Xn ), then such strategies may be universally
asymptotically optimum in that they depend on neither the
probability distribution, nor on α. This is again demonstrated
in several examples.

Abstract—We consider a general problem of minimizing the
exponential moment of a given loss function, with an emphasis
on the relation to the more common criterion of minimization
the ﬁrst moment of the same loss function. Our basic observation
is about simple sufﬁcient conditions for a strategy to be optimum
in the exponential moment sense. This observation is useful and
application examples are given. We also examine the asymptotic
regime and investigate universal asymptotically optimum strategies in light of the aforementioned sufﬁcient conditions.

I. I NTRODUCTION
Many problems in information theory and related ﬁelds are
about the quest for a strategy s that minimizes the expectation
of a certain loss function, (X, s), where X is a random variable. A few examples are the following: (i) Data compression,
where X symbolizes the data to be compressed, s is the data
compression scheme, and (X, s) is the length function or
the distortion or a linear combination of both, (iii) Bayesian
estimation where X designates jointly the desired random
variable and the measurements, s is the estimation function
and (X, s) is the error function, (iv) Prediction, sequential
decision (see, e.g., [17]) and stochastic control problems [3].
While the criterion of minimizing E{ (X, s)} has been
most common, the exponential moments, E exp{α (X, s)}
(α > 0), have received much less attention at least in information theory and signal processing. In optimization and stochastic control, on the other hand, the exponential moment criterion
has received much more attention, and it is well–known as the
risk–sensitive or risk–averse loss function (see, e.g., [5], [6],
and references therein), where one of the main motivations is
imposing a penalty that is sensitive to large values of (X, s),
hence the qualiﬁer “risk–sensitive”. Another motivation is
robustness of the resulting risk–sensitive optimum controllers
[2], A few additional motivations for minimizing exponential
moments, are the following. First, E exp{α (X, s)}, as a
function of α, is the moment–generating function of (X, s),
and hence provides full information about the distribution of
this random variable. If we ﬁnd a strategy that uniformly
minimizes E exp{α (X, s)} for all α ≥ 0 (and there are
examples of this), this is stronger than just minimizing the
ﬁrst moment. Secondly, exponential moments are intimately
related to large–deviations rate functions, and are related
to minimizing the probabilities of large deviations events
Pr{ (X, s) ≥ L0 } (for some threshold L0 ). There are several

II. T HE BASIC O BSERVATION
Let X be a random variable taking on values in an alphabet X , and drawn from a probability distribution P . Let
the variable s designate a strategy chosen from a space S.
Here, the term “strategy” means a mathematical object that,
depending on the application, may be either a scalar, a vector,
a function of X or of another random variable that depends on
X. Associated with each x ∈ X and s ∈ S, is a loss (x, s).
The function (x, s) is called the loss function. The operator
E{·} (or E P {·}) will denote the expectation operator w.r.t.

1

If (x, s) is non-negative, this has the advantage that the
exponential moment is ﬁnite for all α > 0. From the same
considerations as before, here we have:

P , and whenever we refer to the expectation w.r.t. another
probability distribution Q, we use the notation E Q {·}.
For a given α > 0, consider the problem of minimizing
E exp{α (X, s)} across s ∈ S. The following observation
relates the optimum s for this problem to the optimum s for the
problem of minimizing E Q { (X, s)} w.r.t. another probability
distribution Q.
Observation 1: Assume that there exists a strategy s ∈ S
∆
for which Z(s) = E P exp{α (X, s)} < ∞. A strategy
s ∈ S minimizes E P exp{α (X, s)} if there exists a probability distribution Q on X that satisﬁes the following two
conditions: 1) s minimizes E Q { (X, s)} over S. 2) Q(x) =
P (x)eα (x,s) /Z(s).
The proof appears in the full version of this paper [14].
Partially related results have appeared in the literature of
optimization and control (cf. [8, Theorem 4.9]). However, in
[8], a much more complicated paradigm has been considered,
and the results therein do not seem to be completely equivalent
to Observation 1. Moreover, since the setting here is much
simpler, then so is the proof, which is not only short, but also
almost free of regularity conditions.
Note that for a given s, Jensen’s inequality
E P exp{α (X, s)}

max E exp{−α (X, s)}
s

=

(X,s)

= emaxQ [αE Q

,

Consider the problem of quantizing X into M reproduction
levels, x0 , x1 , . . . , xM −1 , and let the distortion metric be
ˆ ˆ
ˆ
d(x, x) = (x − x)2 . The ordinary quantizer design problem is
ˆ
ˆ
about the choice of a function s : X → {ˆ0 , x1 , . . . , xM −1 },
x ˆ
ˆ
that minimizes E P [X − s(X)]2 , i.e., (x, s) = [x − s(x)]2 .
As is well known [13], this problem lacks a closed–form
solution, and one resorts to iterative algorithms for quantizer
design, which alternate between two necessary conditions for
optimality: the nearest–neighbor condition, according to which
s(x) should be the reproduction level that is closest to x
and the centroid condition, i.e., xi should be the conditional
ˆ
expectation of X given that X falls in the interval of values
of x that are to be mapped to the i–th quantization level.
Consider the maximization of the negative exponential
2
moment E P e−α[X−s(X)] . Here the centroid condition is
no longer relevant. However, one can use the fact that this
∆
problem is equivalent to the double minimization of G(s, Q) =
αE Q [X − s(X)]2 + D(Q P ) over s and Q. This suggests
an iterative algorithm that consists of two nested loops: The
outer loop alternates between minimizing s for a given Q,
on the one hand, and minimizing Q for a given s, on the
other hand. The inner loop implements the former ingredient
of minimizing E Q [X − s(X)]2 over s for a given Q, which is
again implementable by the standard iterative procedure that
was described in the previous paragraph.
As a simple example for a combination of s and Q that
are matched (in the above sense), consider the case where
P is the uniform distribution over [−A, +A]. The optimum
MMSE quantizer is uniform as well: The interval [−A, A] is
partitioned evenly into M sub-intervals, each of size 2A/M
and the reproduction level xi , pertaining to each sub-interval,
ˆ
is its midpoint, What happens when the exponential moment
is considered? Let us ‘guess’ that the same quantizer s
remains optimum. Then, Q(x) is proportional to 1{|x| ≤ A} ·
exp{−α mini (x − xi )2 }, which means that Q has “Gaussian
ˆ
peaks” at all reproduction points {ˆi }. It is plausible that
x

(2)

which is also intimately related to the well–known Laplace
principle [7] in large deviations theory.
In view of eq. (2), another look at the problem of minimizing E P exp{α (X, s)} reveals that it is equivalent1 to
∆
the minimax problem mins maxQ F (s, Q) where F (s, Q) =
αE Q (X, s) − D(Q P ). Now, suppose that S and (·, ·) are
such that: mins∈S maxQ F (s, Q) = maxQ mins∈S F (s, Q).
This means that there is a saddle point (s∗ , Q∗ ), where s∗
is a solution of the minimax problem and Q∗ is a solution
to the maximin problem. As mentioned above, the maximizing Q in the inner maximization on the left–hand side
∗
is Q∗ (x) = P (x)eα (x,s ) /Z(s∗ ), which is condition 2 of
Observation 1. By the same token, the inner minimization over
s on the right–hand side obviously minimizes E Q∗ (X, s),
which is condition 1. This means then that the two conditions
of Observation 1 are actually equivalent to the conditions for
a saddle point of F (s, Q).
A related criterion is maxs∈S E exp{−α (X, s)} (again,
with α > 0), which is called a risk–seeking cost criterion.
1 See

(3)

A. Quantization

P (X)
Q(X)
(X, s) − D(Q P )} (1)

(X,s)−D(Q P )]

Q

III. A PPLICATIONS

becomes an equality for Q(x) = P (x)eα (x,s) /Z(s), since for
P (X)
this choice of Q, α (X, s) + ln Q(X) , becomes a constant
with probability one. Since the left–hand side is independent of Q, such an equality in Jensen’s inequality means
that αE Q (X, s) − D(Q P ) is maximized by Q(x) =
P (x)eα(x,s) /Z(s). This leads directly to the well–known
identity (see, e.g., [5, Proposition 2.3]):
E P eα

s

and the optimality conditions relating s and Q are similar to
those of Observation 1, except that now we have a double
minimization problem. However, it should be noted that here
the conditions of Observation 1 are only necessary conditions,
as for the above equalities to hold, the pair (s, Q) should
globally minimize the function F (s, Q), unlike the earlier
case, where only a saddle point was sought. On the other hand,
the advantage is that even if one cannot solve explicitly for the
optimum s, then the double minimization naturally suggests
an iterative algorithm.

≡ E Q exp α (X, s) + ln
≥ exp {αE Q

exp{− min min[αE Q (X, s) + D(Q P )]},

also [5].

2

In other words, we are back to s(x) which means that
our ‘guess’ was successful. Indeed, the MSE of s(x) under
Qθ , which is v T (Λ−1 − 2αvv T )−1 v, can easily be shown
to be identical to the Cram´ r–Rao lower bound under Qθ ,
e
which is given by 1/[uT (Λ−1 − 2αvv T )u]. We can therefore
summarize our conclusion in the following proposition:
Proposition 1: Let X be a Gaussian random vector with
a mean vector θu and a non–singular covariance matrix Λ.
Let a be the supremum of all values of α such that (Λ−1 −
2αvv T ) is positive deﬁnite, where v = Λ−1 u/(uT Λ−1 u).
Then, among all unbiased estimators of θ, the estimator
s(x) = v T x uniformly minimizes the exponential moment
E exp{α[s(X) − θ]2 } for all α ∈ (0, a).

the same uniform quantizer s continues to be optimum (or
at least nearly so) for Q, and hence these s and Q match each
other. Moreover, at least for large α, this quantizer nearly
attains the rate–distortion function of Q at distortion level
D = 1/(2α). To see why this is true, observe that for large α,
the factor exp{−α mini (x− xi )2 } = maxi exp{−α(x− xi )2 }
ˆ
ˆ
is well approximated by i exp{−α(x − xi )2 }, which after
ˆ
normalization of Q, becomes a mixture of M evenly weighted
Gaussians, where the i–th mixture component is centered at xi ,
ˆ
i = 0, 1, . . . , M −1. This mixture can then be viewed as a convolution between the uniform discrete distribution on {ˆi } and
x
the Gaussian density N (0, 1/(2α)). Thus, for D ≤ 1/(2α),
the rate–distortion function of Q agrees with the Shannon
1
lower bound (see [14]), RL (D) = log M + 2 log(1/[2αD]),
which, for D = 1/(2α), gives a coding rate of log M , just
like the aforementioned uniform quantizer.
Finally, consider the case where X = (X1 , . . . , Xn ) is
quantized by a sequential causal quantizer with memory,
i.e., Xt is quantized into one of M quantization levels,
which are allowed to depend on past quantizer outputs
n
ˆ
ˆ
X1 , . . . , Xt−1 . Here, the criterion is E P exp{−α t=1 [Xt −
ˆ
ˆ
s(Xt |X1 , . . . , Xt−1 )]2 }. As is shown in [18], whenever P
is memoryless, the allowed dependence on the past does
not improve the exponential moment performance, i.e., the
optimum quantizer of this type makes use of the current
symbol Xt only. This means that the causal vector quantization
problem actually degenerates back to the scalar quantization
problem considered in the previous paragraphs. This continues
to be true even if variable–rate coding is allowed, except that
then time–sharing between at most two quantizers must also
be allowed. These results are analogous to those of [20] for
the ordinary criterion of expected code length for a given
distortion.

C. Gaussian–Quadratic Joint Source–Channel Coding
Consider the zero–mean Gaussian memoryless source
2
U1 , U2 , . . . with variance σu and the Gaussian memoryless
2
channel y = x + z, where the noise is N (0, σz ). In the
ordinary joint source–channel coding problem, one seeks an
n
1
encoder and decoder that minimize D = n i=1 E{(Ui −
2
Vi ) }, where V = (V1 , . . . , Vn ) is the reconstruction. It
is well known that the minimum achievable distortion is
2
2
D = σu /(1 + Γ/σz ), where Γ is the maximum power allowed
and it may be achieved by a transmitter that simply ampliﬁes
2
the source by a gain factor of Γ/σu and a receiver that
implements linear MMSE estimation of Ui given Yi , on a
symbol–by–symbol basis.
What if we replace the expected distortion by the criterion
E exp{α i (Ui −Vi )2 }? Are the simple linear transmitter and
receiver of the previous paragraph, still optimum? Our strategy
s consists of the choice of an encoding function x = f (u)
and a decoding function v = g(y). The class S is then the set
of all pairs of functions {f, g}, where f satisﬁes the power
constraint E P { f (U ) 2 } ≤ nΓ. Condition 2 of Observation
1 tells us that the modiﬁed probability distribution of u and
z should be of the form
Pn
2
Q(u, z) ∝ P (u)P (z)eα i=1 [ui −gi (f (u)+z )]
(5)

B. Non–Bayesian Parameter Estimation
Let X = (X1 , X2 , . . . , Xn )T be a Gaussian random vector
with mean vector θu, where θ ∈ IR and u = (u1 , . . . , un )T
is a given deterministic vector. Let the non–singular n × n
covariance matrix of X be given by Λ. It is well known that
for this kind of a model, among all unbiased estimators of θ,
the one that minimizes the mean square error is the maximum
likelihood (ML) estimator, which in this case, is given by
s(x) = uT Λ−1 x/uT Λ−1 u. Does this estimator also minimize
E exp{α[s(X) − θ]2 } among all unbiased estimators and for
all values of α in the allowed range?
Let us ‘guess’ that this estimator indeed minimizes also
E exp{α[s(X) − θ]2 } and then check whether it satisﬁes the
conditions of Observation 1. Denoting v = Λ−1 u/(uT Λ−1 u),
the corresponding density Q, which will be denoted here by
1
Qθ , is proportional to exp{− 2 (x − θu)T Λ−1 (x − θu) +
1
T
2
α(v x − θ) }, or equivalently to exp{− 2 (x − θu)T (Λ−1 −
T
2αvv )(x − θu)}, where α is chosen small enough such
that (Λ−1 − 2αvv T ) is still positive deﬁnite. Now, the ML
estimator of θ under Qθ is given by (see [14] for details):
uT (Λ−1 − 2αvv T )x
sQ (x) = T −1
= s(x)
u (Λ − 2αvv T )u

U

Z

where gi is restriction of g to the i–th component of v. Clearly,
if we continue to restrict the encoder f to be linear, with a gain
2
of Γ/σu and the only remaining degree of freedom is the decoder g, then we are basically dealing with a problem of pure
Bayesian estimation, and then the optimum decoder continues
to be the same linear decoder as before (see [19]).2 However,
once we extend the scope and allow f be a non–linear encoder,
then the optimum f and g would no longer remain linear like
in the expected distortion case. It is not difﬁcult to see that
the conditions of Observation 1 are no longer met for any
linear functions f and g. The key reason is that while Q(u, z)
of eq. (5) continues to be Gaussian (though now Ui and Zi
are correlated) when f and g are linear, the power constraint,
E P { X 2 } ≤ nΓ, when expressed as an expectation w.r.t.
Q, becomes E Q { f (U ) 2 P (U )/Q(U )} ≤ nΓ, but “power”
2 This can also be obtained by applying Observation 1 to the problem of
Bayesian estimation in the Gaussian regime under the exponential moment
criterion. See Section 3.2 in the original version of this paper [14].

(4)

3

function f (u) 2 P (u)/Q(u), with P and Q being Gaussian
densities, is no longer the usual quadratic function of f (u) for
which there is a linear encoder and decoder that is optimum.
More details on the optimum encoder and decoder can be
found in [14].

exponentially tight, it is no longer necessary to make the
random variable α (X, s) + ln[P (X)/Q(X)] completely degenerate (i.e., a constant for every realization x, as in condition
2 of Observation 1), but it is enough to keep it essentially ﬁxed
across a considerably large subset of the dominant type class,
TQ∗ , i.e., the one whose empirical distribution Q∗ essentially
achieves the maximum of [αλ(Q) − D(Q P )]. Taking Q∗ (x)
to be the memoryless source induced by the dominant Q∗ , this
is precisely what happens under conditions (a) and (b), which
imply that

IV. U NIVERSAL A SYMPTOTICALLY O PTIMUM S TRATEGIES
The optimum strategy for minimizing E P exp{α (X, s)}
depends, in general, on both P and α. It turns out, however,
that this dependence can sometimes be relaxed if one gives
up the quest for a strictly optimum strategy, and resorts to
asymptotically optimum strategies.
Consider the case where we have a random vector X =
n
(X1 , . . . , Xn ), governed by P (x) = i=1 P (xi ), where each
xi takes on values in a ﬁnite set X . If (x, s) grows linearly
with n for a given empirical distribution of x and a given
s ∈ S, then it is expected that the exponential moment
E exp{α (x, s)} would behave, at least asymptotically, as an
exponential function of n. In particular, for a given s, let us
1
assume that the limit limn→∞ n ln E P exp{α (X, s)} exists.
Let us denote this limit by E(s, α, P ). An asymptotically
optimum strategy is a strategy s∗ for which E(s∗ , α, P ) ≤
E(s, α, P ) for every s ∈ S. An asymptotically optimum
strategy s∗ is called universal asymptotically optimum w.r.t.
a class P of probability distributions, if s∗ is independent of
α and P , yet it is asymptotically optimum for all α in the
allowed range, every s ∈ S, and every P ∈ P. Here, we take
P to be the class of all memoryless sources with a given ﬁnite
alphabet X , We denote by TQ the set of all x ∈ X n whose
empirical distribution is Q.
Suppose there exists a strategy s∗ and a function λ : P → IR
such that following two conditions hold: (a) For every TQ
and every x ∈ TQ , (x, s∗ ) ≤ n[λ(Q) + o(1)], where o(1)
designates a (positive) sequence that tends to zero as n → ∞.
(b) For every TQ and every s ∈ S,
TQ ∩ {x :

α (x, s∗ ) + ln

P (x)
Q∗ (x)

n

≈ nαλ(Q∗ ) +

ln
i=1

P (xi )
Q∗ (xi )

= n[αλ(Q∗ ) − D(Q∗ P )], (7)
for (at least) a non–exponential fraction of the members
of TQ∗ , namely, a subset of TQ∗ that is large enough to
maintain the exponential order of the contribution of TQ∗ to
E exp{α (x, s∗ )}. The combination of conditions (a) and (b)
also means that s∗ is essentially optimum for TQ∗ , which is
a reminiscence of condition 1 of Observation 1. Moreover,
since s∗ “adapts” to TQ , this has the ﬂavor of the maximin
problem of Section 2, where s is optimized for each Q. Since
the minimizing s in the maximin problem is independent of
P and α, this also explains the universality property.
The ﬁrst example is that of ﬁxed–rate rate–distortion coding.
A vector X from a memoryless source P is encoded by s with
ˆ
respect to a given additive distortion measure d : X × X → IR,
ˆ being the reconstruction alphabet. Let DQ (R) denote the
X
distortion–rate function of a source Q (with alphabet X )
relative to d and let (x, s) designate the distortion between x
and its reproduction. This example meets conditions (a) and
(b) with λ(Q) = DQ (R): Condition (a) is based on the type
covering lemma according to which TQ can be completely
covered by essentially enR ‘spheres’ of radius nDQ (R),
centered at the reproduction vectors. Thus, s∗ encodes x in
two parts, the ﬁrst is a header that describes the index of TQ
of x (whose description length is O(log n)) and the second
part encodes the index of the codeword within TQ , using nR
nats. Condition (b) is met since there is no way to cover TQ
with exponentially less than enR spheres within distortion less
than DQ (R).
Similarly, consider variable–rate coding within maximum
distortion D. In this case, every x is encoded by (x, s)
nats, and this time, conditions (a) and (b) apply with λ(Q) =
RQ (D), the rate–distortion function of Q. The considerations
are similar to those of the ﬁrst example. It is interesting to
particularize this to the case D = 0, where RQ (0) = H(Q),
the empirical entropy associated with Q. Here, a more reﬁned
result can be obtained, which extends the main result of [21]:
Given a length function of a lossless data compression (x, s)
(s being the data compression scheme), and given a parametric
n
class of sources of n–vectors, {Pθ }, indexed by a parameter
k
n
θ ∈ Θ ⊂ IR , a lower bound on E Pθ (X, s), that applies to

(x, s) ≥ n[λ(Q) − o(1)]} ≥ e−no(1) |TQ |. (6)

It is then straightforward to show that s∗ is a universal
asymptotically optimum strategy w.r.t. P, with E(s∗ , α, P ) =
maxQ [αλ(Q) − D(Q P )], where condition (a) supports the
direct part and condition (b) supports the converse part. The
interesting point here is not in the last statement, but in the
fact that there are many application examples where these two
conditions hold at the same time.
Before we provide such examples, a few words are in order
concerning conditions (a) and (b). Condition (a) means that
there is a choice of s∗ , that does not depend on x or on its
type class, yet the performance of s∗ , for every x ∈ TQ ,
“adapts” to the empirical distribution Q of x essentially
optimally (i.e., cannot be improved signiﬁcantly), at least for a
considerable (non–exponential) fraction of the members of TQ .
It is instructive to relate conditions (a) and (b) to conditions 1
and 2 of Observation 1. First, observe that in order to guarantee
asymptotic optimality of s∗ , condition 2 of Observation 1 can
be somewhat relaxed: For Jensen’s inequality in (??) to remain

4

exists a universal lossless data compression code s∗ , whose
length function (x, s∗ ) satisﬁes the reversed inequality where
the factor (1 − ) is replaced by (1 + ).

most3 values of θ, is given by
k
n
n
E Pθ (X, s) ≥ H(Pθ ) + (1 − ) log n,
(8)
2
n
where
> 0 is arbitrarily small (for large n), H(Pθ ) is
n
n {·} is the
the entropy of X associated with Pθ , and E Pθ
n
expectation under Pθ . On the other hand, the same expression
is achievable, by a number of universal coding schemes,
provided that the factor (1 − ) in the above expression is
replaced by (1 + ).
Considering now the exponential moment criterion, as
a lower bound, we have (see [14] for a full proof):
n
n
ln E Pθ exp{α (X, s)} ≥ αH1/(1+α) (Pθ ) + α(1 − ) k log n,
2
n
where Hu (Pθ ) is R´ nyi’s entropy of order u. Consider now
e
n
the case where {Pθ , θ ∈ Θ} is the class of all memoryless
sources over X , where the parameter vector θ designates
k = |X |−1 letter probabilities. In this case, since the source is
completely deﬁned by the single–letter probabilities, we can
n
omit the superscript n of Pθ and denote the source by Pθ .
∗
Deﬁne a two–part code s , which ﬁrst encodes the index of
TQ and then the index of x within TQ . Then, (x, s∗ ) =
ˆ
ˆ
ln |TQ | + k log n ≈ nH(x) + k log n, where H(x) is the
2
empirical entropy pertaining to x, and where the approximate
inequality is easily obtained by the Stirling approximation.
Then, as can be seen in [14]: ln E Pθ exp{α (X, s)} ≤
nαH1/(1+α) (Pθ )+α k log n. Rissanen’s result is now obtained
2
a special case by dividing both sides by α and taking the limit
α → 0.
We next summarize these ﬁndings in the form of a theorem,
which is an exponential–moment counterpart of [21, Theorem
1]. The converse part (part (a)) can actually be extended to
even more general classes of sources, which are not even
necessarily parametric, using the results of [16], where the
expression (k log n)/(2n) is replaced by the capacity of the
“channel” from θ to X, as deﬁned by the class of sources
{Pθ } when viewed as a set of conditional distributions of X
given θ. For simplicity, the direct part (part (b)) of this theorem
is formalized for the class of all memoryless sources with a
given ﬁnite alphabet, parametrized by the letter probabilities,
but it can also be extended to wider classes of sources, like
Markov sources of a given order.
n
Theorem 1: Converse part: Let P = {Pθ , θ ∈ Θ} be
a parametric class of ﬁnite–alphabet memoryless sources,
indexed by a parameter θ that takes on values in a compact
subset Θ of IRk . Let the central limit theorem hold, under Qn ,
θ
for the ML estimator of each θ in the interior of Θ. If (x, s)
is a length function of a code s satisfying the Kraft inequality,
1
n
then for every α > 0 and > 0, nα ln E Pθ exp{α (X, s)}
n
is lower bounded by H1/(1+α) (Pθ ) + (1 − )k(log n)/(2n)
for all θ ∈ Θ except for a set A (n) ⊂ Θ whose Lebesgue
measure vanishes as n → ∞.
Direct part: For the case where P is the class of all memoryless sources with a given alphabet of size k + 1 and θ
designates the vector of the k ﬁrst letter probabilities, there

ACKNOWLEDGMENT
Interesting discussions with Rami Atar are acknowledged
with thanks.
R EFERENCES
[1] E. Arikan and N. Merhav, “Guessing subject to distortion,” IEEE Trans.
Inform. Theory, vol. 44, no. 3, pp. 1041–1056, May 1998.
[2] R. Atar, P. Dupuis, and A. Shwartz, “An escape–time criterion for queueing networks: asymptotic risk–sensitive control via differential games,”
Mathemtics of Operation Research, vol. 28, no. 4, pp. 801–835, November
2003.
[3] D. Bertsekas, Dynamic Programming and Optimal Control, Vol. I and II,
3rd ed. Nashua, NH: Athena Scientiﬁc, 2007.
[4] J. P. N. Bishwal, “Large deviations and Berry–Esseen inequalities for
estimators in nonlinear nonhomogeneous diffusions,” REVSTAT Statistical
Journal, vol. 5, no. 3, pp. 249–267, November 2007.
[5] P. Dai Pra, L. Meheghini, and W. J. Runggaldier, “Connections between
stochastic control and dynamic games,” Mathematics of Control, Signals
and Systems, vol. 9, pp. 303–326, 1996.
[6] G. B. Di Masi and L. Stettner, “Risk–sensitive control of discrete–time
Markov processes with inﬁnite horizon,” SIAM Journal on Control and
Optimization, vol. 38, no. 1, pp. 61–78, 1999.
[7] P. Dupuis and R. S. Ellis, A Weak Convergence Approach to the Theory
of Large Deviations, John Wiley & Sons, 1997.
[8] W. H. Fleming and D. Hern´ ndez–Hern´ ndez, “Risk–sensitive control of
a
a
ﬁnite state machines on an inﬁnite horizon I,” SIAM Journal on Control
and Optimization, vol. 35, no. 5, pp. 1790–1810, 1997.
[9] R. M. Gray, Source Coding Theory, Kluwer Academic Publishers, Norwell, MA, U.S.A., 1990.
[10] P. A. Humblet, “Generalization of Huffman coding to minimize the
probability of buffer overﬂow,” IEEE Transactions on Information Theory,
vol. IT–27, no. 2, pp. 230–232, March 1981.
[11] F. Jelinek, “Buffer overﬂow in variable length coding of ﬁxed rate
sources,” IEEE Transactions on Information Theory, vol. IT–14, no. 3,
pp. 490–501, May 1968.
[12] A. D. M. Kester and W. C. M. Kallenberg, “Large deviations of
estimators,” Annals of Mathmatical Statistics, vol. 14, no. 2, pp. 648–
664, 1986.
[13] Y. Linde, A. Buzo, and R. M. Gray, “An algorithm for vector quantizer
design,” IEEE Transactions on Communications, vol. COM–28, no. 1,
pp. 84–95, January 1980.
[14] N.
Merhav,
“On
optimum
strategies
for
minimizing
the
exponential
moments
of
a
given
cost
function,”
http://arxiv.org/PS cache/arxiv/pdf/1103/1103.2882v1.pdf
[15] N. Merhav and E. Arikan, “The Shannon cipher system with a guessing
wiretapper,” IEEE Trans. Inform. Theory, vol. 45, no. 6, pp. 1860–1866,
September 1999.
[16] N. Merhav and M. Feder, “A strong version of the redundancy–capacity
theorem of universal coding,” IEEE Trans. Inform. Theory, vol. 41, no.
3, pp. 714-722, May 1995.
[17] N. Merhav and M. Feder, “Universal prediction,” IEEE Trans. Inform.
Theory, vol. 44, no. 6, pp. 2124–2147, October 1998.
[18] N. Merhav and I. Kontoyiannis, “Source coding exponents for zero–
delay coding with ﬁnite memory,” IEEE Trans. Inform. Theory, vol. 49,
no. 3, pp. 609–625, March 2003.
[19] J. B. Moore, R. J. Elliott, and S. Dey, “Risk–sensitive generalizations
of minimum variance estimation and control,” Journal of Mathematical
Systems and Control, vol. 7, no. 1, pp. 1–15, 1997.
[20] D. L. Neuhoff and R. K. Gilbert, “Causal source codes,” IEEE Trans. Inform. Theory, vol. IT–28, no. 5, pp. 701–713, September 1982.
[21] J. Rissanen, “Universal coding, information, prediction, and estimation,”
IEEE Transactions on Information Theory, vol. IT–30, no. 4, pp. 629–636,
July 1984.
[22] A. D. Wyner, “On the probability of buffer overﬂow under an arbitrary
bounded input-output distribution,” SIAM Journal on Applied Mathematics, vol. 27, no. 4, pp. 544–570, December 1974.

3 “Most values of θ” means all values of θ with the possible exception of
a subset of Θ whose Lebesgue measure tends to zero as n tends to inﬁnity.

5


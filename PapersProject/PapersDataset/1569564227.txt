Creator:         TeX output 2012.05.11:1728
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 11 17:28:34 2012
ModDate:        Tue Jun 19 12:54:13 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      285185 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564227

A General Formula of Rate-Distortion Functions for
Source Coding with Side Information at Many
Decoders
Tetsunao Matsuta∗ , Tomohiko Uyematsu†
Department of Communications and Integrated Systems
Tokyo Institute of Technology
Ookayama, Meguro-ku, Tokyo, 152-8550, Japan
Email:tetsu@it.ss.titech.ac.jp∗ , uyematsu@ieee.org†
side information. For this model, Heegard and Berger [4]
showed the single-letterized upper bound of the rate-distortion
function in the case where the source is stationary memoryless.
Although their bound is optimal for some cases, it is unknown
whether the bound is optimal in general. Hence, the problem
of clarifying the rate-distortion function is still open.
Recently, in order to deal with general sources and channels
which are not required to satisfy ergodicity and stationarity,
Han and Verd´ developed information-spectrum methods (see
u
[5], [6]). By using these methods, Iwata and Muramatsu [7]
generalized the rate-distortion theory for Wyner-Ziv source
coding to the case where the source may be nonstationary
and/or nonergodic. They showed the general formula of the
rate-distortion function for Wyner-Ziv source coding with the
maximum distortion criterion. In this paper, we extend their result to source coding with side information at many decoders,
and clarify the rate-distortion function for this source coding
with the maximum distortion criterion. The proof is based on
information-spectrum methods, and Iwata’s and Muramatsu’s
methods used in [7]. The reason that we can clarify the ratedistortion function for this open problem is due to the use
of multi-letterized formula. We also show some special cases
of the rate-distortion function, and a single-letterized upper
bound of the rate-distortion function in the case where source
is stationary memoryless. Our bound differs from the bound
derived by Heegard and Berger. However, the bound is optimal
in a certain case.

Abstract—Heegard and Berger introduced the model of lossy
source coding in which side information is available at many
decoders. For this model, their showed an upper bound of the
rate-distortion function in the case where the source is stationary
memoryless. In this paper, we extend their model to the case
where the source may be nonstationary and/or nonergodic, and
clarify the rate-distortion function for this model. This result
is based on the information-spectrum method introduced by
´
Han and Verdu. We also show some special cases of the ratedistortion function, and a single-letterized upper bound of the
rate-distortion function in the case where the source is stationary
memoryless.

I. I NTRODUCTION
In multi-terminal information theory, various models of
source coding have been considered and analyzed by many
researchers [1], [2]. One of the most important models is
Wyner-Ziv source coding introduced by Wyner and Ziv [3].
This is a model of lossy source coding in the case where side
information is available at the decoder. For Wyner-Ziv source
coding, Wyner and Ziv clariﬁed the rate-distortion function
for stationary memoryless sources, where the rate-distortion
function is the inﬁmum of rates of codes in which the
distortion between the source sequence and the reproduction
sequence satisﬁes a certain ﬁdelity criterion. Later, Heegard
and Berger [4] extended Wyner-Ziv source coding to the
case where there are many decoders and side information
is available at decoders. This model is called source coding
with side information at many decoders. This is illustrated in
Fig.1. In this model, there are one encoder and many decoders.

Fig. 1.

II. P RELIMINARIES
In this section, we provide a precise deﬁnition of source
coding with side information at many decoders.
We will denote random variables (RVs) by capital letters
X,Y, Z, · · · , the values they can take by lowercase letters
x, y, z, · · · , and the set of these values by calligraphic letters
X , Y , Z , · · · . The probability distribution of a RV X taking
values in X will be denoted by PX . In a similar manner, the
probability distribution of a pair of RVs (X,Y ) taking values in
X × Y will be denoted by PXY . The conditional distribution
for X given Y will be denoted by PX|Y . In what follows, all
logarithms and exponentials are taken to the base of natural
logarithm.

Source coding with side information at many decoders

Encoder encodes a source sequence, and sends a codeword to
decoders. Each decoders independently reproduces the source
sequence with a certain distortion from the codeword and

1

ˆ
ˆ
Let X , Y1 , Y2 , X1 and X2 be arbitrary ﬁnite sets. Let
(X, Y1 , Y2 ) denote a correlated general source [5] which is
characterized by {(X n ,Y1n ,Y2n )}∞ , where (X n ,Y1n ,Y2n ) denotes
n=1
a triple of RVs with the joint probability distribution PX nY1nY2n
over X n ×Y1n ×Y2n . When a source is stationary memoryless,
we simply express a source (X, Y1 , Y2 ) as (X,Y1 ,Y2 ), where
(X,Y1 ,Y2 ) denotes a RV with the joint probability distribution PXY1Y2 over X × Y1 × Y2 . X = {X n }∞ represents the
n=1
source sequence encoded by the encoder. Y1 = {Y1n }∞ and
n=1
Y2 = {Y2n }∞ represent side information which are available
n=1
at decoder 1 and decoder 2, respectively. Then, we can deﬁne
the encoder as

Our main result is the following theorem.
Theorem 1. For a correlated source (X, Y1 , Y2 ), D1 ≥ 0 and
D2 ≥ 0,
R f m (D1 , D2 |X, Y1 , Y2 ) = inf {I(X; Z) − min I(Z; Yk )},
Z∈S

(k)
ˆ
ˆ
ˆn
where D(X, Xk ) = p- lim sup dn (X n , Xk ) and Xk =

fn : X → Mn = {1, 2, · · · , Mn }
n

(k)

Source coding with side information at many decoders
includes some special cases. One of special cases is source
coding when side information may be absent [4], [8]. This
is a model of lossy source coding in which the encoder
encodes a sequence of source X = {X n }∞ in preparation
n=1
for the absence of side information Y = {Y n }∞ . Then,
n=1
the decoder reproduces the source sequence with distortion
D1 when side information is present, and reproduces the
source sequence with distortion D2 when side information
is absent. Just as in Section II, we can deﬁne the f m-rate
distortion function RHBK (D1 , D2 |X, Y) for this source coding
fm
with obvious modiﬁcation. Then, according to Theorem 1, we
have the next corollary.

k

For each decoder, the distortion between the source sequence
(k)
ˆn
X n and the reproduction sequence Xk = ϕn (Ykn , fn (X n )) is
measured by a distortion measure
(k)
ˆ
dn : X n × X n → [0, ∞), k = 1, 2.
k

Then, we call (R, D1 , D2 ) is f m-achievable when there exists
(1)
(2)
a sequence of codes {( fn , ϕn , ϕn )} such that
p- lim sup dn (X n , ϕn (Ykn , fn (X n ))) ≤ Dk , ∀ k = 1, 2
(k)

(k)

n→∞

and

Corollary 1. For a correlated source (X, Y), D1 ≥ 0 and D2 ≥
0,

lim sup Rn ≤ R,
n→∞

RHBK (D1 , D2 |X, Y) =
fm

where “p- lim sup” is a notion deﬁned in the following deﬁnition [5].

inf

Z∈S HBK

I(X; Z),

where S HBK is a set of sequences of RVs Z = {Z n }∞
n=1
satisfying the following conditions:
1) Y n ↔ X n ↔ Z n , ∀ n = 1, 2, · · · .
(1)
(2)
ˆ
2) For some functions ψn : Y n × Z n → X1n and ψn :
n → X n,
ˆ
Z
2
ˆ
D(X, Xk ) ≤ Dk ∀ k = 1, 2,

Deﬁnition 1 (Limit superior and inferior in probability). For
an arbitrary sequence of real-valued RVs {Zn }∞ , we deﬁne
n=1 }
{
p- lim sup Zn inf α : lim Pr {Zn > α } = 0 ,
n→∞
n→∞
{
}
p- lim inf Zn sup β : lim Pr {Zn < β } = 0 .
n→∞

n→∞

{ψn (Ykn , Z n )}∞ .
n=1

and deﬁne the rate Rn of the encoder as
1
Rn
log Mn .
n
Decoder k can be deﬁned as
(k)
ˆ
ϕn : Y n × Mn → X n , k = 1, 2.
k

k∈{1,2}

where S is a set of sequences of RVs Z = {Z n }∞ satisfying
n=1
the following conditions:
1) (Y1n ,Y2n ) ↔ X n ↔ Z n , ∀ n = 1, 2, · · · .
(k)
ˆ
2) For some functions ψn : Ykn × Z n → Xkn ,
∀
ˆ
D(X, Xk ) ≤ Dk
k = 1, 2,

n→∞

where

We now deﬁne f m-rate distortion function as
R f m (D1 , D2 |X, Y1 , Y2 )
= inf{R : (R, D1 , D2 ) is f m-achievable}.

ˆ
D(X, Xk )

=

(k)
ˆn
p- lim sup dn (X n , Xk ),
n→∞

(1)
(2)
ˆ
ˆ
X1 = {ψn (Y n , Z n )}∞ , X2 = {ψn (Z n )}∞
n=1
n=1 and
(k)
n × X n → [0, ∞) is a distortion measure.
ˆ
dn : X
k

Our purposed is to clarify the f m-rate distortion function. To
this end, we introduce following notations [5].

Proof: Source coding when side information may be
absent can be seen as a special case of source coding with side
information at many decoders via the following associations:
Y1 = Y and Y2 = constant. Then, by applying Theorem 1, we
have

Deﬁnition 2. For a given sequence of RVs (X, Y) =
{(X n ,Y n )}∞ , we deﬁne
n=1
PY n |X n (Y n |X n )
1
,
I(X; Y) p- lim sup log
PY n (Y n )
n→∞ n
PY n |X n (Y n |X n )
1
I(X; Y) p- lim inf log
.
n→∞ n
PY n (Y n )

R f m (D1 , D2 |X, Y) =
=

inf {I(X; Z) − min{I(Z; Y), 0}}

Z∈S

inf I(X; Z).

Z∈S
= S HBK , we

By noting that S
have the corollary.
Another special case is Wyner-Ziv source coding. In WynerZiv source coding, an encoder encodes a sequence of source
X = {X n }∞ and sends a codeword to a decoder. The decoder
n=1

III. M AIN RESULT AND SPECIAL CASES
In this section, we show a general formula of f m-rate
distortion function, and show some special cases.

2

(2)

+ Pr{(X n ,Y2n , Z n ) ∈ Tn }.
/

reproduces the source sequence with distortion D by using
side information Y = {Y n }∞ . Then, by deﬁning the f m-rate
n=1
distortion function RWZ (D|X, Y) for Wyner-Ziv source coding
fm
just as in Section II, we have the following corollary.

(k)

On the other hand, we have Pr{(X n ,Ykn , Z n ) ∈ An } → 0
/
n , Z n ) ∈ B (k) } → 0 as n → ∞. Thus, we have
and Pr{(Yk
/ n
(k)
Pr{(X n ,Ykn , Z n ) ∈ Tn } = 0 as n → ∞. This implies that
/
n ,Y n ,Y n , Z n ) = 1} → 0 as n → ∞. According to this
Pr{φn (X 1 2
fact, i.e., Pr{φn (X n ,Y1n ,Y2n , Z n ) = 1} → 0 as n → ∞, we can
show the existence of a sequence {gn }∞ of functions gn :
n=1
˜
X n → {zi }Mn ⊆ Z n such that
i=1
˜
Mn = en(I(X;Z)+γ ) ,
(2)

Corollary 2 (Theorem 1 in [7]). For a correlated source (X, Y)
and D ≥ 0,
RWZ (D|X, Y) =
fm

inf {I(X; Z) − I(Z; Y)},

Z∈S WZ

where S WZ is a set of sequences of RVs Z = {Z n }∞
n=1
satisfying the following conditions:
1) Y n ↔ X n ↔ Z n , ∀ n = 1, 2, · · · .
ˆ
2) For some functions ψn : Y n × Z n → X n ,
ˆ
D(X, X) ≤ D,
ˆ
ˆ
D(X, X) = p- lim sup dn (X n , X n ),

where

and
lim Pr{φn (X n ,Y1n ,Y2n , gn (X n )) = 1} = 0.

n→∞

(3)

The existence of this function can be proved by using the next
lemma.

ˆ
X =

n→∞

n
ˆn
{ψn (Y n , Z n )}∞
n=1 and dn : X × X → [0, ∞) is a
distortion measure.

Lemma 1 (Lemma 1 in [7]). Let U n , V n and W n be RV which
take values in ﬁnite sets U n , V n and W n , respectively, and
satisfy a Markov condition U n ↔ V n ↔ W n for each n. Now,
let {φn }∞ be a sequence of arbitrary functions such that
n=1

Proof: For source coding with side information at many
decoders, we consider the following associations: Y1 = Y,
(1)
Y2 = X, dn = dn , D1 = D and D2 = ∞. Then, the f mrate distortion function for this case is equal to the f m-rate
distortion function for Wyner-Ziv source coding. Thus, by
applying Theorem 1, we have

φn : U n × W n → {0, 1}
and
lim Pr{φn (U n ,W n ) = 1} = 0.

n→∞

Then, for any ε > 0, there exists a sequence {gn }∞ of
n=1
˜
functions gn : V n → {wi }Mn ⊆ W n such that
i=1
˜
Mn = en(I(V;W)+γ ) ,

RWZ (D|X, Y) = inf {I(X; Z) − min{I(Z; Y), I(Z; X)}}
fm
Z∈S

= inf {I(X; Z) − I(Z; Y)},
Z∈S

where the last equality comes form the data processing theorem [6, Theorem 9]. Hence, by noting that S = S WZ , we
have the corollary.

and
lim Pr{φn (U n , gn (V n )) = 1} = 0.

n→∞

According to this lemma, by letting U = X ×Y1 ×Y2 , V =
X , and W = Z , we can show the existence of a sequence
˜
{gn }∞ of functions gn : X n → {zi }Mn ⊆ Z n satisfying (2)
n=1
i=1
and (3).
Now, we construct an encoder and decoders. These are
constructed by using the above function gn as follows:
Codebook generation: We deﬁne C˜n as
C˜n {gn (xn ) ∈ Z n : xn ∈ X n },

IV. P ROOF OF T HEOREM 1
In this section, we prove Theorem 1. Since the proof of the
converse part is similar to that of Theorem 1 in [7], we only
show the direct part.
(k)
Let us ﬁx Z and ψn which satisfy conditions in Theorem
1. For an arbitrarily constant γ > 0, we deﬁne φn : X n × Y1n ×
Y2n × Z n → {0, 1} as
{
(k)
1, if (xn , yn , zn ) ∈ Tn , ∃ k = 1, 2,
/
n n n n
k
φn (x , y1 , y2 , z )
0, else.
where
(k)

(k)

Tn

and set Mn as
Mn = en(I(X;Z)−mink∈{1,2} I(Z;Yk )+3γ ) .
Then, we make Mn bins, and randomly and independently
assign all zn ∈ C˜n into Mn bins according to the uniform
probability distribution over {1, 2, · · · , Mn }. We denote by bn :
Z n → {1, 2 · · · , Mn } the bin function, where bn (zn ) denotes
the index of the bin to which zn belongs.
Encoding: Observing a source sequence xn ∈ X n , encoder
sends the index bn (gn (xn )) as the codeword of xn . Hence, the
encoder is deﬁned as

(k)

=An ∩ (X n × Bn ),
{
(k)
An
(xn , yn , zn ) ∈ X n × Ykn × Z n :
k

(k)

Bn

}
(k)
(k)
ˆ
dn (xn , ψn (yn , zn )) ≤ D(X, Xk ) + γ ,
k
{
PY n |Z n (yn |zn )
1
k
(yn , zn ) ∈ Ykn × Z n : log k
k
n
PYkn (yn )
k
}
≥ min I(Z; Yk ) − γ .
k∈{1,2}

fn (xn )

bn (gn (xn )), ∀ xn ∈ X n .

Decoding: Observing side information yn and the codeword
k
fn (xn ), decoder k (k = 1, 2) ﬁnds a sequence zn ∈ C˜n such that
bn (zn ) = fn (xn ) and (yn , zn ) ∈ B (k) . Then, decoder k declares
k

(1)

Then, we have
(1)

(k)

Pr{φn (X n ,Y1n ,Y2n , Z n ) = 1} ≤ Pr{(X n ,Y1n , Z n ) ∈ Tn }
/

ϕn (yn , fn (xn )) = ψn (yn , zn ).
k
k

3

If there is more than one such zn or there is no such zn , decoder
ˆ
k declares ϕn (yn , fn (xn )) = xk , where xk ∈ Xkn is a certain ﬁxed
ˆn
ˆn
k
sequence.
We consider sequences (xn , yn , yn ) satisfying the following
1 2
conditions:
(1)
1) There is no zn ∈ C˜n such that zn = gn (xn ), (yn , zn ) ∈ Bn
˜
˜
1 ˜
and bn (˜n ) = bn (gn (xn )).
z
(2)
2) There is no zn ∈ C˜n such that zn = gn (xn ), (yn , zn ) ∈ Bn
˜
˜
2 ˜
n ) = b (g (xn )).
and bn (˜
z
n n
(k)
3) (xn , yn , gn (xn )) ∈ Tn , ∀ k = 1, 2.
k
Then, gn (xn ) is a unique element in C˜n such that bn (gn (xn )) =

Thus, we have
Pr{(X n ,Y1n ,Y2n ) ∈ [Sk (Bn )]c }
= ∑ Pr{Bn = bn }
bn

zn ∈C˜n :
˜
zn =gn (xn ),
˜
n ,˜n )∈B (k)
(y z
n
k

(xn ,yn )∈X ×Ykn
k

≤

≤

Pr{Bn = bn }
s.t.

Pr{Bn = bn }

∑

PX nYkn (xn , yn )
k

z ∈Cn :
˜
zn =gn (xn ),
˜
n ,˜n )∈B (k)
(y z
n
k

1
Mn

1
Mn

∑
n ˜

z ∈Cn :
˜
(k)
z
(yn ,˜n )∈Bn
k

yn ∈Y n :
k
k
n ,˜n )∈B (k)
(y z
n
k

∑
n ˜

PYkn |Z n (yn |˜n )
k z
Mn en(mink∈{1,2} I(Z;Yk )−γ )

˜
Mn
n(mink∈{1,2} I(Z;Yk )−γ )

≤

Mn e
≤ e−nγ (1 + e−n(I(X;Z)+γ ) ),

Sk (bn ) {(xn , yn , yn ) ∈ X n × Y1n × Y2n :
1 2
(xn , yn , yn ) satisﬁes the condition k)}, k = 1, 2,
1 2

where (a) comes from (1). Thus, we have

{(xn , yn , yn ) ∈ X n × Y1n × Y2n :
1 2
(xn , yn , yn ) satisﬁes the condition 3)}.
1 2

Pr{(X n ,Y1n ,Y2n ) ∈ [Sk (Bn )]c } → 0, n → ∞, ∀ k = 1, 2.
On the other hand, we have

Note that conditions 1) and 2) depend on the bin function bn ,
(n)
and hence Sk (bn ) depend on bn . We deﬁne Pe (bn ) as
{ (k) n (k) n
(n)
ˆ
Pe (bn ) Pr dn (X , ϕn (Yk , fn (X n ))) > D(X, Xk ) + γ ,
}
∃
k ∈ {1, 2} ,

Pr{(X n ,Y1n ,Y2n ) ∈ [S3 ]c } = Pr{φ (X n ,Y1n ,Y2n , gn (X n )) = 1}.
Since Pr{φ (X n ,Y1n ,Y2n , gn (X n )) = 1} = 0 as n → ∞, we have
Pr{(X n ,Y1n ,Y2n ) ∈ [S3 ]c } → 0, n → ∞.
Hence, there exists a sequence of bin functions {bn }∞ such
n=1
(n)
that Pe (bn ) → 0 as n → ∞, i.e., there exists a sequence of
(1)
(2)
codes {( fn , ϕn , ϕn )}∞ such that
n=1
{ (k) n (k) n
ˆ
Pr dn (X , ϕn (Yk , fn (X n ))) > D(X, Xk ) + γ ,
}
∃
k ∈ {1, 2} → 0, n → ∞.

(1)
(2)
( fn , ϕn , ϕn )

where the code
is deﬁned by the bin function
bn as the above code construction. We consider the average
(n)
probability of Pe (bn ) with respect to the random bin selection, i.e.,
(bn ),

This implies that this sequence of codes also satisﬁes
{ (k)
}
(k)
ˆ
Pr dn (X n , ϕn (Ykn , fn (X n ))) > D(X, Xk ) + γ → 0,

bn

where the RV Bn denotes the random bin function. Then,
according to properties of sequences (xn , yn , yn ) in S1 (bn ) ∩
1 2
S2 (bn ) ∩ S3 , we have

∑

∑

zn ∈C˜n
˜

where S1 (bn ), S2 (bn ) and S3 is deﬁned by

(n)

∑ n PYkn (yn )
k
n

yk ∈Yk
(a)

∑

∑

bn :∃ zn ∈C˜n
˜
zn =gn (xn ),
˜
bn (˜n )=bn (gn (xn )),
z
(k)
(yn ,˜n )∈Bn
z
k

bn :bn (˜n )=bn (gn (xn ))
z

∑n

=

where the inequality comes from the fact that (xn , yn , gn (xn )) ∈
k
(k)
An from condition 3). Hence, any sequences (xn , yn , yn ) ∈
1 2
S1 (bn ) ∩ S2 (bn ) ∩ S3 satisﬁes
(k)
(k)
ˆ
dn (xn , ϕn (yn , fn (xn ))) ≤ D(X, Xk ) + γ , ∀ k = 1, 2,
k

∑bn Pr{Bn = bn }Pe (bn ) ≤

∑

·

PX nY1nY2n (xn , yn , yn )
1 2

PX nYkn (xn , yn )
k

(xn ,yn )∈X n ×Ykn
k

(k)
(k)
ˆ
dn (xn , ϕn (yn , fn (xn ))) ≤ D(X, Xk ) + γ , ∀ k = 1, 2,
k

(n)

∑

≤

Then, we have

∑ Pr{Bn = bn }Pe

PX nYkn (xn , yn )
k

(xn ,yn )∈X n ×Ykn
k

(k)
fn
and (yn , gn (xn )) ∈ Bn , ∀ k = 1, 2. This is because that
k
(k)
gn (xn ) satisﬁes (yn , gn (xn )) ∈ Bn from condition 3), and
k
(k)
there is no zn ∈ C˜n except for gn (xn ) such that (yn , zn ) ∈ Bn
˜
k ˜
n ) = f (xn ) from condition 1) and 2). Thus, when
and bn (˜
z
n
(xn , yn , yn ) satisﬁes the above conditions, decoders declare
1 2
(k)
ϕn (yn , fn (xn )) = ψn (yn , gn (xn )), ∀ k = 1, 2.
k
k

S3

∑

=

(xn )

∑

(xn ,yn ,yn )∈[Sk (bn )]c
1 2

n → ∞, ∀ k = 1, 2.
Thus, for any ﬁxed γ > 0, the sequence of codes
(1)
(2)
{( fn , ϕn , ϕn )}∞ satisﬁes
n=1
(k)
(k)
ˆ
p- lim sup dn (X n , ϕn (Ykn , fn (X n ))) ≤ D(X, Xk ) + γ

Pr{(X n ,Y1n ,Y2n ) ∈ [Sk (Bn )]c }

k∈{1,2}

+ Pr{(X n ,Y1n ,Y2n ) ∈ [S3 ]c },

n→∞

where [·]c denotes the complement of a set. [Sk (Bn )]c can be
described as
[Sk (Bn )]c ={(xn , yn , yn ) ∈ X n × Y1n × Y2n : ∃ zn ∈ C˜n s.t.
˜
1 2

≤ Dk + γ , ∀ k = 1, 2.
Furthermore, it trivially holds that
lim sup Rn ≤ I(X; Z) − min I(Z; Yk ) + 3γ .

(k)

z
zn = gn (xn ), (yn , zn ) ∈ Bn , Bn (˜n ) = Bn (gn (xn ))}.
˜
k ˜

n→∞

4

k∈{1,2}

Now, we choose a positive sequence {γk }∞ satisfying γ1 >
k=1
γ2 > · · · > 0 and γk → 0 as n → ∞, and repeat the same
argument in the order of γ = γ1 , γ = γ2 , · · · . Then, by using the
diagonal method [5], we can show the existence of a sequence
(1)
(2)
of codes {( fn , ϕn , ϕn )}∞ such that
n=1

1) {(Xi ,Y1i ,Y2i , Zi )}n are i.i.d. according to a probability
i=1
distribution PXY1Y2 Z , ∀ n = 1, 2, · · · , where the marginal
distribution PXY1Y2 is equal to the distribution of the
source (X,Y1 ,Y2 ).
2) (Y1n ,Y2n ) ↔ X n ↔ Z n , ∀ n = 1, 2, · · · .
ˆ
3) For some functions ψ (k) : Yk × Z → Xk ,
ˆ
D(X, Xk ) ≤ Dk ∀ k = 1, 2,

p- lim sup dn (X n , ϕn (Ykn , fn (X n ))) ≤ Dk , ∀ k = 1, 2
(k)

(k)

n→∞

and
lim sup Rn
n→∞

where

≤ I(X; Z) − min I(Z; Yk ),

for a given Z ∈ S . Therefore, we conclude that
R f m (D1 , D2 |X, Y1 , Y2 ) ≤ inf {I(X; Z) − min I(Z; Yk )}.
k∈{1,2}

V. S TATIONARY MEMORYLESS SOURCE
In this section, we show an upper bound of the ratedistortion function in the case where a source is stationary
memoryless.
(k)
Let the distortion measure dn be additive, i.e.,
n
1
(k)
dn (xn , xk ) = ∑ d (k) (xi , xik ), k = 1, 2,
ˆn
ˆ
n i=1
ˆ
where d (k) : X × Xk → [0, ∞) is a bounded function. Then,
we have the following theorem.

Z∈S

k∈{1,2}

≤ inf {I(X; Z) − min I(Z; Yk )}. (4)
Z∈S

k∈{1,2}

According to the condition 1) of the set S , we have
I(X; Z) = I(X; Z),
I(Z; Yk ) = I(Z;Yk ),
where (X,Y1 ,Y2 , Z) is the RV with the probability distribution
PXY1Y2 Z . Hence, according to (4), we have
R f m (D1 , D2 |X,Y1 ,Y2 ) ≤ inf {I(X; Z) − min I(Z;Yk )}
Z∈S

k∈{1,2}

(a)

= inf max I(X; Z|Yk ),
Z∈S k∈{1,2}

max I(X; Z|Yk ),

(5)

where (a) comes from the fact that (Y1 ,Y2 ) ↔ X ↔ Z. On the
other hand, in the condition 3), we have
ˆ
ˆ
D(X, Xk ) = E[d (k) (X, Xk )],

Z∈Sml k∈{1,2}

where Sml is a set of RVs Z satisfying the following conditions:
1) (Y1 ,Y2 ) ↔ X ↔ Z
ˆ
2) For some functions ψ (k) : Yk × Z → Xk ,
(k)
∀
ˆ
E[d (X, Xk )] ≤ Dk
k = 1, 2,

ˆ
where Xk = ψ (k) (Yk , Z). This implies that, for any Z ∈ S ,
¯
there exists a Z ∈ Sml such that
¯
I(X; Z|Yk ) = I(X; Z|Yk ), ∀ k = 1, 2.
(6)

ˆ
where Xk = ψ (k) (Yk , Z).

¯
Conversely, for any Z ∈ Sml , there exists a Z ∈ S satisfying
(6). Thus, we have

Remark 1. In this theorem, we consider the following associations: Y1 = Y , Y2 = X, d (k) = d, D1 = D and D2 = ∞,
ˆ
where (X,Y ) is an RV, d : X × X → [0, ∞) is a bounded
function and D is a positive constant. Then, in the similar
way to the proof of Corollary 2, we obtain the upper bound
of the rate-distortion function of Wyner-Ziv source coding for
the stationary memoryless source (X,Y ) as
RWZ (D|X,Y ) ≤
fm

n→∞

R f m (D1 , D2 |X,Y1 ,Y2 ) = inf {I(X; Z) − min I(Z; Yk )}

Theorem 2. For a stationary memoryless source (X,Y1 ,Y2 ),
D1 ≥ 0 and D2 ≥ 0, the rate-distortion function is upper
bounded as
R f m (D1 , D2 |X,Y1 ,Y2 ) ≤ inf

(k)
ˆn
p- lim sup dn (X n , Xk ),

=

(k)
(k)
ˆ
Xk = {ψn (Ykn , Z n )}∞
and
ψn (Ykn , Z n ) =
n=1
(k) (Y , Z ), · · · , ψ (k) (Y , Z )).
(ψ
k1 1
kn n
We denote this set by S . Since the set S is a subset of the
set S in Theorem 1, we have

k∈{1,2}

Z∈S

ˆ
D(X, Xk )

inf max I(X; Z|Yk ) = inf

Z∈S k∈{1,2}

max I(X; Z|Yk ).

Z∈Sml k∈{1,2}

(7)

Combining (5) and (7), we have the theorem.
R EFERENCES
[1] T. M. Cover, J. A. Thomas, J. Wiley, and W. InterScience, Elements of
Information Theory. Wiley-Interscience New York, 2006.
[2] A. Gamal and Y. Kim, “Lecture notes on network information theory,”
Arxiv preprint arXiv:1001.3404v4, 2010.
[3] A. Wyner and J. Ziv, “The rate-distortion function for source coding with
side information at the decoder,” IEEE Trans. Inform. Theory, vol. 22,
no. 1, pp. 1–10, Jan. 1976.
[4] C. Heegard and T. Berger, “Rate distortion when side information may
be absent,” IEEE Trans. Inform. Theory, vol. 31, no. 6, pp. 727–734, Jun.
1985.
[5] T. S. Han, Information-Spectrum Methods in Information Theory.
Springer, 2003.
[6] S. Verd´ and T. S. Han, “A general formula for channel capacity,” IEEE
u
Trans. Inform. Theory, vol. 40, no. 4, pp. 1147–1157, Jul. 1994.
[7] K. Iwata and J. Muramatsu, “An information-spectrum approach to ratedistortion function with side information,” IEICE Trans. Fundamentals,
vol. E85-A, no. 6, pp. 1063–1070, Jun. 2002.
[8] A. Kaspi, “Rate-distortion function when side-information may be present
at the decoder,” IEEE Trans. Inform. Theory, vol. 40, no. 6, pp. 2031–
2034, Nov. 1994.

inf I(X; Z|Y ),

WZ
Z∈Sml

WZ
where Sml is a set of RVs Z satisfying the following
conditions:
1) Y ↔ X ↔ Z
ˆ
2) For some functions ψ : Y × Z → X ,
ˆ ≤ D,
E[d(X, X)]

ˆ
where X = ψ (Y, Z).
According to Theorem 1 in [3], this upper bound is optimal.
Proof: For a stationary memoryless source (X,Y1 ,Y2 ), we
consider a set of sequences of RVs Z = {Z n }∞ satisfying the
n=1
following conditions:

5


Title:          generalizedCutsetAdvanced(ISIT).dvi
Creator:        dvips(k) 5.98 Copyright 2009 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 22:29:28 2012
ModDate:        Tue Jun 19 12:54:51 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      416631 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569566051

Cut-Set Bound for Generalized Networks
Silas L. Fong

Raymond W. Yeung

Gerhard Kramer

Department of Electronic Engineering
Institute of Network Coding and
Institute for Communications Engineering
City University of Hong Kong
Department of Information Engineering
Technische Universit¨ t M¨ nchen
a u
Kowloon, Hong Kong
The Chinese University of Hong Kong
D-80290 M¨ nchen, Germany
u
Email: lhfong5@ie.cuhk.edu.hk
Shatin, N.T., Hong Kong
Email: gerhard.kramer@tum.de
Email: whyeung@ie.cuhk.edu.hk

Abstract—In a network, a node is said to incur a delay if its
encoding of each transmitted symbol involves only its received
symbols obtained before the time slot in which the transmitted
symbol is sent (hence the transmitted symbol sent in a time slot
cannot depend on the received symbol obtained in the same time
slot). A node is said to incur no delay if its received symbol
obtained in a time slot is available for encoding its transmitted
symbol sent in the same time slot. In the classical discrete
memoryless network (DMN), every node incurs a delay. A wellknown result for the classical DMN is the cut-set outer bound.
In this paper, we generalize the model of the DMN in such a way
that some nodes may incur no delay, and we obtain the cut-set
outer bound for the generalized DMN.

realization of X. We use X n to denote a random column
vector [X1 X2 . . . Xn ]T , where the components Xk have the
same alphabet X . We let pX (x) denote the probability mass
function of the discrete random variable X. We let pY |X (y|x)
denote the conditional probability P r{Y = y|X = x} for
any discrete random variables X and Y . For simplicity, we
drop the subscript of a notation if there is no ambiguity. For
any N 2 -dimensional random tuple (W1,1 , W1,2 , . . . , WN,N ) ∈
W1,1 × W1,2 × . . . × WN,N and any set V ⊆ {1, 2, . . . , N }2 ,
we let WV = (Wi,j : (i, j) ∈ V ) be a subtuple of
(W1,1 , W1,2 , . . . , WN,N ).

I. I NTRODUCTION

III. A M OTIVATING E XAMPLE

We consider a general network in which each node may
send information to the other nodes. A node is said to incur a
delay if its encoding of each transmitted symbol involves only
its received symbols obtained before the time slot in which the
transmitted symbol is sent. A node is said to incur no delay
if its received symbol obtained in a time slot is available for
encoding its transmitted symbol sent in the same time slot.
In the classical model of the discrete memoryless network
(DMN) [1], every node incurs a delay. A well-known result for
the classical DMN is the cut-set outer bound [1,2]. However,
the delay assumption makes the classical model not applicable
to some simple networks, including the relay-without-delay
relay network studied by El Gamal et al. [3]. Therefore, we
are motivated to generalize the model of the DMN in such a
way that some nodes may incur no delay, and to prove the
cut-set outer bound for the generalized DMN.
This paper is organized as follows. Section II presents the
notation of this paper. Section III presents a two-node network
in which the classical cut-set outer bound cannot be applied.
Section IV presents the formulation of the generalized DMN.
Section V proves the cut-set outer bound for the generalized
DMN. Section VI investigates a two-node generalized DMN
where one node incurs no delay, and we prove the capacity
region using our cut-set outer bound. Section VII concludes
this paper.

We now consider a two-node network that consists of a
forward channel and a reverse channel, where the nodes are
indexed by 1 and 2. Node 1 and node 2 transmit information to
each other through the channels as follows. In each time slot,
node 1 transmits symbol X1 to node 2 through the forward
channel characterized by a conditional probability distribution
p1 (y2 |x1 ), where X1 and p1 (y2 |x1 ) together deﬁne Y2 , the
output of the forward channel. In the same time slot, node 2
receives Y2 and then transmits symbol X2 to node 1 through
the reverse channel characterized by a conditional probability distribution p2 (y1 |x1 , x2 , y2 ), where (X1 , X2 , Y2 ) and
p2 (y1 |x1 , x2 , y2 ) together deﬁne Y1 , the output of the reverse
channel. Since node 2 receives Y2 before transmitting X2 , X2
can depend on Y2 . In other words, node 2 does not incur a
delay and therefore the classical cut-set outer bound cannot be
applied in this two-node network.
To facilitate discussion, we call this network the discrete
memoryless channel (DMC) with noiseless reverse channel if

II. N OTATION

In this paper, we consider a general network that consists
of N nodes. Let I = {1, 2, . . . , N } be the index set of the
nodes. The N terminals exchange information in n time slots
as follows. Node i chooses message Wi,j ∈ {1, 2, . . . , Mi,j }

p2 (y1 |x1 , x2 , y2 ) =

1
0

if y1 = x2 ,
otherwise.

Note that the DMC with noiseless reverse channel reduces to
the DMC with feedback [1,4] if node 2 transmits in each time
slot the symbol it receives in the same time slot.
IV. G ENERALIZED D ISCRETE M EMORYLESS NETWORK

We use P r{E} to represent the probability of an event E.
We use a capital letter X to denote a random variable
with alphabet X , and use the small letter x to denote the

1

and sends Wi,j to node j for each (i, j) ∈ I × I. We
assume that each message Wi,j is uniformly distributed over
{1, 2, . . . , Mi,j } and all the messages are independent. For
each k ∈ {1, 2, . . . , n} and each i ∈ I, node i transmits
Xi,k ∈ Xi and receives Yi,k ∈ Yi in the k th time slot where
Xi and Yi are some alphabets that depend on i. After n
ˆ
time slots, node i declares Wj,i to be the transmitted Wj,i
n
based on W{i}×I and Yi for each (i, j) ∈ I × I. To
simplify notation, let MI×I denote the N 2 -dimensional tuple
(M1,1 , M1,2 , . . . , MN,N ).

Given a (B, n, MI×I )-code, it follows from Deﬁnition 5
that for each i ∈ I, node i incurs a delay if bi > 0, where
bi is the amount of delay incurred by node i. If bi = 0,
node i incurs no delay, i.e., for each k ∈ {1, 2, . . . , n}, node i
needs to receive Yi,k before encoding Xi,k . The feasibility
condition of B in Deﬁnition 4 ensures that the operations of
any (B, n, MI×I )-code are well-deﬁned for the subsequently
deﬁned discrete memoryless network; the associated coding
scheme is described after the network is deﬁned.
Deﬁnition 6: A discrete network (XI , YI , S α , G α ,
p1 , p2 , . . . , pα ), when used multiple times, is called a discrete
memoryless network (DMN) if the following holds for any
k−1
k−1
(B, n, MI×I )-code. Let Uk−1 = (WI×I , XI , YI ) be
the collection of random variables that are generated before
the k th time slot. Then, for each k ∈ {1, 2, . . . , n} and each
h ∈ {1, 2, . . . , α},

Deﬁnition 1: An α-dimensional tuple (S1 , S2 , . . . Sα ), denoted by S α , consisting of subsets of I is called an α-partition
of I if ∪α Sh = I and Si ∩ Sj = ∅ for all i = j.
h=1
Let T ⊆ I be a set and S α be an α-partition of I. For any
random tuple (X1 , X2 , . . . , XN ) ∈ X1 × X2 × . . . × XN , we
let XT = (Xi : i ∈ T ) be a subtuple of (X1 , X2 , . . . , XN ).
In addition, we let XS h denote (XS1 , XS2 , . . . , XSh ) and let
xS h be the realization of XS h for each h ∈ {1, 2, . . . , α}.
Similarly, for any k ∈ {1, 2, . . . , n} and any random tuple
(X1,k , X2,k , . . . , XN,k ) ∈ X1 × X2 × . . . × XN , we let XT,k =
(Xi,k : i ∈ T ) be a subtuple of (X1,k , X2,k , . . . , XN,k ). In addition, we let XS h ,k denote (XS1 ,k , XS2 ,k , . . . , XSh ,k ) and let
xS h ,k be the realization of XS h ,k for each h ∈ {1, 2, . . . , α}.

P r{Uk−1 = u, XS h ,k = xS h , YG h ,k = yG h }
= P r{Uk−1 = u, XS h ,k = xS h , YG h−1 ,k = yG h−1 }
ph (yGh |xS h , yG h−1 ).
Following the notation in Deﬁnition 6, consider any
(B, n, MI×I )-code on the DMN. In the k th time slot, XI,k
and YI,k are generated in the order

Deﬁnition 2: A delay proﬁle is an N -dimensional tuple
(b1 , b2 , . . . , bN ) where bi ∈ {0, 1, 2, . . .} for each i ∈ I.

XS1 ,k , YG1 ,k , XS2 ,k , YG2 ,k , . . . , XSα ,k , YGα ,k

Deﬁnition 3: The discrete network consists of N ﬁnite input sets X1 , X2 , . . . , XN , N ﬁnite output sets Y1 , Y2 , . . . , YN
and α channels characterized by conditional distributions p1 (yG1 |xS 1 ), p2 (yG2 |xS 2 , yG 1 ), . . . , pα (yGα |xS α , yG α−1 ),
where S α and G α are two α-dimensional partitions of I. We
call S α and G α the input partition and the output partition of
the network respectively. The discrete network is denoted by
(XI , YI , S α , G α , p1 , p2 , . . . , pα ).

by transmitting on the channels in this order p1 , p2 , . . . , pα
using the (B, n, MI×I )-code (as prescribed in Deﬁnition 5).
Speciﬁcally, XS h ,k , YG h−1 ,k and channel ph together deﬁne
YGh ,k for each h ∈ {1, 2, . . . , α}. We will show in the
following that the encoding of XSh ,k before the transmission
on ph and the generation of YGh ,k after the transmission
on ph for each h ∈ {1, 2, . . . , α} are well-deﬁned. Fix
any k ∈ {1, 2, . . . , n} and h ∈ {1, 2, . . . , α}. Consider the
following two cases for encoding Xi,k for each i ∈ Sh :

We deﬁne codes that use the network n times in the
following two deﬁnitions.

(1)

Case bi > 0: Since Xi,k is a function of (W{i}×I , Yik−bi )
and Yik−bi has already been received by node i by the k th time
slot, the encoding of Xi,k at node i before the transmission
on ph in the k th time slot is well-deﬁned.
Case bi = 0: Let m be the unique integer such that i ∈ Gm .
Since Xi,k is a function of (W{i}×I , Yik ) and Yi,k has already
been received by node i after the transmission on pm in the
k th time slot where m < h by feasibility of B, it follows that
the encoding of Xi,k at node i before the transmission on ph
in the k th time slot is well-deﬁned.
Combining the two cases, the encoding of Xi,k before the
transmission on ph in the k th time slot for each i ∈ Sh is
well-deﬁned, which implies that the encoding of XSh ,k before
the transmission on ph in the k th time slot is well-deﬁned.
In addition, the transmission on ph in the k th time slot
only depends on (XS h ,k , YG h−1 ,k ). Since the transmissions on
p1 , p2 , . . . , ph−1 and the encoding of XSh ,k occur before the
transmission on ph in the k th time slot, it follows that YG h−1 ,k
and XSh ,k have already been generated before the generation
of YGh ,k according to (1), which implies that the generation

Deﬁnition 4: Let (XI , YI , S α , G α , p1 , p2 , . . . , pα ) be a discrete network. For each i ∈ I, let hi and mi be the two unique
integers such that i ∈ Shi and i ∈ Gmi . Then, a delay proﬁle
(b1 , b2 , . . . , bN ) is said to be feasible for the network if the
following holds for each i ∈ I: If bi = 0, then hi > mi .
Deﬁnition 5: Let B = (b1 , b2 , . . . , bN ) be a delay
proﬁle feasible for (XI , YI , S α , G α , p1 , p2 , . . . , pα ). A
(B, n, MI×I )-code for n uses of the network consists of the
following:
1) A message set Wi,j = {1, 2, . . . , Mi,j } at node i for
each (i, j) ∈ I × I.
2) An encoding function fi,k : W{i}×I × Yik−bi → Xi for
each i ∈ I and each k ∈ {1, 2, . . . , n}, where fi,k is
the encoding function at node i in the k th time slot such
that Xi,k = fi,k (W{i}×I , Yik−bi ).
n
3) A decoding function gi,j : W{j}×I × Yj → Wi,j for
each (i, j) ∈ I × I, where gi,j is the decoding function
ˆ
for Wi,j at node j such that gi,j (W{j}×I , Yjn ) = Wi,j .

2

of YGh ,k is well-deﬁned.
Example 1: Consider a two-node DMN (X1 , X2 , Y1 , Y2 ,
({1}, {2}), ({2}, {1}), p1, p2 ) where all the alphabets are binary,
1 − ǫ if y2 = x1 ,
p1 (y2 |x1 ) =
(2)
ǫ
otherwise

Proposition 2: Fix any (B, n, MI×I )-code on the DMN
(XI , YI , S α , G α , p1 , p2 , . . . , pα ) and ﬁx an h ∈ {1, 2, . . . , α}.
Then, for each i ∈ Sh , Xi,k is a function of
(W{i}×I , Yik−1 , Y{i}∩G h−1 ,k ) for each k ∈ {1, 2, . . . , n}.
Proof: Let B = (b1 , b2 , . . . , bN ). Fix an i ∈ Sh . By
Deﬁnition 5, Xi,k is a function of (W{i}×I , Yik−bi ) for each
k ∈ {1, 2, . . . , n}. Consider the following two cases:

and

Case bi > 0: Since Xi,k is a function of (W{i}×I , Yik−bi ),
Xi,k is a function of (W{i}×I , Yik−1 , Y{i}∩G h−1 ,k ) trivially.

p2 (y1 |x1 , x2 , y2 ) =

1 if y1 = x2 + y2 ,
0 otherwise.

(3)

Case bi = 0: Let m be the unique integer such that i ∈ Gm .
Since B is feasible for the network (cf. Deﬁnition 4), h > m.
Since i ∈ Gm and Xi,k is a function of (W{i}×I , Yik ), Xi,k is
a function of (W{i}×I , Yik−1 , Y{i}∩Gm ,k ) and hence a function
of (W{i}×I , Yik−1 , Y{i}∩G h−1 ,k ) because h > m.

Note that p1 (y2 |x1 ) is the conditional probability distribution
for the binary symmetric channel (BSC). To facilitate discussion, we call this network the BSC with correlated feedback. For any ((1, 0), n, M{1,2}×{1,2} )-code on this network,
X{1,2},k and Y{1,2},k are generated in the k th time slot in the
order X1,k , Y2,k , X2,k , Y1,k for each k ∈ {1, 2, . . . , n}. Note
that node 2 incurs no delay, and can use Y2,k for encoding
X2,k because Y2,k is generated before the generation of X2,k .

The following proposition is reproduced from Proposition 2.5 in [4] to facilitate discussion.
Proposition 3: For any discrete random variables X, Y
and Z, X → Y → Z forms a Markov Chain if and only
if there exist two functions χ(x, y) and ϕ(y, z) such that
p(x,y,z) = χ(x,y)ϕ(y,z) for all x, y and z whenever p(y) > 0.

In the classical model of the DMN, every node incurs a
delay and the network is characterized by a single channel
p1 (yI |xI ). Therefore, the classical DMN can be viewed as
a generalized DMN with a single channel p1 (yI |xI ), and
every code on the classical DMN can be viewed as some
(B, n, MI×I )-code on the generalized DMN with B =
(1, 1, . . . , 1) (cf. Deﬁnitions 3, 4, 5 and 6). The model studied
in [3] is also a special case of the generalized DMN.
Deﬁnition 7: For a (B, n, MI×I )-code on the DMN, the
average probability of decoding error of Wi,j is deﬁned as
n
Pi,j = P r{gi,j (W{j}×I , Yjn ) = Wi,j } for each (i, j) ∈ I ×I.
Deﬁnition 8: A rate tuple (R1,1 , R1,2 , . . . , RN,N ), denoted
by RI×I , is achievable for the DMN if there exists a sequence
log2 Mi,j
≥ Ri,j such that
of (B, n, MI×I )-codes with lim
n
n→∞
n
lim Pi,j = 0 for each (i, j) ∈ I × I.

Theorem 1: Let (XI , YI , S α , G α , p1 , p2 , . . . , pα ) be a
DMN. Then for each achievable rate tuple RI×I , there exists
a joint distribution for (XI , YI ) satisfying
α

p(xSh |xS h−1 , yG h−1 )ph (yGh |xS h , yG h−1 )

p(xI , yI ) =
h=1

whenever p(xI , yI ) > 0 such that for any T ⊆ I,
Ri,j
i∈T,j∈T c
α

≤

I(XT ∩S h , YT ∩G h−1 ; YT c ∩Gh |XT c ∩S h , YT c ∩G h−1 ).
h=1

n→∞

Proof: Suppose RI×I is in R. By Deﬁnitions 8 and 9,
there exists a sequence of (B, n, MI×I )-codes such that
log M
n
limn→∞ 2n i,j ≥ Ri,j and limn→∞ Pi,j = 0 for each
(i, j) ∈ I × I. Fix n and the corresponding (B, n, MI×I )code. It then follows from Deﬁnition 6 that for each k ∈
{1, 2, . . . , n} and each h ∈ {1, 2, . . . , α},

Without loss of generality, we assume that Mi,i = 1 and
Ri,i = 0 for all i ∈ I in the rest of this paper.
Deﬁnition 9: The capacity region R of the DMN is the
closure of the set of all achievable rate tuples RI×I with
Ri,i = 0 for all i ∈ I.
V. C UT-S ET O UTER B OUND

k−1
k−1
(WI×I , XI , YI ) → (XS h ,k , YG h−1 ,k ) → YGh ,k

Lemma 1: Let (XI , YI , S α , G α , p1 , p2 , . . . , pα ) be a DMN.
Fix any (B, n, MI×I )-code on the DMN. Then, for each k ∈
{1, 2, . . . , n} and each h ∈ {1, 2, . . . , α},

(5)

forms a Markov Chain. Fix any T ⊆ I. Since the N 2 messages
W1,1 , W1,2 , . . . , WN,N are independent, we have

p(xS h ,k , yG h ,k ) = p(xS h ,k , yG h−1 ,k )ph (yGh ,k |xS h ,k , yG h−1 ,k ).
(4)
k−1
k−1
Proof: Let Uk−1 = (WI×I , XI , YI ) be the collection of random variables that are generated before the k th time
slot for the (B, n, MI×I )-code. It follows from Deﬁnition 6
that for each k ∈ {1, 2, . . . , n} and each h ∈ {1, 2, . . . , α},

log2 Mi,j
i∈T,j∈T c

= H(WT ×T c |W(T ×T c )c )
n
n
≤ I(WT ×T c ; YT c |W(T ×T c )c ) + H(WT ×T c |YT c , WT c ×I )
n
≤ I(WT ×T c ; YT c |W(T ×T c )c ) +

H(Wi,j |Yjn , W{j}×I )

i∈T,j∈T c

p(uk−1 , xS h ,k , yG h ,k )
= p(uk−1 , xS h ,k , yG h−1 ,k )ph (yGh ,k |xS h ,k , yG h−1 ,k ),

n
≤ I(WT ×T c ;YT c |W(T ×T c )c )+

n
(1 + Pi,j log2 Mi,j ),

i∈T,j∈T c

(6)

which then implies (4).

3

where (a) follows from (10) and (b) follows from Lemma 1.
Summing the expression in (8) over all k and dividing by n,
we have

where the last inequality follows from Fano’s inequality (cf.
Deﬁnition 7). We now consider
n
I(WT ×T c ; YT c |W(T ×T c )c )
k−1
I(WT ×T c ; YT c ,k |W(T ×T c )c , YT c )

=
=

(H(Y

|W

(T ×T c )c

H(YT c ∩Gh ,k |XT c ∩S h ,k , YT c ∩G h−1 ,k )

=

k−1
k−1
, YT c )−H(YT c ,k |WI×I , YT c )).

h=1 k=1
α

k=1

(7)
∪α Gh
h=1

Since

where (a) follows from (11). Similarly from (9), we have
1
n

α
k−1
H(YT c ∩Gh ,k |WT c ×I , YT c , YT c ∩G h−1 ,k )

n

H(YT c ∩Gh ,k |XS h ,k , YG h−1 ,k )
k=1 h=1
α
n
(a)
h=1 k=1
α

k−1
H(YT c ∩Gh ,k |WT c ×I , YT c , YT c ∩G h−1 ,k , XT c ∩S h ,k )

H(YT c ∩Gh ,k |XT c ∩S h ,k , YT c ∩G h−1 ,k ),

h=1
α

(8)
(b)

H(YT c ∩Gh ,Qn |XS h ,Qn , YG h−1 ,Qn ),

=

h=1

where (a) follows from Proposition 2 that for each
k−1
1 ≤ h ≤ α, XT c ∩Sh ,k is a function of (WT c ∩Sh ×I , YT c ∩Sh ,
YT c ∩Sh ∩G h−1 ,k ). In addition, following (7),

where (a) follows from (11), and (b) follows from the fact that
Qn → (XS h ,Qn , YG h−1 ,Qn ) → YGh ,Qn forms a Markov Chain
(cf. (12) and Proposition 3). Using (6), (7), (8), (9), (13) and
(14), we obtain

α
k−1
H(YT c ∩Gh ,k |WI×I , YT c , YT c ∩G h−1 ,k )

log2 Mi,j

h=1
α

i∈T,j∈T c
α

k−1
k−1
H(YT c ∩Gh ,k |WI×I , XI , XS h ,k , YI , YG h−1 ,k )

≥

i∈T,j∈T c

H(YT c ∩Gh ,k |XS h ,k , YG h−1 ,k )

=

n
(1 + Pi,j log2 Mi,j ) + n

≤

h=1
α
(a)

(14)

h=1

k−1
H(YT c ,k |WI×I , YT c )

=

1
H(YT c ∩Gh ,Qn |XS h ,Qn , YG h−1 ,Qn , Qn = k)
n

H(YT c ∩Gh ,Qn |XS h ,Qn , YG h−1 ,Qn , Qn )

=

h=1
α

≤

α

=

h=1
α

=

(13)

h=1

k−1
≤ H(YT c ∩(∪α Gh ),k |WT c ×I , YT c )
h=1

(a)

1
H(YT c ∩Gh ,Qn |XT c ∩S h ,Qn , YT c ∩G h−1 ,Qn , Qn= k)
n

H(YT c ∩Gh ,Qn |XT c ∩S h ,Qn , YT c ∩G h−1 ,Qn ),

≤

= I, it follows that

k−1
H(YT c ,k |W(T ×T c )c , YT c )

=

α

k=1 h=1
n
α
(a)

k=1
n
T c ,k

n

1
n

n

YT ∩G h−1 ,Qn ; YT c ∩Gh ,Qn |XT c ∩S h ,Qn , YT c ∩G h−1 ,Qn ). (15)

(9)

h=1

For each (B, n, MI×I )-code in the sequence of
(B, n, MI×I )-codes, let pXI,Qn ,YI,Qn be the probability
distribution induced by the (B, n, MI×I )-code. Consider
each distribution for (XI , YI ) as a point in an |XI ||YI |dimensional Euclidean space. Let {pXI,Qn ,YI,Qn }θ=1,2,...
θ
θ
be a convergent subsequence of {pXI,Qn ,YI,Qn }n=1,2,... with
respect to the L1 -distance, where the L1 -distance between
two distributions u(x) and v(x) on the same discrete alphabet
X is deﬁned as x∈X |u(x) − v(x)|. Since the set of all joint
distributions {pXI,Qn ,YI,Qn } is closed with respect to the
L1 -distance, there exists a joint distribution q (xI , yI ) such
¯
that
(16)
q (xI , yI ) = lim pXI,Qn ,YI,Qn (xI , yI ).
¯

where (a) follows from the Markov Chain in (5). Let Qn
be a timesharing random variable distributed uniformly on
{1, 2, . . . , n} such that Qn is independent of the collection
of random variables {XI,k , YI,k | k = 1, 2, . . . , n}. Then, for
any A, B ⊆ I,
pXA,Qn ,YB,Qn |Qn (xA , yB |k) = pXA,k ,YB,k |Qn (xA , yB |k)
= pXA,k ,YB,k (xA , yB ),

(10)

which implies that
H(XA,Qn , YB,Qn |Qn = k) = H(XA,k , YB,k ).

(11)

In addition,

θ→∞

n

θ

θ

Let
Iq (XT ∩S h , YT ∩G h−1 ; YT c ∩Gh |XT c ∩S h , YT c ∩G h−1 )
¯
denote
the
conditional
mutual
information
I(XT ∩S h , YT ∩G h−1 ; YT c ∩Gh |XT c ∩S h , YT c ∩G h−1 ) evaluated at
the distribution q (xI , yI ) for each h ∈ {1, 2, . . . , α}. Since
¯

pQn ,XS h ,Qn ,YGh ,Qn (k, xS h , yG h )
= pQn (k)pXS h ,Q

I(XT ∩S h ,Qn ,
h=1

,YGh ,Qn |Qn (xS h , yG h |k)

(a)

= pQn (k)pXS h ,k ,YGh ,k (xS h , yG h )

α

(b)

= pQn(k)pXS h ,k ,YGh−1,k(xS h , yG h−1)ph (yGh |xS h , yG h−1), (12)

I(XT ∩S h , YT ∩G h−1 ; YT c ∩Gh |XT c ∩S h , YT c ∩G h−1 )
h=1

4

is a continuous functional of pXI ,YI (xI , yI ), it then follows
log M
n
from (15), limn→∞ 2n i,j ≥ Ri,j , and limn→∞ Pi,j = 0
that

and
(b)

R2,1 ≤ I(X2 , Y2 ; Y1 |X1 ) ≤ 1 − H(Y1 |X1 , X2 , Y2 ) = 1 (22)
where (a) follows from (2) and (b) follows from (3). Let

Ri,j
i∈T,j∈T c

R∗ = (0, R1,2 , R2,1 , 0) ∈ R4
+

α

I(XT ∩S h ,Qn , YT ∩G h−1 ,Qn ; YT c ∩Gh ,Qn |

≤ lim inf
n→∞

R1,2 ≤ 1 − H(ǫ),
.
R2,1 ≤ 1

It then follows from (21) and (22) that

h=1

R ⊆ R∗ .

XT c ∩S h ,Qn , YT c ∩G h−1 ,Qn )

(23)

α

≤

We now present a transmission scheme on this network that achieves (0, 1 − H(ǫ), 1, 0). Consider a capacityachieving block code of length n for the BSC with crossover
probability ǫ with rate (1 − H(ǫ))− arbitrarily close to
1 − H(ǫ). Such a code encodes a message W1,2 , where
−
|W1,2 | = ⌈2n(1−H(ǫ)) ⌉, into a codeword consisting of
′
a sequence of bits {X1,k }k=1,2,...,n . In the k th time slot,
′
node 1 transmits X1,k = X1,k through channel p1 . The
message W2,1 consists of a sequence of n uniform i.i.d.
′
bits {X2,k }k=1,2,...,n . In the k th time slot, upon receiving
′
Y2,k , node 2 transmits X2,k = X2,k + Y2,k through channel
p2 , whose output bit Y1,k is received by node 1. Since
′
P r{Y2,k = X1,k } = 1 − ǫ by (2), node 2 can decode
W1,2 with vanishing probability of error as n goes to inﬁnity.
In addition, since P r{Y1,k = X2,k + Y2,k } = 1 by (3)
′
and X2,k = X2,k + Y2,k , it follows that with probability one,

Iq (XT ∩S h ,YT ∩G h−1 ;YT c ∩Gh |XT c ∩S h ,YT c ∩G h−1). (17)
¯
h=1

For each h ∈ {1, 2, . . . , α}, letting qh,n (xS h , yG h−1 ) denote
pXS h ,Qn ,YGh−1 ,Qn (xS h , yG h−1 ), the marginal distribution
(a)

q (xS h , yG h ) = lim pXS h ,Qn
¯
θ→∞

θ

,YGh ,Qn

θ

(xS h , yG h )

(b)

= lim qh,nθ (xS h , yG h−1 )ph (yGh |xS h , yG h−1 )
θ→∞

(18)
where (a) follows from (16) and (b) follows from (12), which
implies that the marginal distribution
q (xS h , yG h−1 ) = lim qh,nθ (xS h , yG h−1 ).
¯
θ→∞

(19)

It then follows from (18) and (19) that the marginal distribution
q (xS h , yG h )
¯
= q (xS h , yG h−1 )ph (yGh |xS h , yG h−1 )
¯

′
′
Y1,k = X2,k + Y2,k = (X2,k + Y2,k ) + Y2,k = X2,k .
′
Therefore, node 1 receives the bit sequence {X2,k }k=1,2,...,n
without error. Consequently, (0, 1 − H(ǫ), 1, 0) is achievable,
which implies from (23) that R = R∗ .

= q (xS h−1 , yG h−1 )¯(xSh |xS h−1 , yG h−1 )ph (yGh |xS h , yG h−1 )
¯
q
for each h ∈ {1, 2, . . . , α} whenever q (xS h , yG h ) > 0, which
¯
implies by recursion that

VII. C ONCLUSION
We deﬁne the generalized DMN which contains the classical
DMN as a special case. We also prove the cut-set outer bound
for the generalized DMN. Our cut-set outer bound coincides
with the classical cut-set outer bound for the classical DMN.
Finally, we investigate the BSC with correlated feedback, a
two-node generalized DMN where one node incurs no delay,
and prove the capacity region of the two-node network using
our cut-set outer bound.

α

q (xSh |xS h−1 , yG h−1 )ph (yGh |xS h , yG h−1 )
¯

q (xI ,yI ) =
¯
h=1

(20)
whenever q (xI , yI ) > 0. Since q (xI , yI ) depends only on the
¯
¯
sequence of (B, n, MI×I )-codes but not on T , the theorem
follows from (17) and (20).
The classical DMN is characterized by a single
channel p1 (yI |xI ). Therefore, the cut-set outer bound in
Theorem 1 reduces to
i∈T,j∈T c Ri,j ≤ I(XT ; YT c |XT c )
for the classical DMN where p(xI , yI ) = p(xI )p1 (yI |xI ),
which coincides with the classical cut-set outer bound.
Using Theorem 1, we obtain R1,2 ≤ I(X1 ; Y2 ) for the DMC
with noiseless reverse channel, which implies the well-known
result that R1,2 ≤ I(X1 ; Y2 ) for the DMC with feedback.
VI. C APACITY R EGION OF BSC
F EEDBACK

WITH

ACKNOWLEDGMENT
The work of Raymond Yeung was partially supported
by a grant from the University Grants Committee (Project
No. AoE/E-02/08) of the Hong Kong Special Administrative
Region, China. The work of Gerhard Kramer was supported
by the German Ministry of Education and Research and the
Alexander von Humboldt Foundation in the framework of the
Alexander von Humboldt Professorship.

C ORRELATED

R EFERENCES

Let R denote the capacity region of the BSC with correlated
feedback in Example 1. Let H(ǫ) denote the entropy of a
Bernoulli random variable X with P r{X = 0} = ǫ. It then
follows from Theorem 1 that for each achievable R{1,2}×{1,2} ,
(a)

R1,2 ≤ I(X1 ; Y2 ) ≤ 1 − H(Y2 |X1 ) = 1 − H(ǫ)

[1] T. M. Cover, Elements of Information Theory, 2nd ed. Wiley, 2006.
[2] A. El Gamal, “On information ﬂow in relay networks,” in Proc. IEEE
Nat. Telecom Conf., vol. 2, Nov. 1981, pp. D4.1.1–D4.1.4.
[3] A. El Gamal, N. Hassanpour and J. Mammen, “Relay networks with
delays,” IEEE Trans. Inf. Theory, vol. 53, pp. 3413–3431, Oct. 2007.
[4] R. W. Yeung, Information Theory and Network Coding. Springer, 2008.

(21)

5


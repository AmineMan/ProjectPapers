Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 21:32:19 2012
ModDate:        Tue Jun 19 12:54:11 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      610693 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566657

High-Rate Sparse Superposition Codes with
Iteratively Optimal Estimates
Andrew R. Barron and Sanghee Cho
Department of Statistics, Yale University, New Haven, CT 06520 USA
e-mail: {andrew.barron, sanghee.cho}@yale.edu
the squared distance between the current ﬁt and the true
coefﬁcient vector. Accordingly, we seek improved estimates
of the coefﬁcients, using the squared error loss, to increase
the separation between the distributions of the statistics and
thereby improve the reliability of the ﬁnal decision.
We use the Bayes optimal coefﬁcient estimates based on the
distribution of iteratively obtained statistics, with a uniform
prior on the choice of the j sent from each section. These
estimates use computed posterior weights of terms in each
section. These weights provide a soft decision decoding, rather
than the {0, 1} valued weights associated with thresholding.
We formulate the statistics, quantify their distribution, and
give identities that relate the expected sum of squared distance
(Bayes risk) of the estimate to an expected posterior probability of error. Taking 1 minus it quantiﬁes the rate of success.
A function g(x) gives the expected success rate on a step if
the previous success rate was x. Numerical evaluations show it
is higher than the corresponding function from the thresholdbased decisions. Indeed, g(x) stays above x for a longer extent
of the interval [0, 1] than previously obtained, leading to a
smaller fraction of mistakes. As before, this remaining fraction
of mistakes are corrected by an optional outer code.
To get rates approaching capacity, a variable power allocation is used, with P proportional to e−2 C /L .
For R < C, theoretically optimal codes have exponentially
small error probability with exponent of order (C −R)2 n. So
far, fast sparse superposition codes in [1], [2], [3] achieve
comparable exponents of order (C−R)2 L, within a logarithmic
factor of the optimal form. More speciﬁcally, the bounds on
the error probability have the exponents (CM −R)2 in place
of (C−R)2 , where CM slowly approaches the capacity C, and
the bounds are applicable only for R < CM . This motivates
additional effort, initiated here, to improve the rate and reliable tradeoff of sparse superposition codes by improving the
adaptive successive decoder.
II. R ESULTS
A. Iterative Decoding Statistics and their Distribution
In the process of estimating which terms were sent, the
ˆ
decoder develops a sequence of estimates βk of the true
ˆ
coefﬁcient vector β, and corresponding ﬁts Fk = X βk of the
codeword Xβ. Let J = {1,. . ., N } be the full set of indices.
T
The initial estimate uses stat0,j = Z0,j = Xj Y / Y and
later estimates use similar inner products with residuals of ﬁts
in place of Y . The distribution of stat0,j is approximately
a standard normal shifted by βj n/(σ 2 + P ). The σ 2 + P
in the denominator is from the variance of the coordinates

Abstract— Recently sparse superposition codes with iterative
term selection have been developed which are mathematically
proven to be fast and reliable at any rate below the capacity for
the additive white Gaussian noise channel with power control.
We improve the performance using a soft decision decoder with
Bayes optimal statistics at each iteration, followed by thresholding
only at the ﬁnal step. This presentation includes formulation of
the statistics, proof of their distributions, numerical simulations
of the performance improvement, and useful identities relating
a squared error risk to a posterior probability of error.

I. I NTRODUCTION
Sparse superposition codes use a dictionary X consisting of
vectors X1 , X2 , . . . , XN , each of n coordinates. The codeword
vectors Xβ are superpositions β1 X1 + β2 X2 + . . . + βN XN .
The entries of X are independent N(0, 1). The codeword is
conveyed through the choice of which L of the coefﬁcients are
non-zero, where L matches n to within a log factor, yet L is
a small fraction of the dictionary size N . For a channel with
additive white Gaussian noise (AWGN), with superposition
coding, what is received is Y = Xβ + ε, a vector of length
n, where ε is the noise vector with distribution N(0, σ 2 I).
The coefﬁcient vector is split into L sections each of size
M = N/L, with one non-zero entry in each section. There are
M L codewords. With M a power of 2, the encoding from an
input bit-string u1 , u2 , . . . , uK , with K = L log M , consists
of partitioning the string into L substrings of length log M
which index the terms chosen to be included in the codeword.
Denote these indices {j1 , j2 , . . . , jL√ The non-zero coefﬁcient
}.
from section takes value βj = P , with
P = P to
control the codeword power.
The rate of the code is R = (L log M )/n and the capacity
of the AWGN channel is C = (1/2) log(1+snr) where snr =
P/σ 2 is the signal-to-noise ratio.
These sparse superposition codes with an adaptive successive decoder are computationally fast codes for the Gaussian
noise channel with any ﬁxed rate below capacity, with error
probability proven to be exponentially small. See [1], [2], [3]
for this conclusion and the relationship to other literature on
compressive sensing, signal recovery, and coding for the Gaussian channel. The adaptive successive decoder uses iteratively
obtained test statistics related to inner products of the Xj with
residuals of the previous ﬁts. There, for each ﬁt update, the
decoder accepts those terms j for which the statistic is above
a threshold, chosen high enough to avoid false alarms.
The conditional distribution of such statistics is approximated as a normal random variable shifted, for the true terms
compared to the others, by an amount inversely related to

1

where Zk = (Zk,j : j ∈ J) has conditional distribution
2
Normal(0, Σk ). Here σ0 = σ 2 + P and for k ≥ 1 it is
2
ˆT Σk−1 βk . Moreover, Gk 2/σ 2 is distributed as a X 2
ˆ
σk = βk
k
n−k
random variable independent of the Zk and the past Fk−1 .
Related to the distribution PZk |Fk−1 is the distribution
QZk |Fk−1 which makes the Zk be Normal(0, I − P rojk ).
The density ratio between PZk |Fk−1 √ QZk |Fk−1 on RN
and
is uniformly bounded by the constant 1 + snr.
The Chi-square random variable divided by n is close to the
constant 1, except in events of exponentially small probability,
as long as the number√ steps k is small compared to n. Thus
of
√
Zk is approximately n bk + Zk , a normal shifted by n bk .
The distribution is further cleaned by addition of an independent normal of covariance P rojk . This makes the cleaned Zk
be distributed N(0, I) with respect to Q. Moreover, as in [2],
the boundedness of the density ratio permits replacement of
the distribution P with the simpliﬁed distribution that arises
from Q, when determining events that have exponentially
small probability. Henceforth for this summary we presume
the cleaned shifted standard normal distribution for the Zk .
k
comb
= λk,0 Z0 − k =1 λk,k Zk , where
Consider Zk
k
the vector λk = (λk,k : 0 ≤ k ≤ k) satisﬁes k =0 λk,k =
1. These can be interpreted as shifts of the standard normals
k
comb
λk,0 Z0 − k =1 √ λk,k Zk , where the shift
Zk
=
arises from combinations of the nbk . The task is to choose
the coefﬁcients of combination to produce a statk with total
shift of the desired form.
ˆ
Motivation comes from the statistics (Y − X βk,−j )T Xj ,
ˆ
ˆ
or scalings thereof, where βk,−j is the vector βk with the
contribution from the current j removed. It takes the form
ˆ
ˆ
(Y − X βk )T Xj + Xj 2 βk,j . We also ﬁnd motivation from
development of approximate Bayes optimality properties. The
statk take the following form, for some choice of vector λk
and some ck typically between σ 2 and σ 2 + P ,
√
n ˆ
comb
statk = Zk
+ √ βk
ck

of Y . The idea of the steps is to use residuals of successive
ﬁts to gradually reduce it to σ 2 . This increases the amount
by which the distribution is shifted, thereby improving the
distinguishability of the true terms from the others.
Deﬁne the shift factor α = P n/(σ 2 +D). The shift in
section takes the form α 1{j=j } . Initially D0 = P . For
ˆ
subsequent steps, the role of D is played by βk − β 2 or its
ˆk − β 2 , which quantiﬁes for the
expected value Dk = E β
statistics we develop the level of remaining interference in the
ˆ
residuals due to inaccuracy of βk . The associated shift factor
2 +D ).
is α ,k = P n/(σ
k
ˆ
Let βk be any estimate constructed from statistics statk−1 =
(statk−1,j , j ∈ J) computed on the previous step. For instance
statk−1,j could be the inner products of residuals with the
ˆ
columns of X. Initialize G0 = Y . For k ≥ 1, let Fk = X βk and
let Gk be the part of the Fk orthogonal to G0 , G1 , . . . , Gk−1 .
ˆ
Assume the current ﬁt X βk is not in the linear span of the
T
previous ﬁts, so Gk > 0. Let Zk,j = Xj Gk / Gk be the
normalized inner product of Xj and Gk .
The Zk = (Zk,j , j ∈ J) and Gk are used to update statk
ˆ
ˆ
and then βk+1 . Require that statk and βk+1 be functions of
Fk = (Z0 , G0 , . . . , Zk , Gk ). Our ﬁrst lemma provides
the conditional distribution of Zk and Gk given Fk−1 . For
k = 0 there is no conditioning Fk−1 .
ˆ
ˆ
For analysis purposes, let βe ,β1,e ,. . .,βk,e be the vectors
N +1
in R
obtained by appending an extra coordinate to the
ˆ
ˆ
vectors β, β1 , . . . , βk in RN . The value of the extra coordinate
ˆ
ˆ
for βe is σ and for the β1,e , . . . , βk,e it is 0. The subscript e
denotes that the vectors are thus extended.
Likewise Xe denotes an extended dictionary with an additional column ε/σ. Armed with this extension we have
opportunity to use a standard linear model trick representing
Y = Xe βe . Then the G0 , G1 , . . . , Gk−1 are the successive
ˆ
ˆ
orthogonal components of Xe βe , Xe β1,e , . . . , Xe βk−1,e .
Parallel to the development of these vectors G in Rn ,
let b0,e , b1,e , . . . , bk,e be deﬁned as the vectors in RN +1
ˆ
ˆ
of successive orthonormalization of βe , β1,e , . . . , βk,e and let
b0 ,b1 , . . . , bk , respectively, be the vectors formed from the
corresponding upper N coordinates.
Let Σk,e = I − (b0,e bT + b1,e bT + . . . + bk,e bT ) be
0,e
1,e
k,e
the R(N +1)×(N +1) matrix of projection onto the linear space
ˆ
ˆ
orthogonal to βe , β1,e , . . . , βk,e . The upper left N ×N portion
of this matrix denoted Σk is the conditional covariance matrix
below. The suggestion to interpret Σk as a portion of a
projection matrix was made by our colleague Antony Joseph,
who credits [4],[5] for some analogous thinking.
Also let P rojk be the matrix of projection onto the span
ˆ
ˆ
of the estimates β1 , . . . , βk , and likewise P rojk,e in which
0 is appended to each of these estimates. Σk,e differs from
I − P rojk,e by accounting for orthogonality to βe .
Lemma 1, proved in the appendix, generalizes conclusions
from [3], [2] to handle the present generality.
Lemma 1: For k ≥ 0, the conditional distribution PZk |Fk−1
of Zk given Fk−1 is determined by the representation
Gk
Zk,j = bk,j
+ Zk,j ,
σk

The combination should √ such that these statistics have the
be
√
n
n
comb
representation Zk
+ √ck β, with the desired shift √ck β.
Here are three related examples of such statistics. Idealized
case [B] has the desired form and case [C] approximately
so. Case [A] is similar, but has additional randomness from
√
T ˆ
ˆ
weights based on Zk βk / n rather than bT βk .
k
[A] Based on residuals: Let
√
ˆ
X T (Y − X βk )
n ˆ
statk =
+ √ βk ,
√
n ck
ck
ˆ
with nck = Y − X βk

2

, from λk proportional to

T ˆ
T ˆ
T ˆ
( Y − Z0 βk )2 , (Z1 βk )2 , . . . , (Zk βk )2 .

[B] Idealized: Based on coefﬁcients of orthogonal compoˆ
nents of the βk , with λk proportional to
ˆ
ˆ
ˆ
(σY − bT βk )2 , (bT βk )2 , . . . , (bT βk )2
0
1
k
ˆ
and ck = σ 2 + β − βk

2

2

, producing the relationship

√

/

eα

statk,j

0.4

0.4

0.6

0.8

g(x)
x

0.2
0.0

0.0

0.2

M = 29 , L = M
snr=7
C=1.5 bits
R=1.2 bits(0.8C)

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

x

0.6

0.8

1.0

x

Fig. 1. Plot of g(x) and the sequence xk . It is computed for a grid of ﬁfteen
x values by Monte Carlo simulation with replicate size 500.

The above results show that these two quantities are related
by Dk = (1 − xk )P permitting their recursion as follows.
Consider, for any realization j1 , j2 , . . . , jL , the next step exL
pected posterior success rate xk+1 =
=1 (P /P ) E[wk,j ].
This expected value is the same no matter which j1 , j2 , . . . , jL
was chosen, so assume here that the ﬁrst term in each section
was sent. Accordingly, at α = α ,k ,
L

xk+1 = g(xk ) =

2

(P /P ) E
=1

eα
2 +α

eα

Z1

+α Z1

+

M
α Zj
j=2 e

,

where Z1 , Z2 , . . . , ZM are independent N(0, 1). What makes
this a recursive characterization of progress is that α ,k is a
function of the preceding xk via its relationship to the expected
squared distance. Indeed, α ,k = α (xk ) where α (x) is
P n/(σ 2 + (1 − x)P ). Thus our update function is
L

.
g(x) =

j∈sec

=1
α2+αZ1

Restricted to section , the βj = P 1j =j . Accordingly,
the posterior mean of βj provides the Bayes estimator,
√
E[βj |statk ] =
P 1{j=j } which reduces to
j ∈sec wk,j
ˆ
βk+1,j =

M = 29 , L = M
snr=7
C=1.5 bits
R=0.74 bits(0.495C)

0.0

B. Iteratively Optimal Coefﬁcient Estimates
Consider the choice of the updated coefﬁcient estimates
ˆ
βk+1 as a function of statk . We arrange these to be the
Bayes optimal posterior mean of β given statk , as derived
here. Use the approximating distribution that the statk,j are
independent Normal α 1{j=j } , 1 , for j in any section ,
where α = α ,k . Let φ(s) be the standard normal density
and note that φ(s − µ)/φ(s) is proportional to eµs . With the
term j chosen according to a uniform distribution over the
M choices in section , the posterior distribution of j is
statk,j

g ( x)
x

0.6

for which, in each section , the shift factor is of the
ˆ
desired form α with Dk = β − βk 2 . It suffers from dependence of the weights of combination on the unknown
ˆ
ˆ
β. The bT βk depend on β T βk , for k = 1, 2, . . . , k.
k
ˆ
[C] Simpliﬁed: As in [B] but with each occurrence of β T βk
replaced by its known expected value.
ˆ
The β T βk is close to its expected value, indeed, within any
speciﬁed small positive η, except in an event of probability
exponentially small in Lη 2 . This is a consequence of Hoeffdˆ
ing’s inequality, interpreting β T βk as an average of L bounded
ˆ
independent random variables. Likewise, the βk − β 2 is
close to its expectation, permitting the approximation to its
ˆ
distribution as a shifted normal using Dk = E βk − β 2 in
deﬁning the shift factor α ,k as before.

P rob{j = j|statk } = wk,j = eα

1.0

1.0

√
n
+ √ β
ck

0.8

statk =

comb
Zk

(P /P ) E [w1 (α (x))]
2

M

where w1 (α) = (e
)/ eα +αZ1 + j=2 eαZj . We initiate investigation of this g(x) and compare it to the corresponding update function that arose from the {0, 1} valued
weights of the thresholding method. As in [1], [2], [3], it
L
is given by g{0,1} (x) =
=1 (P /P )Φ(α (x) − τa ), where
√
τa = 2 log M + a is the threshold. In that scheme a > 0 is
needed to avoid false alarms.
For the algorithm to update properly, we need xk+1 > xk
where xk+1 = g(xk ). Thus it is desired to have g(x) stay
above x (the 45 degree line). In [1], [2], [3], it is conﬁrmed
that, for any ﬁxed rate below the capacity, g{0,1} (x) stays
above x in an interval [0, x∗ ], where x∗ is near 1, though the
gap from 1 was of order 1/ log M . We evaluate g(x) to study
the performance of the soft decision decoder and to compare
it with the {0, 1} valued decoder. Of interest is whether the
crossing point x∗ is moved substantially closer to 1.
Fig. 1 shows our update function with rates at two different
fractions of capacity. Observe that, in both cases, the update
function is above x on the most of the interval [0, 1]. The step
function in the gap in Fig. 1 shows progression of the steps.
The gap between g(x) and x affects the number of steps to
arrive at a success rate near x∗ . [Dan Spielman has suggested
there is similarity of our use of the function g(x), which is for
adaptive successive decoding of sparse superposition codes,
with the EXIT charts of [7], used in the study of iterative
decoders of turbo codes.]

P wk,j .

This is the estimate appropriate to use each step.
At the ﬁnal step, in each section, the decoded term ˆ may
j
be taken to be the one the highest posterior weight wk,j . The
posterior probability of success in a section is the posterior
weight of the true term wk,j .
C. Relating Squared Error and Expected Posterior Success
ˆT
The quantity βk+1 β/P can be interpreted as a posterior sucL
cess rate
=1 (P /P ) wk,j , with a power-weighted average
across the sections.
Lemma 2: The posterior success rate has the same exˆ
pectation as the squared norm βk+1 2 /P . Consequently,
L
P (1 − wk,j ) has the same
the posterior error rate
=1
ˆ
expectation as the squared distance βk+1 − β 2 .
The proof of Lemma 2 is in the appendix.
D. Update Function and its Analysis
Analysis of the progression of the adaptive successive
decoder is considerably simpliﬁed if one ﬁnds a recursively
updated measure of success. The progress may be tracked
ˆ
using either the expected squared distance Dk = E βk − β 2
or the expected posterior success rate which we will denote xk .

3

1.0
0.4

0.6

0.8

1.0
0.8
0.4

0.6

A PPENDIX
Proof of Lemma 1: Consider the representation of the collection of vectors Xj , for 1 ≤ j ≤ N , augmented by one additional
T
vector XN +1 = ε/σ. The Zk ,j = Xj Gk / Gk for k < k
are the coefﬁcients of the representation of Xj in the span
of the orthonormal G0 / G0 , . . . , Gk−1 / Gk−1 , with an orthogonal residual vector Vk,j , for j in Je = {1, . . . , N, N +1}.
Collecting these into a matrix decomposition, it takes the form

g(x)
Lower bound
a=0
a=0.5

0.2

M = 29 , L = M
snr=7
C=1.5 bits
R=1.2 bits(0.8C)

0.0

0.0

0.2

M = 29 , L = M
snr=7
C=1.5 bits
R=0.74 bits(0.495C)

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

x

0.6

0.8

1.0

X=

x

where the vectors Zk = (Zk ,j : j ∈ J) extend to Zk ,e =
(Zk ,j : j ∈ Je ) when representing Xe .
Using these G0 , G1 , . . . , Gk−1 and the columns of the identity, Gram-Schmidt ﬁlls out a basis of Rn with n orthornormal
vectors ξk,0 , ξk,1 , . . . , ξk,n−1 , in which the residuals Vk,j have
n−1
representation i=k Vk,j,i ξk,i , using the last n − k of these
T
orthonormal vectors, with Vk,j,i = Vk,j ξk,i .
With the columns of Xe assumed to be independent
standard normal vectors, we solve for the evolution of the
conditional distributions of the Zk,e and Gk , using the
above representation. The conditional distribution of the Zk,e
and Gk given Fk−1,e = (Z0,e , G0 , . . . , Zk−1,e , Gk−1 )
2
2
has Xn−k = Gk 2 /σk distributed chi-square(n − k) and
Zk,e = bk,e Xn−k + Zk,e with Zk,e distributed N(0, Σk,e ). The
conclusion of the lemma then follows from noting for the Zk
that the conditional distribution given Fk−1,e only depends on
ˆ
Fk−1 , under the assumption that successively the estimates βk
are computed only from the information Fk−1 available to the
decoder (without knowledge of the noise).
Moreover, it is claimed that conditionally given Fk−1,e , the
coordinates Vk,j,i of the vectors Vk,j in the basis ξk,i , for
i = k, k + 1, . . . , n − 1, are conditionally mean-zero Normal
random variables, independent across i, and jointly across j ∈
Je , having covariance Σk−1,e [where for k = 0 the Σk−1,e is
replaced by the identity matrix].
The number of columns is arbitrary. Henceforth in the proof
there is no need to make a distinction between the cases with
and without the extension, so drop the subscript e.
Prove this claim inductively on k ≥ 0. Initially, V0,j = Xj
and the normality of the Xj provides for the validity of the
distributional claim for Vk,j for k = 0. For the induction,
assume the claim to be true at step k and derive from it that it
is true at the next step k + 1. Along the way, the conditional
distribution properties of the Gk and Zk in the lemma are
established as consequences.
2
2
Concerning Gk , note G0 2 /σ0 is Xn distributed. For
ˆ
k ≥ 1, the Gk as the part of X βk orthogonal to the previous
ˆ
ˆ
parts G0 , . . . , Gk−1 is equal to Gk = Vk βk = j βk,j Vk,j
since Vk is the part of X with columns orthogonal to the
previous parts. Representing Gk in the basis ξk,0 , . . . , ξk,n−1
it has coordinates Gk,i equal to 0 for 0 ≤ i ≤ k − 1 and
ˆ
equal to j Vk,j,i βk,j for k ≤ i ≤ n − 1. From the induction
hypothesis, these (Vk,j,i : j ∈ J) have conditional distribution
Normal(0, Σk−1 ). Accordingly, these Gk,i are independent

0.4

0.8

soft decision
hard decision with a=1/2

x=0.2

0.8

0.0

x=0

x=0.4

x

x

0.4

x

x=0.6

0.0

Expected weight of the terms sent

Fig. 2. Comparison of update functions. Red line refers to the g(x) which is
calculated by Monte Carlo simulation and the green line refers to a theoretical
lower bound of g(x). Blue and light blue lines indicates {0, 1} decision using
√
the threshold 2 log M + a with respect to the value a as indicated.

0.0

0.2

0.4

x=0.8

u(l)

0.6

0.8

1.0

0.0

0.2

x

0.4

u(l)
x

0.6

x=1
0.8

1.0

0.0

0.2

0.4

u(l)

0.6

0.8

G1 T
Gk−1 T
G0 T
Z +
Z + ... +
Z
+ Vk ,
G0 0
G1 1
Gk−1 k−1

1.0

x

29 ,

Fig. 3. Transition Plots: M =
L=M, C=1.5 bits, R=0.8C and a=0.5. We
used Monte Carlo simulation with replicate size 10000. The horizontal axis
depicts u( ) = (1 − e−2C /L )/(1 − e−2C ) which is an increasing function
of . The vertical axis gives g (x). This representation allows the area under
the curve to represent g(x). Also, the area of the rectangle to the left of the
vertical bar is x. One can see if g(x) is above x by comparing the two areas.

A simpliﬁed lower-bound on g(x) is obtained via Jensen’s
M
αZj
inequality by replacing the
in the denominator
j=2 e
α2 /2
above by its expectation (M −1)e
.
Fig. 2 evaluates different update functions. The highest is
g(x) of the new procedure. It is much higher than g{0,1} (x)
at realistic thresholds (e.g. a = 1/2) and yet still higher than
g{0,1} (x) with the unrealistic idealized threshold a = 0. In the
realistic case (a = 1/2) the g{0,1} (x) fails to allow rates at 80%
of capacity (for M = 29 and snr = 7) because its curve drops
below x at a value far from 1. In [1], [2] good performance at
reasonable rates required a much larger section size, such as
M = 216 . In contrast, the new decoder is successful at 80%
of capacity with the smaller section size.
The value δ = 1 − x∗ bounds the likely fraction of mistakes
of the ﬁnal step of the decoder. It controls the closeness to 1
of the rate of an outer Reed-Solomon code that corrects the
remain errors (as described in [1], [2]). Our goal in further
research is to establish whether the order of the error 1 − x∗
is superior to the order 1/ log M established in [2], [3].
Fig. 3 considers the success rate g (x) = E[wj (α (x))] as
a function of the section index . It shows an increasing wave
of closeness to 1 as x increases.
After a suitable number of steps, the decoder will succeed
if the weights wk,j are large enough. It is recommended on
the ﬁnal step to decode the sections for which the maximal
wk,j is at least 1/2. In contrast, when maxj∈sec wk,j < 1/2,
the posterior probability of error is more likely than not. In
that case it is recommended to leave the section undecoded as
an erasure to be corrected by the outer R.S code.

4

2
2
ˆT
ˆ
Normal(0, σk ) where σk = βk Σk−1 βk , from which it follows
2
2
2
that Gk /σk is Xn−k distributed, independent of Fk−1 .
Next, for each j, seek bk,j as a regression coefﬁcient based
on the joint distribution of the Vk,j and Gk (given Fk−1 ) to
obtain the representation of the vectors
Gk
Vk,j = bk,j
+ Uk,j .
σk

i ≤ n, j ∈ J) have the speciﬁed conditional distribution and
are conditionally independent of Gk and Zk given Fk−1 . It
follows that the conditional distribution of (Uk+1,j,i : k+1 ≤
i ≤ n − 1, j ∈ J) given Fk = (Fk−1 , Gk , Zk ) is identiﬁed.
Likewise, the vector Vk,j = bk,j Gk /σk + Uk,j has representation in this updated basis with coefﬁcient Zk,j in place of
Zk,j and with Vk+1,j,i = Uk+1,j,i for i from k+1 to n−1. So
these coefﬁcients (Vk+1,j,i : j ∈ J) have the normal N (0, Σk )
distribution for each i, independently across i from k + 1 to
n, conditionally given Fk . Thus the induction is established,
which completes the proof of Lemma 1.

This is done in the basis ξk,k , . . . , ξk,n−1 where the coordinates Vk,j,i and Gk,i are jointly normal (where across
i = k,. . ., n−1 they are independent and identically distributed,
conditionally given Fk−1 , so they share the same regression
coefﬁcient bk,j ). The coordinates of Uk,j,i are conditionally
normal random variables, independent of the Gk,i , and independent for k ≤ i ≤ n − 1. For k = 0 the coefﬁcient
bk,j = E[Vk,j,i Gk,i /σk ] simpliﬁes to E[Xj,i Yi /σY ] = βj /σY .
For k ≥ 1 the bk,j = E[Vk,j,i Gk,i /σk ] may be expressed as
ˆ
E[Vk,j,i j Vk,j ,i βj ] where the expectation is with respect
to the Normal(0, Σk−1 ) distribution for the (Vk,j,i : j ∈ J).
Accordingly, summarize the solution for these coefﬁcients as
ˆ
the vector bk = Σk−1 βk /σk .
As for the parameters of the distribution of the (Uk,j,i :
j ∈ J), use the identity Uk,j,i = Vk,j,i − bk,j Gk,i /σk and
the conditional distribution of the V and G coordinates to
conclude that it has mean 0 and conditional variance Σk−1 −
bk bT , in agreement with Σk .
k
T
T
Note that Zk,j = Xj Gk / Gk reduces to Vk,j Gk / Gk ,
which by the above representation of Vk,j takes the form
Zk,j = bk,j

Gk
σk

+

Proof of Lemma 2: The random variables in question are
sums across the sections. We show equality of the expectations
in each section. Fix a section and a step k and let stat =
(statk,j : j ∈ sec ) be the relevant part of the statistics, with
index set sec regarded as {1, 2, . . . , M }.
The random variables in question have the same expected
value no matter which terms j was sent. Accordingly, the
expectation taken conditionally on any particular realization
j match what is obtained if alternatively one averages with
respect to the uniform prior on j . Let Pj = Pstat|j =j be the
M
conditional distributions and P = (1/M ) j=1 Pstat|j =j be
the marginal distribution of stat in section , and likewise let
Ej and E, respectively, denote corresponding expectations of
functions of stat. Now wj = wk,j is the posterior probability
M
2
P [j = j|stat]. Show that wj and w 2 = j=1 wj have
the same expectation.
The Pj and P have likelihood ratio M wj . Set j = 1. Calculate the expectation E1 [w1 ] using the measure P rather than P1
2
by incorporating the factor M w1 . Thus E1 [w1 ] = M E[w1 ]. By
2
2
symmetry, E[wj ] is same across all j and so M E[w1 ] equals
M
M
2
E[ j=1 wj ] = E[ w 2 ] which is (1/M ) j=1 Ej [ w 2 ].
Each term in this sum is the same so it is E1 [ w 2 ] as claimed.
This completes the proof of Lemma 2.
Space does not permit full listing and discussion of the
relationship of sparse superposition codes to past work in
information theory, compressive sensing, and coding. For such
the reader is invited to see the discussion and reference lists
in [1], [2], [3], [6].
R EFERENCES

T
Uk,j Gk
.
Gk

The latter term is what we call Zk,j . The inner product is
preserved by switching to the basis ξk,0 , . . . , ξk,n−1 . Thus
n−1
Zk,j =
i=0 αi Uk,j,i , with αi = Gk,i / Gk , which is
0 for 0 ≤ i ≤ k − 1. The sum of squares of the αi is
equal to 1. Proceed conditionally on Fk−1 . For any ﬁxed α
n−1
with sum of squares equal to 1, the
i=k αi Uk,j,i shares
the N(0, Σk ) distribution, as a result of the independence
across i. Accordingly, with αi = Gk,i / Gk , the conditional
distribution of Zk given Gk is as indicated, and it does not
depend on Gk , so the Zk and Gk are independent given Fk−1 .
Use Gk to update the orthonormal basis of Rn
by Gram-Schmidt, replacing ξk,k , ξk,k+1 , . . . , ξk,n−1 with
Gk / Gk , ξk+1,k+1 , . . . , ξk+1,n−1 .
The coefﬁcients of Uk,j in this updated basis are
T
T
T
Uk,j Gk / Gk , Uk,j ξk+1,k+1 , . . . , Uk,j ξk+1,n−1 , which are
denoted Uk+1,j,k = Zk,j and Uk+1,j,k+1 , . . . , Uk+1,j,n−1 ,
respectively. Recalling the conditional distribution of the Uk,j ,
these coefﬁcients (Uk+1,j,i : k ≤ i ≤ n − 1, j ∈ J) are also
normally distributed, conditional on Fk−1 and Gk , independent across i from k to n − 1; moreover, for each i from
k to n − 1, the (Uk+1,j,i : j ∈ J) inherit a joint N (0, Σk )
conditional distribution from the conditional distribution that
the (Uk,j,i : j ∈ J) have.
Specializing the conclusion, separating off the i = k case
where the Uk+1,j,i is Zk,j , the remaining (Uk+1,j,i : k +1 ≤

[1] A.R. Barron and A. Joseph, “Toward fast reliable communication at
rates near capacity with Gaussian noise, Proc. IEEE Intern. Symp.
Inform. Theory, Austin, TX, June 13-18, 2010.
[2] A.R. Barron and A. Joseph, “Sparse superposition codes: Fast and
reliable at rates approaching capacity with Gaussian noise,” March
2011. Available from www.stat.yale.edu/∼arb4/publications.html
[3] A.R. Barron and A. Joseph “Analysis of fast sparse superposition
codes,” Proc. IEEE Intern. Symp. Inform. Theory, St. Petersburg,
Russia, 2011.
[4] M. Bayati and A. Montanari, “The dynamics of message passing on
dense graphs, with applications to compressed sensing,” IEEE Trans.
Inform. Theory, vol.57, no.2, pp.764–785, Feb. 2011.
[5] M. Bayati and A. Montanari, “The LASSO risk for gaussian matrices,”
IEEE Trans. Inform. Theory, vol.58, no.4, pp.1997-2017, April 2012.
[6] A. Joseph and A.R. Barron, “Least squares superposition codes of
moderate dictionary size are reliable at rates up to capacity,” IEEE
Trans. Inform. Theory, vol.58, no.5, pp.2541-2557, May 2012.
[7] S. ten Brink, “Convergence behavior of iteratively decoded parallel
concatenated codes,” IEEE Trans. Commun., vol.49, no.10, pp.17271737, Oct 2001.

5


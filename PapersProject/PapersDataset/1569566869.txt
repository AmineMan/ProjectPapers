Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon May 14 21:56:16 2012
ModDate:        Tue Jun 19 12:55:29 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      445227 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566869

Compressive Binary Search
Mark A. Davenport

Ery Arias-Castro

Department of Statistics
Stanford University

Department of Mathematics
University of California, San Diego

with C sufﬁciently large. Moreover, any method based
on such measurements requires µ to satisfy this lower
bound for some constant C > 0 [10]. This is essentially
the whole story when the measurements are nonadaptive.
In contrast, suppose now that the system implementing
(1) can provide feedback in such a way as to allow for
the measurements to be taken adaptively, meaning that
ai may be chosen as a function of the observations up
to time i − 1, that is, (y1 , . . . , yi−1 ). (This implicitly
assumes that ai is a deterministic function of this vector,
but there is no loss of generality in this assumption.
See [11] for details.) This instance of active or online
learning has received comparatively far less attention to
date. However, in recent work [11] we have established
lower bounds showing that any support recovery method
under any adaptive sampling scheme (satisfying the
conditions above) will be unable to recover the correct
support unless the nonzero entry satisﬁes

Abstract—In this paper we consider the problem of
locating a nonzero entry in a high-dimensional vector
from possibly adaptive linear measurements. We consider
a recursive bisection method which we dub the compressive
binary search and show that it improves on what any
nonadaptive method can achieve. We also establish a nonasymptotic lower bound that applies to all methods, regardless of their computational complexity. Combined, these
results show that the compressive binary search is within
a double logarithmic factor of the optimal performance.

I. I NTRODUCTION
How should one approach the problem of ﬁnding a
needle in a haystack? Speciﬁcally, suppose that a highdimensional vector x ∈ Rn is known to have a single
nonzero entry—how can we efﬁciently ﬁnd the location
of the nonzero? We will assume that we can learn about
x by taking m noisy linear measurements of the form
yi = ai , x + zi ,

i = 1, . . . , m,

(1)

where the measurement vectors a1 , . . . , am have Euclidean norm at most 1 and z1 , . . . , zm are i.i.d. according to N (0, 1). Our question reduces to the problem
of choosing the vectors a1 , . . . am and constructing an
algorithm to estimate the location of the nonzero from
the measurements y1 , . . . , ym .
This is a special case of support recovery in compressive sensing (CS) [1, 2], since (1) is equivalent to the
linear model
y = Ax + z,
(2)

µ≥C

(n/m) log n,

(4)

for some constant C > 0.
Our contribution in this paper is twofold. In Section II,
we propose a compressive binary search algorithm which
recursively tests whether the nonzero entry is on the left
or right half of the current interval. We show that the
method reliably recovers the support of a 1-sparse vector
when the nonzero entry satisﬁes
µ≥C

where y = (y1 , . . . , ym ), A is the m × n matrix with
row vectors a1 , . . . , am and z = (z1 , . . . , zm ). (Note
that the rows of A are normalized, as opposed to the
columns, which is another common convention in the
CS literature.) There are a variety of results on support
recovery in the context of (2) where the measurement
matrix A is ﬁxed in advance (i.e., is nonadaptive)
and satisﬁes certain desirable properties [3–9]. As an
example, one can show that if A is generated by drawing
√
i.i.d. ±1/ n (symmetric) entries and the signal x is
1-sparse with nonzero entry equal to µ > 0, then the
Lasso and Orthogonal Matching Pursuit (OMP) recover
the support of x with high probability provided that
µ≥C

n/m,

(n/m) log log2 n,

(5)

with a constant C > 2. We then verify this analysis via
numerical simulations. Note that by using an adaptive
measurement scheme we are able to improve upon the
requirement in (3) by reducing the log n to log log2 n,
but our scheme does not eliminate the logarithmic factor
entirely as in (4). A corollary of this result is that in
contrast to the results of [11], which argued that in general adaptive strategies do not improve over nonadaptive
strategies in terms of our ability to accurately recover
x, we see that when µ satisﬁes (5), adaptive strategies
can signiﬁcantly outperform nonadaptive ones by ﬁrst
identifying the location of the nonzero and then reserving
a set of measurements to more accurately estimate the
value of the nonzero.

(3)

1

In contrast to this upper bound, in Section III, we
provide a simple proof that µ ≥ C n/m is necessary
for any method to work. This novel proof is in some
sense tailored to this binary method as it too is based on
testing whether the nonzero component is in the left or
right half of x. In Section IV, we discuss related work
in more detail and directions for future work.

B. Performance analysis
The binary search improves on methods based on nonadaptive measurements by by weakening the requirement
(3) to (5).
Theorem 1. In our setting, with a single nonzero entry
equal to µ > 0 and a measurement budget of m ≥
2 log2 n, the probability that binary search fails to locate
the nonzero entry (denoted Pe ) satisﬁes

II. C OMPRESSIVE B INARY S EARCH

log2 n
µ2 m
exp −
.
(7)
2
8n
Proof: Since the binary search algorithm is equivariant with respect to the ordering of the entries, we
can begin by assuming without loss of generality that
x = (µ, 0, . . . , 0)T , i.e., the nonzero is located in the
ﬁrst entry of x. Thus, we can use a simple union bound
to argue that
Pe ≤

A. The algorithm
The algorithm is designed assuming that the target
vector x has exactly one nonzero entry equal to µ >
0; both the location and magnitude are unknown. The
methodology described here can be easily adapted to the
case where the sign of the nonzero entry is unknown.
For simplicity, we assume that n is dyadic, and let s0 =
log2 n, where log2 denotes the logarithm in base 2.
With a budget of m ≥ 2 log2 n measurements of the
form (1), the binary search method proceeds as follows.
We divide our m measurements into a total of s0 stages,
allocating ms measurements to stage s, where
ms := (m − s0 )2−s ,

ms := ms + 1,

s0

(s)

(6)

w(s) ∼ N

¯
P w(s) < 0 = Φ µ ·

(s)

1
ms µ2 2s
exp −
,
2
n
since for all t > 0 we have
1
¯
Φ(t) := P(N (0, 1) > t) ≤ exp(−t2 /2).
2
We next note that by construction,

s=1

(s)

= u(s) , x + zi ,

Since m ≥ 2s0 , we have that m − s0 ≥ m/2, and hence
we obtain
ms 2s ≥ (ms + 1)2s ≥ m − s0 ≥ m/2.
Plugging ms 2s ≥ m/2 into (9), we obtain
1
µ2 m
exp −
2
8n
Plugging this into (8) we arrive at
P w(s) < 0 ≤

i = 1, . . . , ms .

Pe ≤

(s)

yi .
i=1

(s+1)

(s)

= J1

s0
µ2 m
exp −
2
8n

.

,

as desired.
√
Note that we need (5) with C > 2 2 for the upper
bound on Pe in (7) to actually tend to zero as n increases.
However, by taking additional measurements beyond the
2 log2 n required by this theorem, we could loosen this
requirement to be able to set C arbitrarily close to 2.

ms

Speciﬁcally, we set J0
(s+1)
(s)
J0
= J2 otherwise.

(9)

ms + 1 ≥ (m − s0 )2−s .

Based on these measurements, we decide between going
left or right, meaning we test whether the nonzero entry
(s)
(s)
is in J1 or J2 . We do so by simply computing
w(s) =

ms 2s
2n

≤

We also have ms ≥ 1 for all s, which is necessary for our
algorithm to be able to run to completion. Starting with
(1)
J0 := {1, . . . , n}, at stage s = 1, . . . , s0 , we have a
(s)
dyadic interval J0 and consider its left and right halves
(s)
(s)
(1)
denoted J1 and J2 . For example, J1 := {1, . . . , n }
2
(1)
and J2 := { n + 1, . . . , n}. Let u(s) denote the vector
2
(s)
with entries indexed by J1 equal to 2−(s0 −s+1)/2 and
(s)
with entries indexed by J2 equal to −2−(s0 −s+1)/2 .
(s)
(s)
Note that u(s) = 1, since |J1 | = |J2 | = 2s0 −s . We
(s)
measure ms times with u , meaning that we observe
yi

ms µ
2(s−1)/2 √ , ms .
n

Thus we can bound

2−s ≤ m.

ms ≤ s0 + (m − s0 )
s=1

s=1

(s)

where J1 = {1, . . . , 2−s n} and J2 = {2−s n +
1, . . . 21−s n}. Under our assumptions, we have that

s0

s0

ms = s0 +

(8)

s=1

where a denotes the largest integer not greater than
a. Note that we do not exceed our total measurement
budget since
s0

P w(s) < 0 ,

Pe ≤

if w(s) > 0, and

2

1

aligns with the core idea of the compressive binary
search.
Let y[i] = (y1 , . . . , yi ) denote the information available after taking i measurements. Let Px denote the distribution of these measurements when the target vector
is x. Without loss of generality, we assume that ai is a
deterministic function of y[i−1] . In that case, using the
fact that yi is independent of y[i−1] when ai is given,
we have

Compressive
Binary Search
Orthogonal
Matching Pursuit

0.8

0.6
Pe
0.4

m

0.2

Px (y[m] ) =

Px (yi |ai ).

(10)

i=1

0
0

5

10

15
µ

20

25

For a subset K ⊂ {1, . . . , n}, let K c := {1, . . . , n} \ K
and let xK be the part of x indexed by K.
Let P − Q TV denote the total variation metric
between distributions P and Q, and K(P, Q) their
Kullback-Leibler divergence [12], related by Pinsker’s
inequality
1
(11)
P − Q 2 ≤ K(P, Q).
TV
2

30

Fig. 1. Comparison between compressive binary search and OMP as
a function of µ for n = 4096 and m = 256. Observe that compressive
binary search can successfully identify the location of the nonzero for
weaker values of µ than OMP, but still requires µ > n/m = 4.

C. Numerical experiments

Lemma 1. Suppose that n is even and let J1 =
{1, . . . , n/2} and J2 = {n/2 + 1, . . . , n}. For r = 1, 2,
let πr denote the uniform prior on the vectors x ∈ Rn
having a single nonzero entry equal to µ > 0, located in
Jr . Let Pr denote the distribution of y[m] when x ∼ πr .
Then
µ2 m
P2 − P1 2 ≤
.
TV
n

To validate our theory, we perform some simple
numerical experiments. Speciﬁcally, we compare the performance of the compressive binary search procedure to
√
that of OMP (with A constructed with random ±1/ n
entries). Note that in the 1-sparse case, OMP simply
reduces to identifying the column of A most highly
correlated with the measurements y. The performance
of these two algorithms is shown in Figure 1, which
shows the empirical probability of error as a function
of µ computed by averaging over 100, 000 trials. For
these experiments, we set n = 4096 and m = 256.
Note that for these values of n and m, we have that
n/m = 4 and
(n/m) log log2 n ≈ 6.3. Thus,
ignoring the constant terms in (4) and (5), we see that
the performance of the compressive binary search is
largely consistent with our theory—namely, it cannot
reliably identify the location of the nonzero when µ ≤ 4
but can for µ
6.3. Moreover, recall that as noted
in (3), the nonadaptive OMP algorithm requires that µ
exceed (n/m) log n ≈ 11.5 to succeed. Again ignoring constants, in our case this corresponds to requiring
µ to be roughly 1.8 times larger than is required for
the compressive binary search procedure, and this is
precisely the behavior we observe in Figure 1.

Proof: Let P0 denote the distribution of y[m] when
x = 0, which is multivariate normal with zero mean and
covariance I. Using Pinsker’s inequality (11), we have
P2 − P1

2
TV

≤ 2 P0 − P1

2
TV

+ 2 P0 − P2

2
TV

≤ K(P0 , P1 ) + K(P0 , P2 ).
Let P(j) denote the distribution of y[m] when the nonzero
entry (equal to µ) is at j ∈ {1, . . . , n}. By the law of
total probability,
P1 =

2
n

P(j) ,
j∈J1

and obviously
P0 =

III. L OWER B OUND : L EFT OR R IGHT ?
We now establish an explicit, non-asymptotic lower
bound for adaptive support recovery, valid for any recovery method based on adaptive measurements satisfying
the conditions required here. Though such bounds were
recently derived in [11], we provide a slightly simpler
proof here for the case of 1-sparse signals that closely

2
n

P0 ,
j∈J1

which allows us to use the convexity of the KL divergence [13], to obtain
K(P0 , P1 ) ≤

3

2
n

K(P0 , P(j) ).
j∈J1

Under P(j) , yi = µai,j + zi , while under P0 , yi = zi , so
that
P(j)
K(P0 , P(j) ) = − E0 log
P0
m
1 2
1
=
(yi − µai,j )2 − yi
E0
2
2
i=1

IV. D ISCUSSION
Our main results can be cast as follows: Theorem 1
implies that, with probability at least 1/4, the binary
search method locates the nonzero entry (for n ≥ 4) if
µ≥4

m

while Theorem 2 shows that any method for locating the
nonzero entry fails with probability at least 1/4 when

E0 −yi µai,j + (µai,j )2 /2

=

i=1
2 m

=

µ
2

E0 a2 .
i,j

µ≤

i=1

m

a2 ,
i,j

E0
i=1

j∈J1

and similarly,
K(P0 , P2 ) ≤

µ2
n

m

a2 ,
i,j

E0
i=1

j∈J2

inf

so that
K(P0 , P1 ) + K(P0 , P2 ) ≤

sup

x x:1−sparse

µ2
n

m

n

a2 ≤
i,j

E0
i=1

j=1

1
2

n
.
m

Clearly, the bounds do not match. Numerically, for n ≤
106 , log log2 n ≤ 3, in which case the discrepancy is a
√
multiplicative factor of 8 3 < 14.
We will return to the issue of whether this gap can be
closed below, but ﬁrst we wish to discuss an additional
implication of Theorem 2. Speciﬁcally, one can show
that Theorem 2 implies that for any estimator S of the
support of x, E|S∆S| ≥ (1 − µ m/n). Following the
same argument as in Theorem 3 of [11], this implies that
under the measurement model in (2) we have

The ﬁrst line is by deﬁnition; the second and third are
consequences of (10) and the deﬁnition of the normal
likelihood; the fourth line is because, under P0 , yi is
independent of ai,j and has zero mean. Hence,
µ2
K(P0 , P1 ) ≤
n

n
,
m

log log2 n

µ2 m
,
n

1
E x−x
n

2

≥C

1
,
m

where C = 1/27. In contrast, Theorem 1 implies that
for sufﬁciently large µ, there exist estimators that do far
better than this bound (by a factor of n).
While the problems of estimating or detecting the
support of a 1-sparse vector might seem to have only
limited applications, in fact one can extend any algorithm
that identiﬁes the support of a 1-sparse vector to one
that works for vectors with k ≥ 2 nonzero entries. This
can be done by ﬁrst exploiting a simple hashing scheme
which will (with high probability) isolate each nonzero,
and then applying the method for 1-sparse vectors to
each hash separately. For an overview of this approach
in a similar context, see [4].
We also note that [4] independently proposes a method
very similar to the compressive binary search approach
we describe. Though [4] considers a different setting
with continuous signals (instead of vectors as we do),
the method proposed is essentially the same, except
that the measurement budget is partitioned differently.
In particular, it is not obvious to us that the strategy
in [4] will always succeed, since it does not account
for rounding effects or enforce that a base number of
measurements are reserved for each scale (stage) and
so (to the best of our understanding) the method might
exhaust its measurement budget before the algorithm
terminates. Another key difference is that by considering
the simpler setting of a vector in Rn , we can signiﬁcantly
simplify the analysis. That being said, the conclusions
of [4] are broadly similar to our own.

since ai ≤ 1 for all i.
Lemma 1 implies a lower bound on the risk of the
problem of testing whether a vector x ∈ Rn with a single
nonzero entry equal to µ is supported on the ﬁrst half
or second half of the index set {1, . . . , n}. Proving this
result by directly looking at the likelihood ratio, which
would be the standard approach, seems quite delicate as
we are testing a mixture (supported on the ﬁrst half)
versus a mixture (supported on the second half).
Theorem 2. In the setting of Lemma 1, consider testing
H1 versus H2 , where Hr is the hypothesis by which x is
supported in Jr . Then under the uniform prior, for any
test T ,
1
P(T fails) ≥
1 − µ m/n .
2
Note that the lower bound is also valid in the minimax
sense. In fact, the uniform prior is least favorable by
invariance consideration [14, Sec. 8.4].
Proof: Under the uniform prior, we are effectively
testing P1 versus P2 . The likelihood ratio test, which
rejects when L > 1, with L := P2 / P1 , has minimum
risk, bounded by
1
(1 − P2 − P1 TV ) .
2
(See Lemma 1 of [11].) We then apply Lemma 1 to
bound the total variance distance on the RHS.

4

Finally, we also note that there a few other adaptive algorithms that have been proposed in this setting.
For example, [15] proposes an algorithm similar to
the compressive binary search procedure but using a
different procedure for allocating measurements to each
stage. As another example, the Compressive Distilled
Sensing (CDS) algorithm proposed in [16] considers a
CS sampling algorithm which performs sequential subset
selection via the random projections typical of CS. In a
different direction, [17, 18] suggest Bayesian approaches
where the measurement vectors are sequentially chosen
so as to maximize the conditional differential entropy
of yi given (y1 , . . . , yi−1 ). While it remains a challenge
to obtain performance bounds for the Bayesian methods
suggested in [17, 18], CDS is analyzed in detail in [16]
for the task of estimating a k-sparse vector x. Following
the proof with a view on support recovery, one can
establish that CDS is reliable in our context when
µ ≥ Cn

[5]

[6]

[7]

[8]

[9]

n/m,
[10]

with Cn → ∞ arbitrarily slowly, coming extremely close
to the lower bound of (4). However, the algorithm seems
to require that m ≥ nα for a constant α > 0 ﬁxed, while
binary search only requires m ≥ 2 log2 n.
An important question would seem to be whether
there exist methods which allow for both small m and
µ approaching the bound in (4). After the submission
of this paper, Malloy and Nowak proposed a slight
modiﬁcation of the compressive binary search approach
(involving a different allocation of the measurements to
each stage) which answers this question [19]. Speciﬁcally, [19] removes the log log2 n term at the cost of a
slightly worse constant. Thus, the gap between the lower
bound in (4) and the upper bound in [19] differs only
by a constant factor. It would be interesting to know
whether either of these bounds can be tightened.

[11]

[12]

[13]
[14]

[15]

ACKNOWLEDGEMENTS
[16]

Thanks to E. Cand` s for many insightful discussions.
e
M. D. is supported by NSF grant DMS-1004718. E. A-C.
is partially by ONR grant N00014-09-1-0258.
R EFERENCES

[17]

[1] R. Baraniuk, “Compressive sensing,” IEEE Signal
Processing Mag., vol. 24, no. 4, pp. 118–120, 124,
2007.
[2] E. Cand` s and M. Wakin, “An introduction to
e
compressive sampling,” IEEE Signal Processing
Mag., vol. 25, no. 2, pp. 21–30, 2008.
[3] N. Verzelen, “Minimax risks for sparse regressions:
Ultra-high-dimensional phenomenons,” Electron. J.
Statist., vol. 6, pp. 38–90, 2012.
[4] M. Iwen, “Group testing strategies for recovery of
sparse signals in noise,” in Proc. Asilomar Conf.

[18]

[19]

5

Signals, Systems, and Computers, Paciﬁc Grove,
CA, Nov. 2009.
S. Aeron, V. Saligrama, and M. Zhao, “Information
theoretic bounds for compressed sensing,” IEEE
Trans. Inform. Theory, vol. 56, no. 10, pp. 5111–
5130, 2010.
M. Wainwright, “Information-theoretic limits on
sparsity recovery in the high-dimensional and noisy
setting,” IEEE Trans. Inform. Theory, vol. 55,
no. 12, pp. 5728–5741, 2009.
T. Cai and L. Wang, “Orthogonal matching pursuit
for sparse signal recovery with noise,” IEEE Trans.
Inform. Theory, vol. 57, no. 7, pp. 4680–4688,
2011.
P. Zhao and B. Yu, “On model selection consistency
of lasso,” J. Mach. Learn. Res., vol. 7, pp. 2541–
2563, 2006.
E. Cand` s and Y. Plan, “Near-ideal model selection
e
by 1 minimization,” Ann. Stat., vol. 37, no. 5A, pp.
2145–2177, 2009.
E. Cand` s and M. Davenport, “How well can
e
we estimate a sparse vector?” Arxiv preprint
arXiv:1104.5246, 2011.
E. Arias-Castro, E. J. Cand` s, and M. A. Dave
enport, “On the fundamental limits of adaptive
sensing,” Arxiv preprint arXiv:1111.4646, 2011.
P. Massart, Concentration inequalities and model
selection, ser. Lecture Notes in Mathematics.
Berlin: Springer, 2007, vol. 1896.
T. Cover and J. Thomas, Elements of information
theory. Hoboken, NJ: Wiley-Interscience, 2006.
E. Lehmann and J. Romano, Testing statistical
hypotheses, ser. Springer Texts in Statistics. New
York: Springer, 2005.
J. Haupt, R. Nowak, and R. Castro, “Adaptive
sensing for sparse signal recovery,” in Proc. Digital
Signal Processing Workshop, Marco Island, FL,
Jan. 2009.
J. Haupt, R. Baraniuk, R. Castro, and R. Nowak,
“Compressive distilled sensing: Sparse recovery
using adaptivity in compressive measurements,” in
Proc. Asilomar Conf. Signals, Systems, and Computers, Paciﬁc Grove, CA, Nov. 2009.
R. Castro, J. Haupt, R. Nowak, and G. Raz, “Finding needles in noisy haystacks,” in Proc. IEEE
Int. Conf. Acoust., Speech, and Signal Processing
(ICASSP), Las Vegas, NV, Apr. 2008.
S. Ji, Y. Xue, and L. Carin, “Bayesian compressive
sensing,” IEEE Trans. Signal Processing, vol. 56,
no. 6, pp. 2346–2356, 2008.
M. Malloy and R. Nowak, “Near-optimal
compressive binary search,” Arxiv preprint
arXiv:1203.1804, 2012.


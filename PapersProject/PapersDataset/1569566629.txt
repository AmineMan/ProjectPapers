Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 23:38:10 2012
ModDate:        Tue Jun 19 12:54:08 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      1729220 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566629

Simpler Achievable Rate Regions
for Multiaccess with Finite Blocklength
Ebrahim MolavianJazi and J. Nicholas Laneman
Department of Electrical Enginerring
University of Notre Dame
Notre Dame, IN 46556, USA
Email: {emolavia,jnl}@nd.edu

the average mutual information, or ﬁrst-order characterization.
Similarly, in the regime of ﬁnite blocklength, according to
the Central Limit Theorem, the CDF of the n-letter mutual
information rate RV approaches that of a Gaussian, so we
can estimate the outage probability and approximate achievable rates by using ﬁrst and second moments of the mutual
information rate RV, or the second-order characterization.
In this paper, we show how similar ideas can be extended to
a multi-user setting in which multiple users are communicating
several independent messages to a single receiver over a
multiple access channel. In particular, we explore the increase
in coding rate, especially its second-order, as a function of the
ﬁnite blocklength for a ﬁxed average error probability. A key
element of our work is to use an outage-splitting approach for
the problem of assigning a single average error probability to
several outage events arising in a DM-MAC. We demonstrate
that this approach leads to simple, but rather tight achievable
regions in the ﬁnite blocklength regime.

Abstract—Although practical communication networks employ
coding schemes with blocklengths as low as several hundred
symbols, classical information theoretic setups consider blocklengths approaching inﬁnity. Building upon information spectrum
concepts and recent work on channel dispersion, we develop
a non-asymptotic inner bound on as well as a low-complexity,
second-order achievable rate region for a discrete memoryless
multiple access channel with a given ﬁnite blocklength and
positive average error probability. Our bounds appear to capture
essentially the same region as those of Tan and Kosut, but
are less computationally complex because they require only the
means and variances of the relevant mutual information random
variables instead of their full covariance matrix.

I. I NTRODUCTION
Traditional channel coding theorems of information theory
study the fundamental limits of communication in the presence
of noise and interference using coding schemes of asymptotically large blocklengths. In such extremes, information can be
encoded at a rate approaching a ﬁrst order statistic (the channel
average mutual information). Delay and complexity limitations of many practical applications, however, require coding
with ﬁnite blocklength, even on the order of several hundred
symbols, for which classical results do not hold. Following
Strassen [1], it has recently been shown [2], [3] that a second
order statistic (the channel dispersion) plays an important role
in the fundamental limits with ﬁnite blocklength.
From a high-level perspective, both analyses stem from the
common framework of information spectrum approach [4],
i.e., treating mutual information as a random variable (RV);
its limiting version for asymptotically large blocklength [4],
and its n-letter form for ﬁnite blocklength [1], [2], [3]. In
either case, the cumulative distribution function (CDF) of this
RV characterizes performance in terms of the probability that
channel cannot support the communication rate and causes an
“outage” for the actual codeword to be correctly detected at
the receiver. High coding rates turn out to arise when error
probability is approximated by the outage probability, and the
probability of “confusion”, i.e., the observation is wrongly
decoded to any incorrect codeword, decays to zero.
For the important case of stationary memoryless channels,
the limiting mutual information rate RV concentrates with
probability one at the average mutual information, so we are
dealing with a zero or one outage probability, depending upon
whether the communication rate is less than or greater than

II. P ROBLEM S TATEMENT AND BACKGROUND
A 2-user discrete memoryless multiple access channel (DMMAC) without feedback consists of two ﬁnite input alphabets
X1 and X2 , a ﬁnite output alphabet Y, and a channel transition
probability matrix PY |X1 X2 (y|x1 , x2 ) : X1 × X2 → Y whose
n-th extension follows
n
n
n
PY n |X1 X2 (y n |xn , xn ) =
1
2

PY |X1 X2 (yl |x1l , x2l ).
l=1

For such a DM-MAC (X1 , X2 , PY |X1 X2 (y|x1 , x2 ), Y), an
(n, M1 , M2 , ) code is composed of two message sets M1 =
{1, ..., M1 } and M2 = {1, ..., M2 }, and a corresponding set
of codeword pairs and mutually exclusive decoding regions
{(xn (j), xn (k), Dj,k )}, with j ∈ M1 and k ∈ M2 , such that
1
2
the average error probability satisﬁes
(n)
Pe

1
M1 M 2

M1 M2
n
n
Pr[Y n ∈ Dj,k |X1 (j), X2 (k) sent] ≤ .
/
j=1 k=1

Accordingly, a (log M1 (n, )/n, log M2 (n, )/n) pair is
achievable for a DM-MAC (X1 , X2 , PY |X1 X2 (y|x1 , x2 ), Y)
with ﬁnite blocklength n, and average error probability if
such an (n, M1 , M2 , ) code exists.

1

Theorem 1. For a DM-MAC (X1 , X2 , PY |X1 X2 (y|x1 , x2 ), Y)
and for any joint distribution p(t)p(x1 |t)p(x2 |t), there exists
a (n, M1 , M2 , ) code such that

As mentioned in Section I, channel coding rates depend
upon the behavior of the relevant mutual information RVs.
Speciﬁcally, a 2-user DM-MAC involves the following three
mutual information RVs:
PY |X1 X2 (Y |X1 X2 T )
i(X1 ; Y |X2 T ) log
,
PY |X2 T (Y |X2 T )
PY |X1 X2 (Y |X1 X2 T )
i(X2 ; Y |X1 T ) log
,
PY |X1 T (Y |X1 T )
PY |X1 X2 (Y |X1 X2 T )
i(X1 X2 ; Y |T ) log
,
PY |T (Y |T )

n
n
n
n
≤ Pr [i(X1 ; Y n |X2 T n ) ≤ log γ1 (X1 , X2 )]
M1 − 1
n ¯
n
n
n
Pr i(X1 ; Y2n |X2 T n ) > log γ1 (X1 , X2 )
+
2
n
n
n
n
+ Pr [i(X2 ; Y n |X1 T n ) ≤ log γ2 (X1 , X2 )]
M2 − 1
n ¯
n
n
n
Pr i(X2 ; Y1n |X1 T n ) > log γ2 (X1 , X2 )
+
2
n n
n
n
+ Pr [i(X1 X2 ; Y n |T n ) ≤ log γ3 (X1 , X2 )]
(M1 −1)(M2 −1)
n n ¯
n
n
Pr i(X1 X2 ; Y0n |T n )> log γ3 (X1 , X2 )
+
2
(1)

where T is an auxiliary “time sharing” RV satisfying the
Markov Chain T → X1 X2 → Y . In the regime of asymptotically large blocklength, achievable rates will depend on
the ﬁrst order statistics of these RVs:
I(X1 ; Y |X2 T )

E[i(X1 ; Y |X2 T )],

I(X2 ; Y |X1 T )

E[i(X2 ; Y |X1 T )],

I(X1 X2 ; Y |T )

¯ ¯ ¯
where
Y n , Y0n , Y1n , Y2n
are
n-fold
distributions
=
according to PY Y0 Y1 Y2 |X1 X2 T (y, a, b, c|x1 , x2 , t)
¯ ¯ ¯
PY |X1 X2 (y|x1 , x2 )PY |T (a|t)PY |X1 T (b|x1 , t)PY |X2 T (c|x2 , t),
n
n
and where γ1 , γ2 , γ3 : X1 × X2 → [0, ∞) are arbitrary
measurable functions whose optimal choices to give highest
rates are as follows:

E[i(X1 X2 ; Y |T )],

where expectation is taken with respect to the distribution
p(t)p(x1 |t)p(x2 |t)PY |X1 X2 (y|x1 , x2 ). Using these quantities,
Ahlswede and Liao [5] established the capacity region of a
2-user DM-MAC. Subsequently, Dueck [6] and Ahlswede [7]
proved the strong converse, concluding that, even for a nonvanishing average error probability 0 <
≤ 1, the ﬁrstorder characterization of the capacity region of a DM-MAC
(X1 , X2 , PY |X1 X2 (y|x1 , x2 ), Y) is given by the closure as
n→∞ of all (log M1 (n, )/n, log M2 (n, )/n) pairs satisfying

n
n
γ1 (X1 , X2 ) ≡

Theorem 2. For a DM-MAC (X1 , X2 , PY |X1 X2 (y|x1 , x2 ), Y)
and for any joint distribution p(t)p(x1 |t)p(x2 |t), there exists
a (n, M1 , M2 , ) code such that

log M2 (n, ) < nI(X2 ; Y |X1 T ) + o(n)

n
n
n
n
≤ Pr [i(X1 ; Y n |X2 T n ) ≤ log γ1 (X1 , X2 )

for
some
choice
of
the
joint
distribution
p(t)p(x1 |t)p(x2 |t)PY |X1 X2 (y|x1 , x2 ) with the auxiliary
random variable T deﬁned on a set |T | ≤ 2.
In the following, we sharpen these classical results for the
ﬁnite blocklength regime using the second order statistics or
dispersions of the relevant mutual information RVs:

n
n
n
n
∪ i(X2 ; Y n |X1 T n ) ≤ log γ2 (X1 , X2 )
n n
n
n
∪ i(X1 X2 ; Y n |T n ) ≤ log γ3 (X1 , X2 )]

M1 − 1
n ¯
n
n
n
Pr i(X1 ; Y2n |X2 T n ) > log γ1 (X1 , X2 )
2
M2 − 1
n ¯
n
n
n
+
Pr i(X2 ; Y1n |X1 T n ) > log γ2 (X1 , X2 )
2
(M1 −1)(M2 −1)
n n ¯
n
n
+
Pr i(X1 X2 ; Y0n |T n )> log γ3 (X1 , X2 ) .
2
(2)
+

Var[i(X1 ; Y |X2 T )],

V(X2 ; Y |X1 T )

M2 − 1
,
2

(M1 − 1)(M2 − 1)
.
2
The above expression for DT bound is stated to match our
outage-splitting approach later in Theorem 3. It is, however,
possible to strengthen this bound by focusing on the three
outages jointly.

log M1 (n, ) + log M2 (n, ) < nI(X1 X2 ; Y |T ) + o(n)

Var[i(X2 ; Y |X1 T )],

V(X1 X2 ; Y |T )

n
n
γ2 (X1 , X2 ) ≡

n
n
γ3 (X1 , X2 ) ≡

log M1 (n, ) < nI(X1 ; Y |X2 T ) + o(n)

V(X1 ; Y |X2 T )

M1 − 1
,
2

Var[i(X1 X2 ; Y |T )],

¯ ¯ ¯
are
n-fold
distributions
where
Y n , Y0n , Y1n , Y2n
according to PY Y0 Y1 Y2 |X1 X2 T (y, a, b, c|x1 , x2 , t)
=
¯ ¯ ¯
PY |X1 X2 (y|x1 , x2 )PY |T (a|t)PY |X1 T (b|x1 , t)PY |X2 T (c|x2 , t),

where the variances are again calculated with respect to the
distribution p(t)p(x1 |t)p(x2 |t)PY |X1 X2 (y|x1 , x2 ).
III. M AIN R ESULTS

Next, we give an achievable region for a DM-MAC with
(sufﬁciently large) ﬁnite blocklength, which is a consequence
of the DT bound for DM-MAC in Theorem 1 by appealing to
the Central Limit Theorem to utilize a Gaussian approximation
for the relevant mutual information RVs, thus estimating the
outage and confusion probabilities. In the following, Q−1 (·) is
the well-known inverse of the complementary-CDF function
2
∞
1
of a standard Gaussian distribution Q(x) 2π x e−t /2 dt.

This section summarizes our main results in this paper. We
ﬁrst state the following Dependence Testing (DT) bound for a
DM-MAC, which provides a non-asymptotic achievable region
valid for any blocklength. It basically describes the error
probability in terms of the outage and confusion probabilities,
and is based on the DT bound of [2] and ideas from the
information-spectrum approach for a general MAC [4].

2

Theorem 3. An achievable region for the DM-MAC
(X1 , X2 , p(y|x1 , x2 ), Y) is given by the union of all
(log M1 (n, )/n, log M2 (n, )/n) pairs satisfying

0.5
0.45
0.4

log M1 (n, ) < nI(X1 ; Y |X2 T )

0.35

log M2 / n

− Q−1 (λ1 ) nV(X1 , Y |X2 T )+O(1),
log M2 (n, ) < nI(X2 ; Y |X1 T )
− Q−1 (λ2 ) nV(X2 , Y |X1 T )+O(1),
log M1 (n, )+log M2 (n, ) < nI(X1 X2 ; Y |T )

0.3
0.25
0.2
0.15

− Q−1 (λ3 ) nV(X1 X2 , Y |T )+O(1), (3)

0.1

for
some
choice
of
the
joint
distribution
p(t)p(x1 |t)p(x2 |t)PY |X1 X2 (y|x1 , x2 ) with the auxiliary
random variable T deﬁned on a set |T | ≤ 6, and for some
positive constants λ1 , λ2 , λ3 satisfying λ1 + λ2 + λ3 = 1.

0.05
0
0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

log M / n
1

In light of our discussions in Section I, this theorem suggests
that high rates arise from coding schemes in which outages
dominate confusions, such that the average error probability
is split among the three outage events of a 2-user DM-MAC
according to some λ1 , λ2 , λ3 partitioning. In comparison with
Ahlswede and Liao’s result [5], Theorem 3 suggests that taking
ﬁnite blocklength into account introduces rate penalties (for
the interesting case of < 1 ) that depend on blocklength,
2
error probability and DM-MAC dispersions.
Our main result depends only on the mean and variance
of the relevant mutual information RVs, each of which is
approximated with a scalar Gaussian distribution. By contrast,
in a concurrent work on this problem, Tan and Kosut [8] treat
the outage events jointly, without using a union bound to split
the outages. In fact, although our Theorem 3 follows from the
DT bound of Theorem 1, the Tan and Kosut result [8] can be
obtained as a second-order approximation of the generalized
DT bound of Theorem 2. Although the approach of [8] leads
to an inner bound for the DM-MAC that is larger in principle,
it requires dealing with a full covariance matrix and the
inverse CDF of a multi-dimensional Gaussian distribution,
which we expect to be more computationally complex than
our result particularly as the number of users grows. It is
worth mentioning that choosing λ1 → 1 or λ2 → 1 in our
Theorem 3 recovers the point-to-point results of [2] along the
two axes, and selecting λ3 → 1 recovers (a signiﬁcant part of)
the dominant face of the achievable region of [8].

Fig. 1. Achievable region of Theorem 3 for the real adder DM-MAC Y =
X1 + X2 + Z with n = 200 and = 10−3 . The full union is approximated
by taking several values of λ1 , λ2 , λ3 with steps of 0.1. The capacity region
(n → ∞) is also shown.

Fig. 2. Achievable region of Theorem 3 for the real adder DM-MAC Y =
X1 + X2 + Z with n = 200, 300, 500, 1000, 5000 and = 10−3 . Also
shown for comparison are: the result of Tan-Kosut [8], the single-user outer
bound of [2], and the capacity region (n → ∞).

IV. N UMERICAL E XAMPLE

with that of Tan and Kosut [8], the genie-aided single-user
outer bound implied by [2], and the classical capacity region.
Both achievable regions in the ﬁnite blocklength regime operate quite close to the outer bound and both have smooth
shapes with no sharp corners, but as blocklength grows, they
approach the well-known pentagon shape of the capacity
region. Furthermore, for this example, our region appears to
achieve much of the region of [8], except for a very slight
gap at the blunt “corners” of the region which shrinks as
blocklength grows. In terms of numerical evaluation, our result
takes at least one order of magnitude less time than that of [8]
for the same resolution.

In this section, we illustrates our results through the example
of the “real adder” DM-MAC Y = X1 + X2 + Z that takes
the real addition of binary inputs X1 , X2 and Bernoulli( 1 )
2
noise Z, leading to a quaternary output Y .
Figure 1 depicts, in addition to the classical capacity region,
the achievable region of Theorem 3 for n = 200 and =
10−3 . For each valid selection of the parameters λ1 , λ2 , λ3 , a
pentagon is obtained and taking the union over all such choices
gives rise to a convex hull, that is, a curved shape.
Figure 2 compares our achievable rate region in Theorem 3
for blocklengths n = 200, 300, 500, 1000, 5000 and = 10−3

3

V. P ROOF OF T HEOREMS 1 AND 2

realizations and the ensemble of all codebook pairs.
n
n
n
n
≤ Pr [i (X1 ; Y n |X2 T n ) ≤ log γ1 (X1 , X2 )]
n ¯n
n n
n
+ (j − 1)Pr i X ; Y |X T > log γ1 (X , X n )

Here, we summarize the proof of the DT bound for DMMAC. The proof uses the coded time sharing method of [5]
with the usual random coding technique, i.e., proving that
the average error probability over the ensemble of all codes
generated at random satisﬁes the DT bound, thus concluding
the existence of a code with the described rate and error probability performance. The decoding rule, however, is likelihood
ratio test (LRT) decoding [2], which can be considered as a
generalization of joint typicality decoding that evaluates the
partial (conditional) and full (unconditional) dependence of
the output on the input codeword pair under test.
A time sharing realization tn is generated according to
n
l=1 p(tl ) and revealed to both the receiver and the two
n
transmitters. Then, M1 codewords X1 (j), j ∈ M1 , and M2
n
codewords X2 (k), k ∈ M2 , all of length n are generated
n
n
independently according to l=1 p(x1l |tl ) and l=1 p(x2l |tl ),
respectively. These two codebooks are also revealed to the
receiver and both transmitters. Given the time sharing realization tn , the codebook pair {xn (j)}M1 × {xn (k)}M2 , and the
1
2
j=1
k=1
channel output y n , the decoder runs for all M1 M2 codeword
pairs the following three LRTs

E[

j,k ]

1

2

2

1

2

n
n
n
n
+ Pr [i (X2 ; Y n |X1 T n ) ≤ log γ2 (X1 , X2 )]
n ¯
n
n
n
+ (k − 1)Pr i X2 ; Y1n |X1 T n > log γ2 (X1 , X2 )
n n
n
n
+ Pr [i (X1 X2 ; Y n |T n ) ≤ log γ3 (X1 , X2 )]
n n ¯n
n
n
n
+(j − 1)(M2 − 1)Pr i X1 X2 ; Y0 |T > log γ3 (X1 , X2 ) .

Averaging this over all message pairs (j, k) gives the DT
bound (1). To conclude the proof of Theorem 1, it is sufﬁcient
to observe that each line on its RHS is a weighted sum of
two types of error in a Bayesian binary hypothesis test, and
therefore corresponds to average error probability of the test.
Then, it is known that the optimal test is an LRT (as we
have used) with the optimal threshold equal to the ratio of
priors or simply the ratio of the coefﬁcients of the two error
probabilities in each test.
VI. P ROOF OF T HEOREM 3

Here, we sketch how Theorem 1 is used for proving
Theorem 3. We basically expand each of the three mutual
(1)
Zj,k (y n ) = 1{i (xn (j); y n |xn (k)tn ) > log γ1 (xn (j), xn (k))}, information RVs in the DT bound (1) as sums of i.i.d. RVs
1
2
1
2
and use the Central Limit Theorem, or more speciﬁcally the
(2) n
n
n n
n
n
Zj,k (y ) = 1{i (x2 (j); y |x1 (k)t ) > log γ2 (x1 (j), xn (k))},
2
Berry-Esseen Theorem, to calculate the associated outage and
(3)
Zj,k (y n ) = 1{i (xn (j)xn (k); y n |tn ) > log γ3 (xn (j), xn (k))}, confusion probabilities, analogous to [2].
1
2
1
2
Upon ﬁxing the distribution p(t)p(x1 |t)p(x2 |t), the output
(1) n
(2) n
Y n of the DM-MAC p(y|x1 , x2 ) corresponding to the inputs
choosing the ﬁrst pair (j, k) for which Zj,k (y ) = Zj,k (y ) =
n
n
X1 and X2 and the time sharing variable T n described in the
(3)
Zj,k (y n ) = 1, where “ﬁrst” is deﬁned as a row-by-row search,
DT bound above satisﬁes
so that (m, p) < (j, k) iff either m < j or m = j, p < k. The
n
n
error probability for message pair (j, k) is thus given by
p(Yl |X1l X2l Tl )
n
n
i (X1 ; Y n |X2 T n ) =
log
i1l ,
p(Yl |X2l Tl )
(1)
(2)
(3)
l=1
l=1
n
n
n
j,k = Pr Zj,k (Y ) = 0 ∪ Zj,k (Y ) = 0 ∪ Zj,k (Y ) = 0
n
n
p(Yl |X1l X2l Tl )
n
n
i (X2 ; Y n |X1 T n ) =
log
i2l ,
(1)
n
(2)
n
(3)
n
Zm,p (Y ) = Zm,p (Y ) = Zm,p (Y ) = 1 Aj,k,tn
p(Yl |X1l Tl )
l=1
n

(m,p)<(j,k)
n n
i (X1 X2 ; Y n |T n ) =

3

≤
s=1

(s)

Pr Zj,k (Y n )= 0 Aj,k,tn +

(1)

Pr Zm,k (Y n )= 1 Aj,k,tn

log
l=1

p(Yl |X1l X2l Tl )
p(Yl |Tl )

l=1
n

i3l ,
l=1

m<j,p=k

(2)
(3)
+
Pr Zj,p (Y n )= 1 Aj,k,tn +
Pr Zm,p (Y n )= 1 Aj,k,tn
p<k,m=j
m<j,p=k

where for all l = 1, ..., n,
i1l ∼ i (X1 ; Y |X2 T ) ,

i2l ∼ i (X2 ; Y |X1 T ) ,

i3l ∼ i (X1 X2 ; Y |T ) ,

where the notation Aj,k,tn in the conditioning is a shorthand
n
n
for the event {X1 = xn (j), X2 = xn (k), T n = tn }, and we
1
2
have used the deﬁnition of the LRT’s and the union bound.
Not using the union bound for the ﬁrst three summands and
leaving them as a joint outage event, while keeping the rest
of proof unchanged, will result in the generalized bound (2).
Now, since the time sharing sequence is generated i.i.d
according to distribution p(t) and all the codewords in the ﬁrst,
resp. second, codebook are generated independently according
to the distribution p(x1 |t), resp. p(x2 |t), we can take the
average of the above inequality over all possible time sharing

and their mean and variance can be described in terms of
the corresponding average mutual information and dispersion
terms as
E[i1l ] = I(X1 ; Y |X2 T ),

Var[i1l ] = V(X1 ; Y |X2 T ),

E[i2l ] = I(X2 ; Y |X1 T ),

Var[i2l ] = V(X2 ; Y |X1 T ),

E[i3l ] = I(X1 X2 ; Y |T ),

Var[i3l ] = V(X1 X2 ; Y |T ).

Assume that all three dispersions are strictly positive. In

4

such a case, we obtain using the Berry-Esseen Theorem that

the fact that a mutual information RV with zero dispersion
(variance) is concentrated almost surely at the average mutual
information (mean). For example, if the ﬁrst dispersion is zero,
V(X1 ; Y |X2 T ) = 0, then with the optimal choice of threshold

n
n
n
Pr [i (X1 ; Y n |X2 T n ) ≤ log γ1 ] = Pr

i1l ≤ log γ1
l=1

≤Q

nI(X1 ; Y |X2 T ) − log γ1
nV(X1 ; Y |X2 T )

B11
+ √ ,
n

log γ1 = log

(4)

M1 − 1
= nI(X1 ; Y |X2 T ) + log(λ1 ),
2
≤0

6S[i(X1 ;Y |X2 T )]
where the constant B11
, with S[·] being the
V(X1 ;Y |X2 T )3/2
third moment operator, represents the Berry-Esseen gap to the
Gaussian distribution. On the other hand, we can use a change
of measure technique as in [2]

the ﬁrst two summands on the RHS of the DT bound (1) can
be evaluated using a change of measure technique as in (5).

to obtain

n
n
Pr [i (X1 ; Y n |X2 T n ) ≤ log γ1 ]
M1 − 1
n ¯
n
+
Pr i X1 ; Y2n |X2 T n > log γ1
2
M1 − 1
exp{−nI(X1 ; Y |X2 T )} · 1 = λ1 .
=0+
2
Now, notice that the achievable rate is simply

n ¯
n
Pr i X1 ; Y2n |X2 T n > log γ1

log M1< nI(X1 ; Y |X2 T )−Q−1 (λ1 ) nV(X1 , Y |X2 T )+O(1),

Q

dP
dP
>γ = 1
> γ dQ =
dQ
dQ

n

= E exp −

−1

1

dP
> γ dP
dQ
(5)

n

i1l
l=1

dP
dQ

1

i1l > log γ1
l=1

B12 −1
≤ √ γ1 ,
n
(6)

=0

as in (3). This concludes the proof of Theorem 3.
VII. C ONCLUSION

where (6) is according to [2, Lemma 47].
By substituting (4) and (6) and the analogous bounds for
the other two mutual information RVs into the DT bound (1)
with the optimal selection for thresholds γ1 , γ2 , γ3 , we obtain
≤Q
+Q
+Q

nI(X1 ; Y |X2 T ) − log

M1 −1
2

nV(X1 ; Y |X2 T )
nI(X2 ; Y |X1 T ) − log

M2 −1
2

nV(X2 ; Y |X1 T )
nI(X1 X2 ; Y |T ) − log

B1
+√
n
B2
+√
n

(M1 −1)(M2 −1)
2

nV(X1 X2 ; Y |T )

We have proved a simple achievable rate region for DMMAC in the regime of ﬁnite blocklength by splitting the
allowed average error probability among several “outage”
events, in which the channel cannot support the target rates of a
subset of the users. This region appears to have a curved blunt
shape in general and implies rate penalties with respect to the
inﬁnite blocklength regime that depend on the allowed error
probability, the chosen ﬁnite blocklength, and the dispersions
of the DM-MAC, i.e., the variances of the relevant mutual
information RVs. We have observed that our achievable rate
region covers a signiﬁcant portion of the concurrent result
in [8], while its numerical computation is much easier. We
aim as a future direction to prove the tightness of an outer
bound we have developed and compare it with the existing
inner bounds.

B3
+√ ,
n

where B1 = B11 + B12 and analogously for B2 and B3 . Now,
splitting among the three ﬁrst terms of each line gives
B1
log M1≤ nI(X1 ; Y |X2 T )− nV(X1 ; Y |X2 T )Q−1 λ1 − √
n
B2
log M2≤ nI(X2 ; Y |X1 T )− nV(X2 ; Y |X1 T )Q−1 λ2 − √
n
log M1 +log M2 ≤ nI(X1 X2 ; Y |T )
B3
− nV(X1 X2 ; Y |T )Q−1 λ3 − √
(7)
n

R EFERENCES
[1] V. Strassen, “Asymptotische Abschatzungen in Shannon’s Informationstheorie,” in Transaction of the 3rd Prague Conference on Information
Theory, Prague, pp. 689–723, 1962.
[2] Y. Polyanskiy, H. V. Poor and S. Verd´ , “Channel Coding Rate in the
u
Finite Blocklength Regime,” IEEE Transactions on Information Theory,
vol. 56, no. 5, pp. 2307–2359, May 2010.
[3] M. Hayashi, “Information Spectrum Approach to Second-Order Coding
Rate in Channel Coding,” IEEE Transactions on Information Theory, vol.
55, no. 11, pp. 4947–4966, Nov. 2009.
[4] T.-S. Han, Information-Spectrum Methods in Information Theory,
Springer-Verlag, Berlin, Germany, 2003.
[5] A. El Gamal and Y.-H. Kim, Network Information Theory, Cambridge
University Press, UK, 2012.
[6] G. Dueck, “The Strong Converse to the Coding Theorem for the Multiple–
Access Channel,” J. Comb. Inform. Syst. Sci., vol. 6, pp. 187–196, 1981.
[7] R. Ahlswede, “An Elementary Proof of the Strong Converse Theorem for
the Multiple Access Channel,” J. Comb. Inform. Syst. Sci., vol. 7, no. 3,
pp. 216–230, 1982.
[8] V. Y. F. Tan and O. Kosut, “On the Dispersion of Three Network Information Theory Problems,” submitted to IEEE Transactions on Information
Theory, Available at http://arxiv.org/abs/1201.3901, Jan 2012.

where positive constants λ1 , λ2 , λ3 that sum up to 1 can
be arbitrarily chosen to represent the weight of each of the
three types of outage for communication over a DM-MAC
with average error probability . We can further simplify the
B
bounds in (7) using Taylor’s expansion Q−1 (λ − √n ) ≥
√
˜
Q−1 (λ ) + B/ n and the fact that dispersions are ﬁnite over
the set of distributions p(t)p(x1 |t)p(x2 |t), so V ≤ Vmax and
˜
˜
B ≤ Bmax , to obtain the bounds of Theorem 3.
In the case that one or more of the dispersion terms are zero,
we directly evaluate the corresponding probabilities, using

5


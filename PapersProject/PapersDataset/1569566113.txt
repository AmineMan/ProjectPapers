Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Tue May 15 22:50:06 2012
ModDate:        Tue Jun 19 12:56:31 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      423579 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566113

Error Exponents of Optimum Erasure/List and
Ordinary Decoding for Channels with Side
Information
Erez Sabbag

Neri Merhav

Densbits Technologies Ltd.
P.O. Box 15111,
Haifa 31015, Israel
Email: sabbage@gmail.com

Department of Electrical Engineering
Technion - Israel Institute of Technology
Technion City, Haifa 32000, Israel
Email: merhav@ee.technion.ac.il

with an erasure option. These bounds were obtained by
exploring a generalized maximum mutual information (MMI)
decoder [2, p. 164] which has an erasure option.
Recently, upper bounds on the error probabilities under
generalized decoding rules were provided for linear block
codes over memoryless symmetric channels in [3].
In a recent paper [4], a decoder with an erasure option and
a variable size list decoder for channels with non-casual side
information at the transmitter were considered. A universally
achievable region of error exponents was presented for decoding with an erasure option using a parameterized decoder
in the spirit of Csisz´ r and K¨ rner’s decoder. Moreover, the
a
o
proposed decoding rule was generalized by extending the
range of its parameters to allow variable size list decoding.
This extension gives a uniﬁed treatment for erasure/list decoding. An achievable region of exponential bounds on the
probability of list error and the average number of incorrect
messages on the list were given. The results were obtained
using a random binning code with conditionally constant
composition codewords proposed by Moulin and Wang [5]
with the proposed parameterized decoder.
While these results extended the work of Csisz´ r and K¨ rner
a
o
to channels with side information present at the transmitter
non-causally and generalized it to the case of variable list size
decoding, the results in [4] were based on heuristic decoding
rule with no proven relation to the optimal decoding rule.
Moreover, while in [1] both error exponents were achieved by
the same codebook distribution, in [4] the two error exponents
might be achieved by different codebook distributions. This
fact may indicate that the results of [4] are not tight. These
two weak spots of [4] were the main motivation for the current
work.
When reviewing Forney’s proof [1, p.218] it is evident
that the proof can be applied to the case of channels with
side-information present at the transmitter if we could handle
the analysis of two expressions: (i) EC P λ (y|m), namely, the
moments of the probability of the channel output given that
λ
the correct message was sent, and (ii) EC
m =m P (y|m )
the moments of the sum of the probabilities of all competing

Abstract—In this work we analyze achievable random coding
error exponents pertaining to erasure/list decoding for channels
with side information present at the transmitter. The analysis
is carried out using the optimal decoding rule proposed by
Forney. A key ingredient in the analysis is the evaluation
of moments of a certain distance enumerator. This approach
leads to a new exponentially tight bounds. These results are
obtained by exploring a random binning code with conditionally
constant composition codewords previously proposed by Moulin
and Wang. Later, these results are used to obtain an achievable
random coding error exponent for ordinary decoding.

I. I NTRODUCTION
Decoding with an erasure option and variable list size
decoding are generalizations of the ordinary decoding rule
in which the decoder gives one estimate for the transmitted
message. A decoder with an erasure option is a decoder which
has the option of not deciding, i.e., to declare an “erasure”. On
the other hand, a variable size list decoder is a decoder which
produces a list of estimates for the correct message rather than
a single estimate, where a list error occurs when the correct
message is not on the list. In [1], Forney analyzed random
coding error exponents of erasure/list decoding for discrete
memoryless channels (DMC’s). These bounds were obtained
by exploring the optimal decoding rule [1, eq. (11)]
y ∈ Rm iff Pr(y, xm ) ≥ eN T

Pr(y, xm )

(1)

m =m

where Pr(y, xm ) is the joint probability of the channel output
y and the codeword xm , Rm is the decision region of message
m, and T is an arbitrary parameter. The bounds were obtained
using Gallager’s bounding techniques. It was shown that the
erasure option and the list option are “two sides of the same
coin”, namely, by changing the value of T one can switch
from list decoding (T < 0) to decoding with an erasure option
(T > 0).
Later, Csisz´ r and K¨ rner [2, p.175, Th. 5.11] derived for
a
o
DMCs universally achievable error exponents for a decoder
E. Sabbag was with the Department of Electrical Engineering, Technion,
Haifa 32000, Israel. This paper is part of his Ph.D. thesis.

1

ˆ
ˆ
is the vector Pxy = Pxy (a, b), a ∈ X , b ∈ Y

messages, where the expectation is taken with respect to the
code ensemble. Forney tackled these expressions using the
H¨ lder’s and Jensen’s inequalities and the fact that the codeo
words were chosen in a symbol-by-symbol fashion. However,
when the binning technique is introduced to the encoding rule
x = f (m, s), which maps a channel state sequence s and a
message index m into a channel input sequence x using an
auxiliary sequence u, Forney’s analysis does not seem to be
applicable.
To overcome these difﬁculties a different approach was
taken. In this work we apply the distance enumerator approach
(see [6] for detailed tutorial on the subject). This approach
was applied in recent years to other problems in Information
Theory with considerable success (see [7], [8], [9], [10]). In
[11], the analysis of random coding error exponents for erausre/list decoding for DMCs was revisited, where the distance
enumerator approach was used instead of the use of H¨ lder’s
o
and Jensen’s inequalities as done by Forney. The resulting
bounds, which are at least as tight as Forney’s bounds, are
simpler in the sense that it involves an optimization over one
parameter only as opposed to Forney’s bounds which involves
two parameters. Moreover, when applying this technique to a
certain universal decoder with erasures the tightness of these
bounds is exempliﬁed.
In this work we use the distance enumerator approach to
analyze the optimal decoding rule proposed by Forney while
adopting a random binning code with conditionally constant
composition codewords (CCC) proposed by Moulin and Wang
[5]. Later, these results are applied on Gallager’s well-known
upper bound presnted in [12] to get an achievable error
exponent for the above channel model and code construction.
The outline of this work is the following: In Section II we
present some notation which will be used throughout the paper,
present the channel model, describe the codebook construction
and the decoding rule which will be used during this paper. In
Section III we present the main result of the paper. Section IV
describes the case of ordinary decoding rule and presents
the corresponding achievable error exponent. In Section V
we discuss the results and present some directions for future
research.

1
ˆ
Pxy (a, b) =
n

n

1 xi = a, yi = b ,

where

x ∈ X, y ∈ Y ,

i=1

ˆ
i.e., Pxy (a, b) is the relative frequency of the pair (a, b) along
the pair sequence (x, y). Likewise, the type class Txy is the
ˆ˜ ˜
ˆ
set of all pairs (˜ , y ) ∈ X n × Y n such that Pxy = Pxy . The
x ˜
conditional type class Ty |x , for given vectors x ∈ X n , and
˜
y ∈ Y n is the set of all vectors y ∈ Y n such that Txy = Txy .
˜
The Kullback-Leibler divergence between two distributions P
and Q on B, where |B| < ∞ is deﬁned as
D(P Q) =

P (b) ln
b∈B

P (b)
,
Q(b)

with the conventions that 0 ln 0 = 0, and p ln p = ∞ if p > 0.
0
ˆ
We denote the empirical entropy of a vector x ∈ X n by H(x),
ˆ
ˆx (a) ln Px (a). Other information
ˆ
where H(x) = − a∈X P
theoretic quantities governed by empirical distributions (e.g.,
conditional empirical entropy, empirical mutual information)
will be denoted similarly.
A. Channel Model and Code Construction
In this section, we describe the channel model and the code
construction. Note that the channel model is the same channel
model described in [4], and the code construction is almost
identical. However, the decoding rule used in this work is
based on the optimal decoding rule, while in [4] a sub-optimal
decoding rule was used.
We consider a discrete memoryless state-dependent channel with a ﬁnite input set X , a ﬁnite state alphabet S, a
ﬁnite output alphabet Y, and a transition probability distribution W (y|x, s). Given an input sequence x and a state
sequence s, emitted from a discrete memoryless source
N
PS (s) = i=1 PS (si ), the channel output sequence y is generated according to the conditional distribution W (y|x, s) =
N
i=1 W (yi |xi , si ). A message m ∈ {1, . . . , M }, where
M = exp{N R} and R is the code rate, is to be transmitted
to the receiver. We assume that the state sequence s is available
at the transmitter non-causally, but not at the receiver. We also
assume that all messages are a-priori equiprobable. Given s
and m, the transmitter produces a sequence x = fN (s, m)
which is used to convey message m to the decoder. We note
that the channel is ﬁxed and known to all parties, as presented
in the work of Gel’fand and Pinsker [13].
In [5, p. 1337], Moulin and Wang used in their derivation
a binning codebook with conditionally constant composition
(CCC) codewords. A similar codebook will be used in our
proofs. For the sake of completeness, we brieﬂy describe
the codebook construction and the encoding process. The
decoding part will be described in detail later. The codebook
construction requires the use of an auxiliary random variable
U ∈ U which takes on values in a ﬁnite set of size |X ||S| as
will be shown later.
ˆ∗
For a given empirical conditional distribution Pu|s , a subˆ
codebook C(Ps ) is constructed for each state sequence type

II. N OTATION AND P RELIMINARIES
We begin with some notation and deﬁnitions. Throughout
this work, capital letters represent scalar random variables
(RVs), and speciﬁc realizations of them are denoted by the
corresponding lowercase letters. Random vectors of dimension
N will be denoted by bold-face letters. The notation 1{A},
where A is an event, will designate the indicator function of
A (i.e.,1{A} = 1 if A occurs and 1{A} = 0 otherwise).
ˆ
ˆ
Let the vector Px = Px (a), a ∈ X denote the empirical
ˆ
distribution induced by a vector x ∈ X n , where Px (a) =
n
1
1{xi = a}. The type class Tx is the set of vectors
i=1
n
ˆ˜
ˆ
˜
x ∈ X n such that Px = Px . A type class induced by the emˆ
ˆ
pirical distribution Px will be denoted by T (Px ). Similarly,
the joint empirical distribution induced by (x, y) ∈ X n × Y n

2

ˆ
ˆ
class Ts = T (Ps ). Given a state type class T (Ps ), compute
the marginal distribution

In [1], Forney showed that the optimal tradeoff between the
two error events is attained by the following rule [1, p.208]:

ˆ
ˆ∗
Pu|s (u|s)Ps (s) ,

ˆ∗
Pu (u) =
s

ˆ
where Ps is the empirical distribution induced by Ts . Note
∗
ˆu (u) is a function of Ps and it might be different for
ˆ
that P
ˆ
other state type classes. Draw exp{N [R + ρ(Ps )]} random
∗ ˆ
vectors independently from the type class TU (Ps ) induced
ˆ∗
ˆ
by Pu , according to uniform distribution where ρ(Ps ) =
∗
ˆs ) + , and
IU S (P
∗
ˆ
I U S ( Ps ) =
u,s

This choice ensures that the probability of encoding error
vanishes at a double-exponentially rate [5, p. 1338]. Arrange
the vectors in an array with M = exp{N R} columns and
ˆ
exp{N ρ(Ps )} rows. The codebook C is the union of all
ˆ
sub-codebooks, i.e., C = P C(Ps ). Note that the number of
ˆ
s
these sub-codebooks is polynomial in N (at most (N + 1)|S| ).
To encode message m given a state sequence s, the following two steps are done: (i) Find an index l such that
ˆ
ul,m ∈ C(Ps ) is a member of the conditional type class
∗
ˆ
ˆ∗
ˆ
Tul,m |s = {u : Pu s = Pul,m |s Ps }. If more than one such
l exists, pick one at random under the uniform distribution. If
∗
no such l can be found, pick u at random from Tu|s under
the uniform distribution. (ii) The channel input sequence is
given by x = x(s1 , u1 ), x(s2 , u2 ), . . . , x(sN , uN ) where
x : S × U → X is some ﬁxed function. Since there is a
deterministic mapping from S × U we can deﬁne a direct
˜
channel W : S × U → Y as

where U (s, m) is the encoding rule. Following the ﬁrst steps
of the derivation of [1, p.218], for a given codebook we get:
Pr{E1 } =

=

≤

Pr{E2 }

=

1
M

=

P (y|m)

(6)

m=1 y ∈Rc
m

M

P (y|m)1

eN T

M

P (y|m)

m =m

P (y|m )

P (y|m)

m=1 y ∈Y N

eN T

m =m

≥1
λ

P (y|m )

P (y|m)

m=1 y ∈Y N

eN λT
M

M

λ

¯

P λ (y|m)
m=1 y ∈Y N

P (y|m )
m =m
λ

¯

P λ (y|m)
y

P (y|m )

∈Y N

(8)

m =m

¯
where 0 ≤ λ ≤ 1 is an arbitrary parameter, and λ
Similarly, for E2 we get that
¯

P λ (y|m)
y

∈Y N

1 − λ.
λ

¯

Pr{E2 } ≤ e−N λT

P (y|m )

. (9)

m =m

Notice that the difference between these two expressions is
given by a constant factor e−N T , therefore, we will concentrate on achieving an upper bound on Pr{E1 }. It is important to
note that starting this point on, our derivation is exponentially
tight. Now, taking the expectation with respect to the code
ensemble and using the fact that the encoding rule U (s, m)
(i.e., the process of choosing u given m and s) is independent
of all other encoding rules U (s, m ), we get that

(2)

m=1 y ∈Rc
m
M

(3)

m=1 y ∈Rm m =m

λ

¯

Pr{E1 } ≤ eN λT

where P (y|m) =
s∈S N PS (s)W y|xm (s), s , and the
average probability of erasure is given by:
Pr{R0 } = Pr{E1 } − Pr{E2 } .

1
M

M

= eN λT

M

P (y|m)

1
M

1
M

(7)

Given a codebook C, a decoder with an erasure option is
a partition of Y N into (M + 1) regions R0 , R1 , . . . , RM .
The decoder decides in favor of message m if y ∈ Rm ,
m = 1, . . . , M , or it declares “erasure” if y ∈ R0 . Following
Forney [1], let us deﬁne two error events. The event E1 is
the event in which y does not fall in the decision region of
the correct message, namely, the event in which the decoder
decides wrongly. The event E2 is the event of undetected error,
namely, the event in which y falls in Rm , m = 0, while
m = m was transmitted. The average probabilities of these
error events are given by
P (y|m)

m ∈ {1, . . . , M }

s∈S N

B. Optimal Decoding Rule

=

˜
PS (s)W y|s, U (s, m) ,

P (y|m) =

˜
W (y|s, u) = W y|s, x(s, u) .

Pr{E1 }

(5)

otherwise, declare “erasure” ( i.e., y ∈ R∗ ), where T ≥ 0 is
0
a parameter which controls the trade-off between E1 and E2 .
When taking a closer look on the decoding rule derived by
Forney [1, p.208], it is clear that these results can be applied
to the channels with side-information available at the encoder
non-causally as describe above where the only difference is
that the probability of received channel output y given a
message index m, denoted by P (y|m), is more involved.
For a given channel output sequence y and a message index
m deﬁne

∗
Pu|s (u|s)
∗
ˆ
.
Ps (s)Pu|s (u|s) log
ˆ∗
Pu (u)

1
M

P (y|m)
≥ exp{N T } ,
m =m P (y|m )

y ∈ R∗ iff
m

EC P λ (y|m) · EC
y ∈Y N

P (y|m )

,

m =m

where EC {·} designates the expectation operator with respect
to the code ensemble.

(4)

3

III. M AIN R ESULTS

IV. O RDINARY D ECODING RULE

For a given joint distribution QSU Y on S × U × Y, a
conditional distribution WY |SU : S × U → Y and a nonnegative constant γ deﬁne

The technique used in the proof of Theorem 1 can also
be used to derive an achievable error exponent for ordinary
decoding, i.e., a decoder which gives a single estimate for
the transmitted message without the ability to declare “erase”,
when the same channel model and code-construction presented
in Section II-A are used.
Again, by re-examining the derivation of Gallager’s upper
bound on the error probability presented in [16, Sec.2.4], it
can be seen that the same results apply to channels with sideinformation available at the encoder non-causally where the
only difference is that the probability of a received channel
output y given a channel input xm (denoted by P (y|xm ))
should be replaced by the probability of a received channel
output y given a message index m (denoted by P (y|m)).
Hence, we get that

J QSU Y , WY |SU , γ = IQSU Y (U, S; Y ) − γEQ ln WY |SU ,
where EQ is the expectation operator associated with Q. For
a given constant K ≥ 0, a conditional distribution PU |S on
S × U and a distribution PY on Y, deﬁne the following set of
distributions
G(PY , K) = QSU |Y
QU |S = PU |S , IQ (U ; S) − IQ×PY (U ; Y ) + K ≥ 0 , (10)
and deﬁne the complement of G(PY , K), denoted by
G c (PY , K), as

1
Pr{E} ≤
M

G c (PY , K) = QSU |Y
QU |S = PU |S , IQ (U ; S) − IQ×PY (U ; Y ) + K < 0 . (11)

max

− N max

0≤λ≤1

min

x:S×U →X PY ∈PN (Y)
P ∗ (U |S)

P

Pr{E} ≤

˜
Ecd (λ, R, PS , PY , WY |SU ) − H(PY )

EC P

. (12)

Pr{E} ≤ exp

QSU |Y ∈G(PY ,0)c

min

QSU |Y ∈G(PY ,0)

¯
λ D(QS PS )

1
1+ρ

(y|m)EC

max

ρ>0

min

Eab (

,

1
˜
, PS , PY , WY |SU )+
1+ρ

min

QSU |Y ∈G(PY ,R)

˜
− R + J QSU |Y × PY , WY |SU , 1

min

,

QSU |Y ∈G(PY ,R)c

1
1+ρ

ρ D(QS PS )

,

ρD(QS PS ) − ρIQ×PY (S; U, Y ) − R+
¯

˜
J QSU |Y × PY , WY |SU ,

(14)

min

QSU |Y ∈G(PY ,R)

˜
− R + J QSU |Y × PY , WY |SU ,

λ D(QS PS )

¯
λD(QS PS ) − λIQ×PY (S; U, Y ) − R+
.

. (15)

1
˜
where Eab ( 1+ρ , PS , PY , WY |SU ) is deﬁned in eq.(13) and

(13)

˜
˜
Ecd (ρ, R, PS , PY , W ) = min

˜
J QSU |Y × PY , WY |SU , λ

.

− N max

˜
˜
Ecd (λ, R, PS , PY , WY |SU ) − H(PY )

and

min

(y|m )

,

¯
˜
J QSU |Y × PY , WY |SU , λ

QSU |Y ∈G(PY ,R)c

P

1
1+ρ

m =m

x:S×U →X PY ∈PN (Y)
P ∗ (U |S)

¯
λD(QS PS ) − λIQ×PY (S; U, Y )+

˜
Ecd (λ, R, PS , PY , W ) = min

(y|m )

The following theorem provides an upper bound for the
average decoding error.
Theorem 2:

where

min

P
m =m

y ∈Y N

˜
Eab (λ, PS , PY , WY |SU )+

˜
+ J QSU |Y × PY , WY |SU , 1

(y|m)

m=1 y ∈Y N

1
1+ρ

ρ

− λT +

˜
Eab (λ, PS , PY , WY |SU ) = min

1
1+ρ

for ρ > 0. Taking the expectation with respect to the code
ensemble, and using the fact that U (s, m) is independent of
all other encoding rules U (s, m ), we get

Theorem 1:
Pr{E1 } ≤ exp

ρ

M

1
1+ρ

.

(16)

The results of the above theorem 1
were achieved
by
analyzing
two
moments
EC P 1+ρ (y|m)
and
1
ρ
EC
P 1+ρ (y|m )
using the same machinery
m =m
used to derive Theorem 1 presented in the previous section.

The complete proof of the Theorem can be found in [14], [15].
We brieﬂy describe the main items of the proof.

4

V. D ISCUSSION AND F UTURE W ORK

bounded by
ˆ
exp{N ρ(Ps )}

Theorem 1 gives achievable error exponents of decoding
with erasure/list decoding using the optimal decoding rule,
originally proposed by Forney for DMCs, for the Gel’fandPinsker channel. The main approach used in the analysis is
the distance enumerator method which was applied in [11]
to obtain new error exponents of erasure/list decoding for
DMCs. Theorem 2 is achieved using the same machinery.
Both results originated from an optimal decoding rule (the
use of Forney’s approach in the ﬁrst theorem, and Gallager’s
approach in the second). Additional generalization can be
made to the way the channel input sequence x is chosen.
While in [4], x was chosen randomly from the conditional type
∗
class Tx|su induced by a conditional distribution P ∗ (X|U, S)
which can be optimized, in this work the encoder computes
the channel input sequence x using a deterministic symbol-bysymbol function xi = f (si , m) (see Section II-A). While in
[13] such a deterministic mapping was sufﬁcient to achieve the
capacity of the channel it is not clear whether a deterministic
mapping is optimal also in the error exponent regime.

Ly (QSU |Y ) ≤

Zm,l (QSU |Y , y) ,
l=1

where Zm,l (QSU |Y , y) is a random variable which takes on
two values:
0
, um,l ∈ Tu|y (QU |Y )
/
Zm,l (QSU |Y , y) =
.
|Ts|uy | , um,l ∈ Tu|y (QU |Y )
Hence, we can rewrite Ly (QSU |Y ) as an exponential sum of
independent and identically distributed binary RV:
eN (I

∗ (U ;S)+ )

˜
Zm,l (QSU |Y , y)

Ly (QSU |Y ) ≤ |Ts|y |
l=1

˜
where Zm,l (QSU |Y , y) is a binary RV, and
.
˜
Pr Zm,l (QSU |Y , y) = 1 = exp − N IQ×P (U ; Y ) .
ˆ
y
The rest of the proof is mainly technical. It follows the distance
enumerator method with similar guidelines shown in [11].
R EFERENCES

VI. P ROOF OF T HEOREM 1: O UTLINE

[1] G. D. Forney, “Exponential error bounds for erasure, list, and decision
feedback schemes,” IEEE Trans. Inf. Theory, vol. 14, no. 2, pp. 206–220,
Mar. 1968.
[2] I. Csisz´ r and J. K¨ rner, Information Theory: Coding Theorems for
a
o
Discrete Memoryless Systems. Academic Press, 1981.
[3] E. Hof, I. Sason, and S. Shamai, “Performance bounds for erasure, list
and feedback schemes with linear block codes,” IEEE Trans. Inf. Theory,
vol. 56, no. 8, pp. 3754–3778, Aug. 2010.
[4] E. Sabbag and N. Merhav, “Achievable error exponents for channels with
side information – erasure and list decoding,” IEEE Trans. Inf. Theory,
vol. 56, no. 11, pp. 5424–5431, Nov. 2010.
[5] P. Moulin and Y. Wang, “Capacity and random-coding error exponents
for channel coding with side information,” IEEE Trans. Inf. Theory,
vol. 53, no. 4, pp. 1326–1347, Apr. 2007.
[6] N. Merhav, “Statistical physics and information theory,” Foundations
and Trends in Communications and Information Theory, vol. 6, no. 1-2,
pp. 1–212, 2009.
[7] ——, “Relations between random coding exponents and the statistical
physics of random codes,” IEEE Trans. Inf. Theory, vol. 55, no. 1, pp.
83–92, Jan. 2009.
[8] Y. Kaspi and N. Merhav, “Error exponents for broadcast channels
with degraded message sets,” IEEE Trans. Inf. Theory, submitted for
publication, May 2009.
[9] R. Etkin, N. Merhav, and E. Ordentlich, “Error exponents of optimum
decoding for the interference channel,” IEEE Trans. Inf. Theory, vol. 56,
no. 1, pp. 40–56, Jan. 2010.
[10] A. S. Baruch and N. Merhav, “Exact random coding exponents for
erasure decoding,” in Proc. Int. Symp. on Information Theory (ISIT’10),
Austin TX, USA, Jun. 2010, pp. 260–264.
[11] N. Merhav, “Error exponents of erasure/list decoding revisited via
moments of distance enumerators,” IEEE Trans. Inf. Theory, vol. 54,
no. 10, pp. 4439–4447, Oct. 2008.
[12] R. Gallager, “A simple derivation of the coding theorem and some
applications,” IEEE Trans. Inf. Theory, vol. 11, no. 1, pp. 3–18, Jan.
1965.
[13] S. Gel’fand and M. Pinsker, “Coding for channels with random parameter,” Problems of Information and Control, vol. 9, no. 1, pp. 19–31,
1980.
[14] E. Sabbag and N. Merhav, “Achievable error exponents for channels
with side information - erasure and list decoding,” In preparation. To
be submitted to IEEE Trans. on Inf. Theory, 2012.
[15] E. Sabbag, “Topics in channel reliability and error exponent analysis,”
Ph.D. dissertation, Technion - Israel Institute of Technology, 2011.
[16] A. J. Viterbi and J. K. Omura, Principles of Digital Communication and
Coding. New York: McGraw-Hill, 1979.

The main challenge of the proof is to analyze two main
λ
¯
expressions: EC P λ (y|m) and EC
m =m P (y|m ) . We
brieﬂy describe how we tackle the ﬁrst expression. Analyzing
the second expression is more involved, but, uses the same
¯
λ
¯
guidelines. Starting with EC Pm , where λ 1 − λ:


¯
λ


¯
˜
EC P λ y|m = EC
PS (s)W y|s, U m (s)


s

λ 
¯




˜
PS (s ∈ Ts )
W y|˜, U m (˜) 
s
s
= EC 



 T ∈PN (S)
˜
s∈Ts
s
PS (s ∈ Ts )

= EC
Ts ∈PN (S)

˜
Ly (QSU |Y ) · exp{N EQ ln W (Y |S, U )}

¯
λ

ˆ
QSU |Y :QS =P (Ts )

.
=

max
Ts ∈PN (S)

max

QSU |Y :
ˆ
QS =P (Ts )

¯

¯

λ
PS (s ∈ Ts )EC Lλ (QSU |Y )
y

¯
˜
· exp λN EQ ln W (Y |S, U )
where Ly (QSU |Y ) is the number of codewords corresponding
ˆ
to message m in the sub codebook C(P (Ts )) whose joint
conditional empirical distribution with y is QSU |Y . Note that
by introducing the term Ly (QSU |Y ) the two summations
are taken over subexponential number of terms which in
turn enables the simpliﬁcation of the expressions (a similar
expression is introduced in the second part of the proof which
simpliﬁes the additional summtion over all messgaes m = m.
See [14]). Later, it can be shown that Ly (QSU |Y ) is upper

5


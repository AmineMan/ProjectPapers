Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Tue May 15 10:32:05 2012
ModDate:        Tue Jun 19 12:55:20 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      485715 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565867

Non-coherent Network Coding:
An Arbitrarily Varying Channel Approach
Mahdi Jafari Siavoshani∗ , Shenghao Yang† , Raymond W. Yeung‡
∗ Ecole

Polytechnique F´ d´ rale de Lausanne
e e
Email: mahdi.jafarisiavoshani@epfl.ch
† ITCS, Institute for Interdisciplinary Information Sciences, Tsinghua University
Email: shyang@tsinghua.edu.cn
‡ Institute of Network Coding, The Chinese University of Hong Kong
Email: whyeung@ie.cuhk.edu.hk
non-coherent linear network coding channel is modeled by
a multiplicative matrix channel.
Montanari et al. [5] introduced a probabilistic model to
capture the end-to-end functionality of non-coherent network
coding operation, with a focus on the case of error correction
capabilities. Jafari et al. [6], [7], [8] modeled the non-coherent
network coding channel by assuming that the transfer matrix
has i.i.d. entries selected uniformly at random in every timeslot. They showed that coding over subspaces is sufﬁcient
to achieve the capacity. Moreover, they obtained the channel
capacity as a solution of a convex optimization problem
over O(min[M, N ]) variables and when the ﬁeld size is
greater than a threshold, they characterized the capacity by
solving the optimization problem. Silva et al. [9] derived
the capacity of the multiplicative ﬁnite ﬁeld matrix channel
under the assumption that the transfer matrix is square and
chosen uniformly at random among all full-rank matrices.
Similarly, in this model the coding over subspaces is sufﬁcient
to achieve the capacity. Yang et al. [10], [11] (see also [12],
[13]) considered a completely general scenario, making no
assumption on the distribution of the transfer matrix. They
obtained upper and lower bounds on the channel capacity,
and give a sufﬁcient condition on the distribution of the
transfer matrix such that coding over subspaces is capacity
achieving. They also studied the achievable rates of coding
over subspaces. Nobrega et al. [14] considered the case where
the probability distribution of the rank of the transfer matrix
is arbitrary; however all matrices with the same rank are
equiprobable. Then, following an approach similar to [8], they
expressed the capacity as the solution of a convex optimization
problem over O(min[M, N ]) variables. They also observed
that in this case the subspace codes are sufﬁcient to achieve
the capacity.
In most of the previous works, only certain probability
models for the channel transfer matrix have been discussed.
However, in practice a complete probabilistic characterization
of the matrix channel is difﬁcult and the network may not
follow a given probability model. Instead of assuming a
complete probability model, we consider in this paper that
only a partial knowledge about the probabilistic model of the
channel is known.

Abstract—In this paper, we propose an “arbitrarily varying channel” (AVC) approach to study the capacity of noncoherent transmission in a network that employs randomized
linear network coding. The network operation is modeled by
a matrix channel over a ﬁnite ﬁeld where the transfer matrix
changes arbitrarily from time-slot to time-slot but up to a known
distribution over its rank. By extending the AVC results to
this setup, we characterize the capacity of such a non-coherent
transmission scheme and show that subspace coding is optimal
for achieving the capacity.
By imposing a probability distribution over the state space of
an AVC, we obtain a channel which we called “partially arbitrarily varying channel” (PAVC). In this work, we characterize the
“randomized” as well as the “deterministic” code capacity of a
PAVC under the average error probability criterion. Although we
introduce the PAVC to model the non-coherent network coding,
this extension to an AVC might be of its own interest as well.

I. I NTRODUCTION
Randomized linear network coding [1] is an efﬁcient and
practical approach to implement network coding [2], [3] in
large dynamically changing networks because it does not
require a priori the knowledge of the network topology.
However, in order to enable the receivers to decode, to each
packet a coding vector is appended to learn the transfer matrix
induced by the network.
A different approach, other than using coding vectors, is
to assume a non-coherent scenario for communication, as
proposed in [4], where neither the source(s) nor the receiver(s)
have any knowledge of the network topology or the network
nodes operations. Non-coherent communication allows creation of end-to-end systems that are completely oblivious to
the network state. In [4], the authors proposed communications
via choosing subspaces and they introduced a subspace channel called “operator channel” (a channel which has subspaces
as input and output symbols). Then, they focused on algebraic
subspace code constructions over a Grassmannian for the
operator channel.
Following [4], different probabilistic models have been proposed to model the non-coherent randomized linear network
coding channel, where these models enable one to deﬁne
and characterize the capacity for such a channel. In all of
these works, when there are no errors in the network, the

1

More precisely, we assume that the rank distribution of
the transfer matrix is known a priori, but the distribution of
matrices among each rank is unknown and arbitrary. Though
very similar to the arbitrarily varying channel (AVC) model
introduced in [16] (refer to [17] and the references therein),
but this non-coherent network coding model is not exactly
an AVC. We introduce a “partially arbitrary varying channel”
(PAVC) to capture the statistical property of this non-coherent
network coding model.
By extending results for the AVC, we obtain the capacities
of the PAVC for randomized and deterministic codes (Theorem 1 and 2). We further show that the randomized and
the deterministic code capacities of the non-coherent network
coding model are the same (Theorem 3), and that subspace
coding is sufﬁcient to achieve the capacity (Corollary 4). This
AVC approach to the non-coherent network coding provides a
justiﬁcation for the optimality of subspace coding in a more
general setting.

probability is
n
n
Wm

Wm (y[t]|x[t]; h[t]) ,
t=1

where Wm (y|x; h) 1{y=xh} is a stochastic matrix.
The above model is very similar to an arbitrarily varying
channel (AVC) model (refer to [17] for more information about
AVC) but it does not completely ﬁt into that model. In this
work, we will show that it is indeed possible to extend the
AVC concepts and results for the above channel model and
characterize its capacity.
C. Partially Arbitrarily Varying Channel (PAVC)
Before deﬁning a partially arbitrarily varying channel
(PAVC), let us ﬁrst consider an AVC model. Let X ∈ X
and Y ∈ Y denote the input and output symbol of a channel
where X and Y are ﬁnite sets denoting the channel input and
output alphabets, respectively. Let us consider a transmission
scenario where the channel parameters vary arbitrarily from
symbol to symbol during the course of a transmission. More
precisely, for the channel transition matrix, we can write

II. P ROBLEM S ETUP AND N OTATION
A. Notation
We use bold letters to denote vectors and matrices. For
convenience of notation, we use [i : j] to denote the set
{i, i + 1, . . . , j − 1, j} where i, j ∈ Z. Let Uni(M) denote the
uniform distribution over the set M. For m × n matrices over
Fq , we use Uni(Fm×n , r) to denote the uniform distribution
q
over all m × n matrices with rank r.

n
n

W (yt |xt ; st ),

W (y|x; s)

(2)

t=1

where s = (s1 , . . . , sn ), si ∈ S, and W : X × S → Y is
a given stochastic matrix. S is a ﬁnite set, often referred to
as the state space. This model, called a “discrete memoryless
arbitrarily varying channel,” will be referred to as an AVC.
Now, we deﬁne a PAVC as an AVC with a probability
constraint over the state space S. Deﬁne a function q : S → Q
where Q {0, . . . , m} and deﬁne a random variable Q with
alphabet Q whose distribution is known by the encoder and
the decoder. For a PAVC, we have q(St ), t = 1, 2 . . ., are
independent and follow the same distribution of Q. In other
words,

B. Non-coherent Network Coding Channel Model
Consider a unicast communication over a network where
the relay nodes perform random linear network coding over a
ﬁnite ﬁeld Fq . Suppose that time is slotted and the channel is
block time-varying. At every time-slot, the source injects M
packets X 1 [t], . . . , X M [t] of length T symbols from Fq into
the network, i.e., X i [t] ∈ FT . The receiver collects N packets
q
Y 1 [t], . . . , Y N [t] and aims to decode the transmitted packets.
We use matrices X[t] and Y [t] to denote respectively, the
transmitted and received packets, i.e., the ith column of these
matrices represent the ith transmitted and received packets,
respectively. For a unicast communication, at time-slot (block)
t, the receiver observes
Y [t] = X[t]H[t],

(y[1 : n]|x[1 : n]; h[1 : n]) =

n

Pq(S) (q1 , . . . , qn ) =

PQ (qt ),
t=1

where q(S)
(q(S1 ), . . . , q(Sn )). We call this model a
“discrete memoryless partially arbitrarily varying channel,”
and will refer to it as a PAVC.
In this work, we are interested in characterizing the capacity
of a PAVC. However, we ﬁrst have to deﬁne the capacity. As
there are different notions of capacity for an AVC based on
different error criteria, the same is true for a PAVC (for more
information refer to [17]).
Let the message set of a code be M = {1, . . . , K}. A
length n block code is deﬁned by a pair of mappings (ψ, φ),
where ψ : M → X n is the encoder mapping, and φ : Y n →
M ∪{0} is the decoder mapping, where the output 0 indicates
a decoding error. When this code is used on a PAVC and the
state sequence is s, the error probability for message i is

(1)

where X[t] ∈ FT ×M , Y [t] ∈ FT ×N , and H[t] ∈ FM ×N . We
q
q
q
assume that the channel transfer matrix H[t] is unknown to
both the transmitter and the receiver and it changes arbitrarily
from one block to another block with a constraint on its rank.
More precisely, the ranks of H[t], t = 1, 2, . . ., are independent and follow the same distribution of a random variable
R. The conditional distribution of H[t] given rk (H[t]) is
unknown and changes arbitrarily for different t. However,
we assume that the distribution of the random variable R
is known. We may consider the channel transfer matrix as
the channel state. For given h[1 : n] the channel transition

W n (y|ψ(i); s),

ed (i, s)
y: φ(y)=i

2

Accordingly, the average probability of error for a state
sequence s is
1
K

ed (s)
¯

A. Capacity of a PAVC
Before stating the deterministic code capacity of a PAVC,
we need the following deﬁnition.

K

ed (i, s).

(3)

Deﬁnition 2. A PAVC is called symmetrizable if for some
channel U : X × Q → S, and for every x, x , and y we have

i=1

Deﬁnition 1. A number R > 0 is called an achievable rate
for the given PAVC (for deterministic code and average error
probability criterion) if for every > 0, δ > 0, and sufﬁciently
large n, there exists a length n block code (ψ, φ) with

W (y|x; s)U (s|x , q(s)) PQ (q(s)) =
s

W (y|x ; s)U (s|x, q(s)) PQ (q(s)) .
s

1
log K > R − δ,
n
max E [¯d (S)]
e

Let U(X × Q → S) be the set of all such channels. If U(X ×
Q → S) = ∅ then the PAVC is called non-symmetrizable.

and

The following two theorems, which are proved using techniques similar to those in [18], [19], characterize the capacity
of the PAVC for the average error criterion. Due to space limit,
we refer the reader to [20] for the proof of Theorem 1 and 2.

PS|q(S)

ed (s)PS|q(S) (s|q(s)) PQn (q(s)) ≤ ,
¯

max

PS|q(S)

s
n

where PQn (q)
t=1 PQ (qt ). The maximum achievable rate
d,a
is called the capacity of the PAVC and is denoted by Cpavc
(where superscript “a” denotes the average error probability
criterion in (3) and “d” denotes determinist code).

d,a
Theorem 1. For the deterministic code capacity Cpavc we have
d,a
Cpavc > 0 if and only if the PAVC is non-symmetrizable. If
d,a
Cpavc > 0, then
d,a
¯
¯
Cpavc = max min I(PX , WS ) = min max I(PX , WS ),
PX PS|q(S)

Remark: Note that if there is no probability constraint on
the state space in Deﬁnition 1 (i.e., PS instead of PS|q(S) is
unknown), then by replacing PS|q(S) in the maximization by
PS , we recover the average error criterion for an AVC, namely,
maxPS E [¯d (S)] ≤ is equivalent to maxs ed (s) ≤ .
e
¯
In contrast to using deterministic codes, there exists another
communication technique called randomized coding which can
provide improvement in performance if a common source of
randomness is available between the source and the receiver.
Precisely, a randomized code (Ψ, Φ) is a random variable
with values in the family of all length n block codes (ψ, φ),
deﬁned earlier in this section, with the same message set M.
Let us deﬁne

(4)
where
¯
WS (y|x)

¯
and I(PX , WS )
I(X; Y ) such that Y is connected to X
¯
through the channel WS .
Theorem 2. The randomized code capacity of the PAVC,
r,a
denoted by Cpavc , is given by (4).
Remark: Same as an AVC, the randomized code capacity of
a PAVC for the maximum and the average error probability
criteria are the same.
B. Capacity of Non-coherent Network Coding
According to the deﬁnition of the PAVC in §II-C, the noncoherent network coding model deﬁned by (1) is a PAVC
whose deterministic code capacity, as stated in Theorem 1,
can be characterized as follows.

When this code is used on a PAVC and the state sequence
is s, the error probability for message i is
EΨ,Φ [e(i, s, Ψ, Φ)],

Corollary 1. The deterministic code capacity of the channel
(1) is non-zero and is given by

and the average probability of error for a state sequence s is
er (s)
¯

1
K

W (y|x; s)PS|q(S) (s|q(s)) PQ (q(s)) ,
s

y: φ(y)=i

er (i, s)

E [W (y|x; S)]
=

W n (y|ψ(i); s).

e(i, s, ψ, φ)

PS|q(S) PX

C = max

K

min

PX PH| rk(H)

er (i, s).

I(X; Y ) =

min

max I(X; Y ), (5)

PH| rk(H) PX

if and only if the channel is non-symmetrizable, i.e., if there
is no stochastic matrix U : X × [0 : min[M, N ]] → H such
that we have

i=1

r,a
Similar to Deﬁnition 1, we deﬁne the capacity Cpavc by
replacing the function ed (s) with er (s).
¯
¯

min[M,N ]

Wm (y|x; h)U (h|x , r)PR (r) =

III. M AIN R ESULTS

r=0

h: rk(h)=r

min[M,N ]

Our main goal is to characterize the capacity of the noncoherent network coding channel described in §II-B. Toward
this end, we ﬁrst determine the capacity of a general PAVC.

Wm (y|x ; h)U (h|x, r)PR (r),
r=0

3

h: rk(h)=r

for all x ∈ FT ×M , x ∈ FT ×M , and y ∈ FT ×N .
q
q
q

Proof: Let G = AHB. Then

Similarly, using Theorem 2, the randomized code capacity
of the non-coherent network coding deﬁned by (1) is stated in
the following corollary.

PA (a)PB (b)PH (a−1 gb−1 ),

PG (g) =
a∈FM ×M ,b∈FN ×N ,
q
q
rk(a)=M,rk(b)=N

where PA (a) and PB (b) respectively do not depend on a and
b. Now, for another instance g of G with g = U gV for some
full rank matrices U and V , we can see that PG (g) = PG (g ).
In the following we show that if rk (g) = rk (g ), then there
exist full rank matrices U and V such that g = U gV .
Fix two decompositions g = bc and g = b c with rk (b) =
rk b = rk (g), which implies rk (c) = rk (c ) = rk (g).
Then there exist full rank square matrices U and V such that
U b = b and cV = c . Hence, g = U gV .

Corollary 2. The randomized code capacity of the channel
deﬁned by (1) is given by (5).
It is hard to show directly that the channel deﬁned by (1)
is non-symmetrizable. Instead, we prove this indirectly in the
next lemma by showing the existence of a coding scheme that
gives a non-zero transmission rate over the channel.
Lemma 1. If E [R] > 0, the channel deﬁned by (1) is nonsymmetrizable, and so by Corollary 1, its capacity is non-zero
and is given by (5). If E [R] = 0, then the capacity is zero.

Lemma 3. In the capacity expression (5), the u.g.r. distribution for PH| rk(H) is a minimizer for the expression.

Proof: The case for E [R] = 0 follows because H[t]
is the zero matrix with probability one. To show the nonsymmetrizability of the channel deﬁned by (1) when E [R] >
0, we construct a deterministic coding scheme that can achieve
a strictly positive rate. The idea is to degrade the channel
deﬁned by (1) to a binary memoryless Z-channel with a known
cross-over probability.
For each time slot t, let G[t] be a random matrix over F1×M
q
with uniform i.i.d. components. Deﬁne a binary-input binaryoutput channel as follows. Let B[t] be the input of the channel
at time t, which takes the value 0 or 1 in Fq . The output of
the channel at the time t is Y [t] = rk (B[t]G[t]H[t]). Since
the dimension of the matrix B[t]G[t]H[t] is 1 × N , Y [t] takes
the integer value 0 or 1. Let us check the transition matrix of
this channel. If B[t] = 0, then Y [t] = 0. If B[t] = 1, then
Y [t] = rk (G[t]H[t]). Note that rk (G[t]H[t]) is a random
variable whose distribution only depends on the distribution
of rk (H[t]) ∼ R (see the computation in [10, Section IV]).
Since rk (H[t]), t = 1, 2, . . . are independent, the channel is
a binary memoryless Z channel.
What remains is to check the cross over probability of the
Z channel given by

∗
Proof: Let PH| rk(H) be the distribution that minimizes
(5). Now consider a new channel deﬁned by AHB where
A ∼ Uni(FM ×M , M ) and B ∼ Uni(FN ×N , N ) are uniform
q
q
full rank random matrices (note that A, B, and H are
independent). Then by Lemma 2, the rank distribution of
AHB is the same as that of H, but AHB has a u.g.r.
distribution.
By the data processing inequality, the mutual information
between the input and output of the new channel is less than or
∗
equal to the original channel. So if PH| rk(H) is a minimizer,
then the u.g.r. distribution with the same rank distribution is
also a minimizer.
From Corollary 1, Corollary 2, Lemma 1, and Lemma 3 we
obtain the following theorem.

Theorem 3. The randomized and deterministic code capacities of the non-coherent network coding model, i.e., the matrix
channel deﬁned by (1), are the same and are equal to the
¯
¯
capacity of the matrix channel Y = HX where H has
the same rank distribution as H but has uniform distribution
among matrices having the same rank, i.e.,
C = max

P [Y [t] = 0|X[t] = 1] = P [rk (G[t]H[t]) = 0].

min

PX PH| rk(H)

¯
I(X; Y ) = max I(X; HX).
PX

Theorem 3 shows that, if only the knowledge of the rank
distribution of the transfer matrix is available, the maximum
rate that we can communicate over the channel deﬁned by (1)
is equal to the communication rate over a channel which has
the same rank distribution but the channel transfer matrix is
u.g.r.
Now, it is shown in [14, Theorem 16] that for a matrix
multiplicative channel with u.g.r. distribution over the transfer
matrix, the subspace coding is sufﬁcient to achieve the capacity. So we have the following result.

Since E [rk (H[t])] = E [R] > 0, P [rk (G[t]H[t]) = 0] < 1,
because otherwise H[t] is the zero matrix with probability
one, a contradiction to the assumption that E [R] > 0. Hence,
the channel has a positive capacity.
Deﬁnition 3 ([14]). A random matrix is called u.g.r. (uniform
given rank) if any two matrices with the same rank are
equiprobable.
Lemma 2. For any M × N random matrix H, AHB
is u.g.r. with the same rank distribution as of H, where
A ∼ Uni(FM ×M , M ) and B ∼ Uni(FN ×N , N ) are uniform
q
q
and full-rank random matrices, and A, B, and H are independent1 .

Theorem 4. Subspace coding [4] is sufﬁcient to achieve the
capacity (randomized and deterministic) of the non-coherent
network coding channel discussed in §II-B.
Although determining the exact value of the capacity in
Theorem 3 is still open, as shown in [14], the capacity can be

1 The result in Lemma 2 was also claimed in [14] without any proof (see
also [15, Theorem 5]).

4

expressed as the solution of a convex optimization problem
with only O (min[M, N ]) parameters which is computationally tractable. Indeed, to achieve the optimal communication
rate, the u.g.r. distribution over the input symbols (input
matrices X) is sufﬁcient.

University Grants Committee of the Hong Kong Special
Administrative Region, China (Project No. AoE/E-02/08). The
work of S. Yang was partially supported by the National Basic
Research Program of China through Grant 2011CBA00300,
2011CBA00302.
R EFERENCES

C. An Alternative Proof of Theorem 3
Here, we present an alternative proof for Theorem 3 that is
not based on the derived PAVC results2 .
We may consider the non-coherent network coding channel
introduced in §II-B as a matrix channel where the transfer
matrix is chosen arbitrarily by an adversary, in such a way
that the constraint on the rank distribution is preserved. Let this
channel is denoted by Chadv . Then we deﬁne Chugr to denote
the discrete memory-less channel deﬁned by Y [t] = X[t]H[t]
with a u.g.r. transfer matrix [14].
Now we can state the following facts. First, for Chadv ,
the adversary can always emulate Chugr by choosing the
transfer matrix according to the u.g.r. distribution of Chugr .
Secondly, by using the randomization described in Lemma 2,
we may degrade Chadv to Chugr for whatever strategy the
adversary has been performing. Note that we need to apply a
similar technique in Lemma 2 to show that the degradation
is memoryless. Thus, from the above argument, for every
deﬁnition of the capacity of Chadv , it must be equal to the
capacity of Chugr with the same rank distribution.
Although the above argument provides a simpler proof for
the capacity of Chadv , we believe that our AVC approach gives
a different insight and enables us to study more general models
without the knowledge of the full probabilistic characterization
of the rank distribution.

[1] T. Ho, M. Medard, R. Koetter, D. R. Karger, M. Effros, J. Shi, and
B. Leong, “A random linear network coding approach to multicast,”
IEEE Trans. Inform. Theory, vol. 52, no. 10, pp. 4413–4430, Oct. 2006.
[2] R. Ahlswede, N. Cai, S-Y. R. Li, and R. W. Yeung, “Network information ﬂow,” IEEE Trans. Inform. Theory, vol. 46, no. 4, pp. 1204–1216,
Jul. 2000.
[3] S.-Y. R. Li, N. Cai, and R. W. Yeung, “Linear network coding,” IEEE
Trans. Inform. Theory, vol. 49, no. 2, pp. 371–381, Feb. 2003.
[4] R. Koetter and F. R. Kschischang, “Coding for errors and erasures in
random network coding,” IEEE Trans. Inform. Theory, vol. 54, no. 8,
pp. 3579–3591, Aug. 2008.
[5] A. Montanari and R. Urbanke, “Coding for network coding,” Dec. 2007,
[Online]. Available: http://arxiv.org/abs/0711.3935/.
[6] M. Jafari Siavoshani, C. Fragouli, and S. Diggavi, “Non-coherent multisource network coding,” IEEE International Symposium on Information
Theory, pp. 817–821, Jul. 2008.
[7] M. Jafari Siavoshani, S. Mohajer, C. Fragouli, and S. Diggavi, “On
the capacity of non-coherent network coding”, IEEE International
Symposium on Information Theory, pp. 273–277, Jun. 2009.
[8] M. Jafari Siavoshani, S. Mohajer, C. Fragouli, and S. N. Diggavi,
“On the capacity of noncoherent network coding,” IEEE Trans. Inform.
Theory, vol. 57, no. 2, pp. 1046–1066, Feb. 2011.
[9] D. Silva, F. R. Kschischang, and R. Koetter, “Communication over ﬁniteﬁeld matrix channels,” IEEE Trans. Inform. Theory, vol. 56, no. 3,
pp. 1296–1305, Mar. 2010.
[10] S. Yang, S.-W. Ho, J. Meng, E.-hui Yang, and R. W. Yeung, “On
Linear operator channels over ﬁnite ﬁelds,” 2010. [Online]. Available:
http://arxiv.org/abs/1002.2293v2.
[11] S. Yang, S.-W. Ho, J. Meng, and E.-hui Yang, “Symmetric properties
and subspace degradations of linear operator channels over ﬁnite ﬁelds,”
2011. [Online]. Available: http://arxiv.org/abs/1108.4257.
[12] S. Yang, J. Meng, and E. hui Yang, “Coding for linear operator channels over ﬁnite ﬁelds,” IEEE International Symposium on Information
Theory, Jun. 2010.
[13] S. Yang, S.-W. Ho, J. Meng, and E.-h. Yang, “Optimality of subspace
coding for linear operator channels over ﬁnite ﬁelds,” IEEE Information
Theory Workshop, pp. 400–404, Jan. 2010.
[14] R. W. Nobrega, B. F. Uchoa-Filho, D. Silva, “On the capacity of multiplicative ﬁnite-ﬁeld matrix channels,” IEEE International Symposium
on Information Theory, pp. 341–345, 2011.
[15] R. W. Nobrega, D. Silva, and B. F. Uchoa-Filho, “On the Capacity of
Multiplicative Finite-Field Matrix Channels,” 2011, [Online]. Available:
http://arxiv.org/abs/1105.6115.
[16] D. Blackwell, L. Breiman, and A. J. Thomasian, “The Capacities
of Certain Channel Classes Under Random Coding,” The Annals of
Mathematical Statistics, vol. 31, no. 2, pp. 558–567, Sep. 1960.
[17] A. Lapidoth and P. Narayan, “Reliable communication under channel
uncertainty,” IEEE Trans. Inform. Theory , vol. 44, no. 6, pp. 2148–
2177, Oct. 1998.
[18] I. Csiszar and P. Narayan, “Arbitrarily varying channels with constrained
inputs and states,” IEEE Trans. Inform. Theory, vol. 34, pp. 27–34,
Jan. 1988.
[19] I. Csiszar and P. Narayan, “The capacity of the arbitrarily varying
channel revisited: Positivity, constraints,” IEEE Trans. Inform. Theory,
vol. 34, pp. 181–193, Jan. 1988.
[20] M. Jafari Siavoshani, S. Yang, and R. W. Yeung, “Non-coherent network coding: an arbitrarily varying channel approach,” EPFL Technical
Report. [Online]. Available: http://infoscience.epﬂ.ch/record/174714.

C ONCLUSION
In this work, we proposed an arbitrarily varying channel
(AVC) approach to model the non-coherent network coding
by a matrix channel where the rank distribution of the transfer
matrix is known and apart from that the transfer matrix can be
changed arbitrarily from time-slot to time-slot. We believe that
this AVC approach better ﬁts to model a complex, dynamically
changing network where relay nodes perform randomized
network coding.
In order to characterize the capacity of such a channel, we
deﬁned a new class of channels, called partially AVC (PAVC),
with a partial probabilistic constraint over the state space. By
extending the previous result on AVC to PAVC, we proved
that the subspace coding is optimal to achieve the capacity of
non-coherent network coding.
ACKNOWLEDGMENT
The authors would like to thank Ning Cai and Emre Telatar
for many useful discussions. The work of M. Jafari Siavoshani
was supported by the Swiss National Science Foundation
through Grant PP00P2-128639. The work of S. Yang and
R. W. Yeung was partially supported by a grant from the
2 The idea of this proof is due to one of the anonymous reviewers where
hereby we acknowledge him/her.

5


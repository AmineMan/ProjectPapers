Title:          isit-fin-sub.dvi
Creator:        dvips(k) 5.991 Copyright 2011 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 04:58:26 2012
ModDate:        Tue Jun 19 12:54:22 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      412796 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569563845

On Three-Receiver More Capable Channels
Chandra Nair, Lingxiao Xia
Department of Information Engineering
The Chinese University of Hong Kong
Shatin, NT, Hong Kong S.A.R.
Email:{chandra,lingxiao}@ie.cuhk.edu.hk
not occur for less noisy ordering. Hence our result should
not come as very surprising. However, based on the work in
[7], a natural instinct for beating the superposition coding’s
achievable region would be to show that the maximum
sum-rate achieved by superposition coding region is strictly
smaller than the sum-rate capacity. This fails because, as
we show later, the sum-rate capacity can be achieved by
transmitting only to the best receiver. Furthermore, since
the capacity region would indeed reduce to the time-division
region when all the channels have the same point-to-point
capacity, which is the class studied in [7], we cannot
conclude in a straightforward manner from the results in
[7] that superposition coding is sub-optimal. Therefore, even
though this work is based on one of the authors’ insights in
[7], the counterexample is nevertheless interesting.

Abstract—In this paper we show that superposition coding
is not optimal for three-receiver more capable channels. The
optimality of superposition coding region has been open for
k-receiver (k ≥ 3) more capable broadcast channel. The main
contribution is in identifying a counterexample to demonstrate
that superposition coding is sub-optimal. We also compute
the capacity region for the counter example. On the other
hand, we show that the sum-capacity for a k-receiver more
capable broadcast channel is obtained by transmitting all the
information to the most capable receiver.

I. I NTRODUCTION
Broadcast channel models a basic communication scenario where a single sender X wishes to communicate
possibly different messages to multiple receivers Y1 , ..., Yk
over a noisy medium. For details of previous results on this
problem the readers are encouraged to refer to Chapters 5,
8, and 9 of [1]. This paper addresses an open question in the
book (open problem 5.2), and we show that superposition
coding technique (the main material presented in Chapter 5)
is not optimal for the three-receiver more capable channel.
Determining the capacity region of a two-receiver discrete
memoryless broadcast channel continues to remain open as
one of the most fundamental unsolved problems in this
ﬁeld; yet, there has been remarkable success when there
is an ordering between the decoding capabilities of the
receivers. The ﬁrst such ordering is the degraded broadcast
channel[3] where X → Y1 → · · · Yk forms a Markov chain,
and the optimality of superposition coding was established
by Bergmans[2] and Gallager[6]. Two progressively less
stringent orderings[8] called less noisy and more capable
were introduced by Korner and Marton. The optimality of
the superposition coding scheme for these classes for the
two-receiver case were established in [8], [5], respectively.
The optimality of superposition coding for three-receiver
less noisy broadcast channels was established in [10]. Our
results here show that superposition coding is not optimal
for three-receiver more capable channels. This represents
the ﬁrst instance where an ordering on the decodability of
the receivers is established, yet superposition coding is not
optimal.
It has been shown in [7] that the more-capable ordering
is a much weaker ordering than less noisy ordering. In
particular it was shown that if one substitutes a receiver
(in a two-receiver broadcast channel) by a more-capable
receiver then the capacity region could strictly decrease (!).
Further it was also shown that such a phenomenon would

Deﬁnition 1. A receiver Y1 is said to be more capable than
receiver Y2 if the following holds: for every -error channel
codebook1 of size 2nR from sender X to Y2 , there exists an
-error channel codebook of size 2n(R−δ) from sender X
to Y1 where δ, → 0 as → 0.
Remark: This is essentially equivalent to saying that Y1
could decode any codebook that Y2 could decode.
Korner and Marton[8] showed that the above deﬁnition is
equivalent to the following:
Deﬁnition 2. A receiver Y1 is said to be more capable than receiver Y2 if the following holds: I(X; Y1 ) ≥
I(X; Y2 ), ∀p(x).
El Gamal[5] showed that superposition coding is optimal,
i.e. achieves the capacity region, for any two-receiver more
capable broadcast channel. Note that the deﬁnition of more
capable induces a partial ordering among the receivers (or
equivalently probability transition matrices), hence we are
assuming here that the three receivers satisfy an induced
more capable ordering.
Deﬁnition 3. A three-receiver more capable channel consists of a single sender X and three receivers Y1 , Y2 and
Y3 such that the mutual information I(X; Y1 ), I(X; Y2 ) and
I(X; Y3 ) satisﬁes I(X; Y1 ) ≥ I(X; Y2 ) ≥ I(X; Y3 ) for any
ﬁxed distribution p(x) on X.
First we state(without the standard proof) the superposi1 An -error codebook of size 2nR consists of a set of codewords xn (m),
n
m ∈ [1 : 2nR ] and disjoint decoding regions B(m) ∈ Y2 such that
n /
P(y2 ∈ B(m)|xn (m) is transmitted) < , ∀m.

1

i=1
n

As

which, as we mentioned before, is achieved by transmitting
only to the best receiver with superposition coding. In the
above chain of inequalities we have used Fano’s inequality,
chain-rule for mutual information, data-processing inequality, Csiszar-sum lemma (equalities (a),(c)) (reproduced below) and the more capable ordering (inequalities (b),(d)).
The data-processing inequalities used above come from the
following Markov chain
i−1
n
i−1
(Y11 , Y2i+1 , Y31 , M1 , M2 , M3 ) → Xi → (Y1i , Y2i , Y3i ).

Furthermore, noting the similarity of the second inequality
and (2), a k-receiver proof could be generated by eliminating
one receiver at a time.
n
n
Lemma 1. (Csiszar-sum Lemma) For any p(U, y11 , y21 ) we
have

(1)

i−1
n
I(Y11 ; Y2i |U, Y2i+1 ) =
i=1

II. T HE S UB - OPTIMALITY

n
i−1
I(Xi ; Y3i |M2 , M1 , Y31 )
i=1
n
i−1
I(Xi ; Y3i |M2 , M1 , Y2i+1 , Y31 )
i=1
n
n
i−1
I(Y2i+1 ; Y3i |M2 , M1 , Y31 )

+

n
I(M2 ; Y2i |M1 , Y2i+1 )

+
i=1

i=1
n

(a)

n
= I(M1 ; Y11 ) +

n
i−1
I(Xi ; Y3i |M2 , M1 , Y2i+1 , Y31 )
i=1
n

n
i−1
n
I(Y31 ; Y2i |M2 , M1 , Y2i+1 ) +

+
i=1

n
I(M2 ; Y2i |M1 , Y2i+1 )
i=1

n

=

I(M1 ; Y1n )

n
i−1
I(Xi ; Y3i |M2 , M1 , Y2i+1 , Y31 )

+
i=1

n
i−1
n
I(M2 , Y31 ; Y2i |M1 , Y2i+1 )

+
i=1

n

(b)

≤ I(M1 ; Y1n ) +

n
i−1
I(Xi ; Y2i |M2 , M1 , Y2i+1 , Y31 )
i=1

n

T =

i−1
n
I(M2 , Y31 ; Y2i |M1 , Y2i+1 )

+
i=1

n
n
= I(M1 ; Y11 ) +

n
I(Xi ; Y2i |M1 , Y2i+1 )

max

(R1 ,R2 ,R3 )∈C

R1
1−

n
n
i−1
n
i−1
I(Xi ; Y2i |M1 , Y2i+1 , Y11 ) + I(M1 , Y2i+1 ; Y1i |Y11 )

=

S UPERPOSITION C ODING

R1
1−

+
1

R2 + R3
.
1− 2

Lemma 2. For all (R1 , R2 , R3 ) ∈ S we have

(2)

i=1
(c)

OF

Theorem 2 implies that it is not possible to beat the sumrate. What we could try is to beat the achievable region
along some other directions, which is what we will do in
the counter example.
We consider a DM-BC with X ∈ {0, 1}, Y1 ∈ {0, 1, e},
Y2 ∈ {0, 1, e}, and Y3 ∈ {0, 1}, where the channel from
X to Y1 , Y2 and Y3 are BEC( 1 ), BEC( 2 ) and BSC(p),
1
respectively (see Figure 1). Let p ∈ [0, 2 ], 1 = 4p(1 − p)
and 2 = H(p), then from [9] we know that this is a threereceiver more capable channel.
In fact, for this particular case, Y1 is also less noisy than
Y3 and Y2 is a degraded version of Y1 . However Y2 is
only more capable than Y3 . Let C denote the true (as yet
unknown) capacity region and S denote the superposition
coding region.
Suppose the private message rates are R1 , R2 and R3 for
receivers Y1 , Y2 and Y3 , respectively. We try to maximize
the following equation

n

n

n
i−1
I(Y2i+1 ; Y1i |U, Y11 ).
i=1

Originally presented in [4], this is one of the most commonly used identities to derive outer bounds and converses
for discrete memoryless broadcast channels.

n(R1 + R2 + R3 ) − n n
n
n
n
≤ I(M1 ; Y11 ) + I(M2 ; Y21 |M1 ) + I(M3 ; Y31 |M2 , M1 )

+

n

n

Proof: We will prove the theorem for three-receiver
more capable channels, the proof for more receivers shall
follow with similar steps. Note that

=

→ 0 when n → ∞, we have
p(x)

Theorem 2. Any set of achievable rates R1 , . . . , Rk for a
k-receiver more capable channel must satisfy

n
I(M1 ; Y11 )

n

R1 + R2 + R3 ≤ max I(X; Y1 ),

Superposition coding yields the optimal sum-rate.

n
n
≤ I(M1 ; Y11 ) + I(M2 ; Y21 |M1 ) +

p(x)

i=1

over all pairs of random variables (U2 , U3 ) such that
(U2 , U3 ) → X → (Y1 , Y2 , Y3 ) forms a Markov chain is
achievable for the private-messages-only case. We call this
region the superposition coding region.

p(x)

i−1
I(Xi ; Y1i |Y11 ) ≤ n max I(X; Y1 )

=

≤ I(U3 ; Y3 )
≤ I(U2 ; Y2 |U3 ) + I(U3 ; Y3 )
≤ I(U2 , U3 ; Y2 )
≤ I(X; Y1 |U2 , U3 ) + I(U2 ; Y2 |U3 ) + I(U3 ; Y3 )
≤ I(X; Y1 |U2 , U3 ) + I(U2 , U3 ; Y2 )
≤ I(X; Y1 |U3 ) + I(U3 ; Y3 )
≤ I(X; Y1 )

R1 + R2 + · · · + Rk ≤ max I(X; Y1 ).

n
i−1
n
i−1
I(Xi ; Y1i |M1 , Y2i+1 , Y11 ) + I(M1 , Y2i+1 ; Y1i |Y11 )

≤

Theorem 1. The following set, S, obtained by taking the
union of all non-negative rate triples (R1 , R2 , R3 ) satisfying
R3
R2 + R3
R2 + R3
R1 + R2 + R3
R1 + R2 + R3
R1 + R2 + R3
R1 + R2 + R3

n

(d)

tion coding region for the three receiver broadcast channel.
We select the natural order induced by the more capable
ordering for superposition coding.

+
1

R2 + R3
≤ 1.
1− 2

Proof: Note that if (R1 , R2 , R3 ) ∈ S is in the achievable region, then so is (R1 , R2 , 0), where R2 = R2 + R3 .

i=1

(3)

2
2

0
e

By setting p and s to
Y1

1 − H(s ∗ p)
1 − H(p)
1 − H(0.18)
= H(0.1) +
1 − H(0.1)
≥ 1.07.

0

0
X

e

2

Therefore, superposition coding region is not optimal for
the three-receiver more capable channel.

Y2

A. An achievable rate region for the three-receiver more
capable broadcast channel
Since the sum-capacity is bounded by what one can
transmit to the receiver Y1 , a natural guess would be to allow
the Y1 to decode all the messages. Now, one can employ
Marton’s binning scheme to transmit non-nested messages
to receivers Y2 and Y3 .

1

1
p

0
Y3
1

Fig. 1.

3-receiver more capable channel:

1

= 4p(1 − p),

2

= H(p)

Theorem 3. Consider a three receiver more capable broadcast channel with Y1 being the most capable receiver and
Y3 the least. Then any rate triple (R1 , R2 , R3 ) satisfying

One can see this by plugging the choice into the region
in Theorem 1. However the channel X → Y1 → Y2 is a
degraded broadcast channel consisting of two BEC’s and its
capacity region consists of all non-negative pairs (R1 , R2 )
satisfying
R2
R1
+
≤ 1.
1− 1
1− 2
This well-known fact holds since for any U → X →
(Y1 , Y2 ) we have I(U ; Y2 ) = (1 − 2 )I(U ; X) and
I(X; Y1 |U ) = (1 − 1 )H(X|U ) which can be obtained by
routine manipulations. Hence

R2 ≤ I(U2 , W ; Y2 )
R3 ≤ I(U3 , W ; Y3 )
R2 + R3 ≤ min{I(W ; Y2 ), I(W ; Y3 )} + I(U2 ; Y2 |W )
+ I(U3 ; Y3 |W ) − I(U2 ; U3 |W )
R1 + R2 + R3 ≤ I(X; Y1 )
R1 + R2 + R3 ≤ I(U2 , W ; Y2 ) + I(X; Y1 |U2 , W )
R1 + R2 + R3 ≤ I(U3 , W ; Y3 ) + I(X; Y1 |U3 , W )
R1 + R2 + R3 ≤ min{I(W ; Y2 ), I(W ; Y3 )} + I(U2 ; Y2 |W )
+ I(U3 ; Y3 |W ) + I(X; Y1 |U2 , U3 , W ) − I(U2 ; U3 |W )
R1 + 2R2 + 2R3 ≤ I(U2 , W ; Y2 ) + I(U3 , W ; Y3 )
+ I(X; Y1 |W ) − I(U2 ; U3 |W )
2R1 + 2R2 + 2R3 ≤ I(U2 , W ; Y2 ) + I(U3 , W ; Y3 )
+ I(X; Y1 |U2 , U3 , W ) + I(X; Y1 |W ) − I(U2 ; U3 |W )

I(U ; Y2 )
I(X; Y1 |U )
+
= H(X|U ) + I(U ; X) = H(X) ≤ 1.
1− 1
1− 2

Lemma 2 implies that T ≤ 1 if superposition coding were
optimal.
Beating superposition coding region: Next, we will show
that one can actually achieve T > 1. Instead of treating
Y2 as the second best receiver, we ignore Y2 completely;
i.e. it does not need to decode any message. This way
the channel is transformed into a two-receiver less noisy
broadcast channel with receivers Y1 , Y3 . Using superposition
coding on this two-receiver channel, we can achieve R1 =
I(X; Y1 |U ), R3 = I(U ; Y3 ) for any U → X → (Y1 , Y3 ).
Hence
T ≥
=

max

U →X1 →(Y1 ,Y2 ,Y3 )

max

U →X1 →(Y1 ,Y2 ,Y3 )

is achievable for any (W, U2 , U3 ) → X → (Y1 , Y2 , Y3 ).
Proof: The proof follows standard techniques of random binning, superposition coding, and jointly typical den
coding. Receiver Y2 decodes U2 , W n , receiver Y3 decodes
n
n
n
n
U3 , W , and receiver Y1 decodes W n , U2 , U3 , X n . The
analysis is routine and straightforward (but messy) and
hence is omitted.
Remark 1. A constraint

I(X; Y1 |U )
I(U ; Y3 )
+
(1 − 1 )
1− 2
I(X; Y1 |U )
I(U ; Y3 )
+
.
(1 − 1 )
1 − H(p)

0 ≤ I(U2 ; Y2 |W ) + I(U3 ; Y3 |W ) − I(U2 ; U3 |W )

is obtained during the process of Fourier-Motzkin Elimination, but it is easy to see that this condition is redundant to
the computation of the region.
In the event where Y1 is less noisy than both Y2 and Y3
pairwise(as in the counter example), the achievable region
in Theorem 3 reduces to

Let U → X be a BSC with crossover probability s, 0 <
1
s < 1 . Further, let P (U = 0) = 2 . We have,
2
I(X; Y1 |U )
I(U ; Y3 )
+
1− 1
1 − H(p)
(1 − 1 )H(s)
1 − H(s ∗ p)
=
+
1− 1
1 − H(p)
1 − H(s ∗ p)
.
= H(s) +
1 − H(p)

we see that

T ≥ H(s) +

1

1

1
10 ,

R1 + R2 + R3 ≤ min{I(W ; Y2 ), I(W ; Y3 )} + I(U2 ; Y2 |W )
+ I(U3 ; Y3 |W ) + I(X; Y1 |U2 , U3 , W ) − I(U2 ; U3 |W )
R2 + R3 ≤ min{I(W ; Y2 ), I(W ; Y3 )} + I(U2 ; Y2 |W )
+ I(U3 ; Y3 |W ) − I(U2 ; U3 |W )
(5)
R2 ≤ I(U2 , W ; Y2 )
R3 ≤ I(U3 , W ; Y3 )

T ≥

(4)

3
3

(c)

n
n
i−1
I(Y3i+1 , Y21 , M2 ; Y2i )

=

Also, since Y3 is essentially less noisy[9] than Y2 in
the counter example, by symmetrization argument2 we can
1
further assume P(X = 0) = 2 , and hence I(U2 , W ; Y3 ) ≥
˜
˜
I(U2 , W ; Y2 ). Therefore we can set W = (U2 , W ), U2 =
˜
∅, U3 = U3 to obtain the following achievable region:

i=1
i−1
n
i−1
+ I(M2 , Y11 ; Y3i |Y3i+1 , M2 , Y21 )
n
i−1
i−1
+ I(Xi ; Y1i |M2 , M3 , Y3i+1 , Y11 , Y21 )
n

i=1

Theorem 4. For the channel depicted in 1, the union of rate
triples (R1 , R2 , R3 ) satisfying

In the above the usual toolset: Fano’s inequality, Csiszar
sum-lemma (steps (a), (b)), data-processing inequality, and
chain rule of mutual information. All the data processing
inequalities come from the following Markov chain:

˜
˜
˜
˜ ˜
R1 + R2 + R3 ≤ I(W ; Y2 ) + I(U3 ; Y3 |W ) + I(X; Y1 |U3 , W )
˜
˜
˜
R2 + R3 ≤ I(W ; Y2 ) + I(U3 ; Y3 |W )
˜ 3 , W ; Y2 )
R2 ≤ I(U

n
i−1
i−1
(M1 , M2 , M3 , Y3i+1 , Y11 , Y21 ) → Xi → (Y1i , Y2i , Y3i ).

˜ ˜
over all (W , U3 ) → X → (Y1 , Y2 , Y3 ) is achievable.

The equality (c) comes from the fact that Y2 is a degraded
version3 of Y1 and hence

This is just superposition coding by treating Y3 as the
second best receiver. We will prove in the next section that
this is indeed the capacity region for the counterexample.

i−1
i−1
n
Y21 → Y11 → (M1 , M2 , M3 , Y3i+1 , Xi , Y1i , Y2i , Y3i )

is Markov. Finally in the usual manner, let Q be an
independent random variable distributed uniformly in [1 : n]
˜
˜
˜
˜
and set W = (WQ , Q), U3 = U3Q , X = XQ .
The other inequalities follow a similar line (but is simpler)
of reasoning. Observe that

B. The capacity region of the counterexample
In this section we show that the region presented in Theorem 4 is indeed the capacity region for the counterexample.
Note that it sufﬁces to just show a converse to Theorem 4
to establish the capacity region. The arguments are reasonably routine once the identiﬁcations of the auxiliaries have
been made:
i−1
i−1
n
˜
˜
Identify U3i = (M3 , Y11 ) and Wi = (M2 , Y3i+1 , Y21 ).
Observe that

n(R2 + R3 ) − n n
n
n
≤ I(M2 ; Y21 ) + I(M3 ; Y31 |M2 )
n
i−1
n
I(M2 ; Y2i |Y21 ) + I(M3 ; Y3i |M2 , Y3i+1 )

=
i=1
n

i−1
i−1
n
I(M2 ; Y2i |Y21 ) + I(Y21 ; Y3i |M2 , Y3i+1 )

≤

n(R1 + R2 + R3 ) − n n
n
n
n
≤ I(M2 ; Y21 ) + I(M3 ; Y31 |M2 ) + I(M1 ; Y11 |M2 , M3 )

i=1
n
i−1
+ I(M3 ; Y3i |M2 , Y3i+1 , Y21 )

n

n
i−1
n
n
I(M1 ; Y1i |M2 , M3 , Y11 ) + I(M3 ; Y31 |M2 ) + I(M2 ; Y21 )

=

˜
˜
˜
˜
˜
I(Xi ; Y1i |U3i , Wi ) + I(U3i ; Y3i |Wi ) + I(Wi ; Y2i ).

=

i=1

i=1
n

n
n
≤ I(M2 ; Y21 ) +

n
i−1
I(M1 ; Y1i |M2 , M3 , Y3i+1 , Y11 )

i=1
n

n
i−1
n
+ I(Y3i+1 ; Y1i |M2 , M3 , Y11 ) + I(M3 ; Y3i |Y3i+1 , M2 )

i=1

n
i−1
I(M1 ; Y1i |M2 , M3 , Y3i+1 , Y11 )

The last inequality (on R2 ) is very straightforward with this
identiﬁcation and is omitted. This completes the proof for
the capacity region of the channel in Figure 1.

i=1

+

˜
˜
˜
I(Wi ; Y2i ) + I(U3i ; Y3i |Wi ).

=

n

n
= I(M2 ; Y21 ) +

n
i−1
i−1
n
i−1
I(M2 , Y3i+1 , Y21 ; Y2i ) + I(M3 , Y11 ; Y3i |M2 , Y3i+1 , Y21 )

≤

i=1

(a)

n
i−1
n
i−1
I(M2 , Y3i+1 ; Y2i |Y21 ) + I(M3 ; Y3i |M2 , Y3i+1 , Y21 )

=

i−1
n
I(M3 , Y11 ; Y3i |Y3i+1 , M2 )

n
i−1
i−1
n
i−1
I(M2 ; Y2i |Y21 ) + I(M3 , Y11 ; Y3i |Y3i+1 , M2 , Y21 )

Remark 2. It may appear a bit strange to see that even
though superposition coding in the natural more-capable
i−1
n
n
i−1
+ I(Y21 ; Y3i |Y3i+1 , M2 ) + I(M1 ; Y1i |M2 , M3 , Y3i+1 , Y11 ) ordering (i.e. Y1 better than Y2 better than Y3 ) is suboptimal,
n
a re-ordering of the receivers, i.e. (i.e. Y1 better than Y3
(b)
i−1
i−1
n
n
=
I(Y3i+1 , M2 ; Y2i |Y21 ) + I(M1 ; Y1i |M2 , M3 , Y3i+1 , Y11 )
better than Y2 ) could make superposition coding optimal
i=1
again. But of course, this is a carefully chosen counteri−1
n
i−1
+ I(M3 , Y11 ; Y3i |Y3i+1 , M2 , Y21 )
example and hence the peculiar situation. It is natural to ask
n
n
i−1
n
i−1 whether there exists a three-receiver more capable broadcast
≤
I(Y3i+1 , Y21 , M2 ; Y2i ) + I(Xi ; Y1i |M2 , M3 , Y3i+1 , Y11 )
channel where superposition coding is not optimal with
i=1
i−1
n
i−1
either ordering. We will show such an example (a minor
+ I(M2 , Y11 ; Y3i |Y3i+1 , M2 , Y21 )
perturbation of the example in Figure 1) in the next section.

≤

i=1

2 Symmetrization argument can be found in [9], [11], [7] or in Chapter
5 of [1]. The main purpose of this argument is to show that points on the
boundary for a binary input symmetric output channels can be computed
1
using distributions that satisfy P(X = 0) = 2 .

3 Since capacity region depends only on the marginals p(y |x), p(y |x)
1
2
and p(y3 |x), we can assume without loss of generality that Y2 is a
physically degraded version of Y1 in the example in Figure 1.

4
4

C. A modiﬁed counterexample

On the other hand, we showed that one can achieve the
sum-rate capacity for any k-receiver more capable channel
by just transmitting to the best receiver. Motivated by this
result we presented certain achievable regions for the threereceiver more capable broadcast channels.

Consider the same channel as in Figure 1. Set 1 = 4 ∗
(0.1) ∗ 0.9 = 0.36, 2 = H(0.1). Slightly change the value
of p from 0.1 to 0.11. Clearly since the new receiver Y3
is a degraded version of the old receiver Y3 (which was
BSC(0.1)), this setting is still a three-receiver more capable
channel. As before, we try to maximize
T =

max

(R1 ,R2 ,R3 )∈C

ACKNOWLEDGEMENTS
The work of Chandra Nair was partially supported by
the following grants from the University Grants Committee
of the Hong Kong Special Administrative Region, China: a)
(Project No. AoE/E-02/08), b) GRF Project 415810. He also
acknowledges the support from the Institute of Theoretical
Computer Science and Communications (ITCSC) at the
Chinese University of Hong Kong.

R1
R2 + R3
+
.
1− 1
1− 2

If superposition coding in the more capable ordering were
optimal, then again the same arguments would imply that
T ≤ 1. However, if we ignore Y2 again and use superposition coding between receivers Y1 and Y3 , we can obtain,
taking U → Xto be BSC(0.1) with uniform distribution,

R EFERENCES

I(X; Y1 |U )
I(U ; Y3 )
+
1− 1
1− 2
1 − H(0.11 ∗ 0.1)
= H(0.1) +
1 − H(0.1)
≥ 1.039.

T ≥

[1] Young-Han Kim Abbas El Gamal, Network information theory,
Cambridge University Press, 2012.
[2] P F Bergmans, Coding theorem for broadcast channels with degraded
components, IEEE Trans. Info. Theory IT-15 (March, 1973), 197–
207.
[3] T Cover, Broadcast channels, IEEE Trans. Info. Theory IT-18 (January, 1972), 2–14.
[4] I Csiz´ r and J K¨ rner, Broadcast channels with conﬁdential messages,
a
o
IEEE Trans. Info. Theory IT-24 (May, 1978), 339–348.
[5] A El Gamal, The capacity of a class of broadcast channels, IEEE
Trans. Info. Theory IT-25 (March, 1979), 166–169.
[6] R G Gallager, Capacity and coding for degraded broadcast channels,
Probl. Peredac. Inform. 10(3) (1974), 3–14.
[7] Y Geng, C Nair, S Shamai, and Z V Wang, On broadcast channels
with binary inputs and symmetric outputs, International Symposium
on Information Theory (2010).
[8] J K¨ rner and K Marton, Comparison of two noisy channels, Topics
o
in Inform. Theory(ed. by I. Csiszar and P.Elias), Keszthely, Hungary
(August, 1975), 411–423.
[9] C Nair, Capacity regions of two new classes of two-receiver broadcast
channels, Information Theory, IEEE Transactions on 56 (2010), no. 9,
4207–4214.
[10] C. Nair and Z.V. Wang, The capacity region of the three receiver less
noisy broadcast channel, Information Theory, IEEE Transactions on
57 (2011), no. 7, 4058 –4062.
[11] Chandra Nair, Abbas El Gamal, and Yeow-Khiang Chia, An achievability scheme for the compound channel with state noncausally
available at the encoder, CoRR abs/1004.3427 (2010).

Hence, superposition coding in the more capable ordering
is not optimal.
To show that superposition coding in the Y1 , Y3 , Y2 ordering is not optimal, we can maximize
T =

max

(R1 ,R2 ,R3 )∈C

R2 + R3 .

If superposition coding in Y1 , Y3 , Y2 ordering were optimal,
this would be the same as maximizing R3 , whose maximum
is 1 − H(0.11) ≈ 0.501. On the other hand, by just
transmitting to receiver Y2 we can obtain R2 = 1 − 2 =
1 − H(0.1) ≥ 0.531. Thus superposition coding in the
Y1 , Y3 , Y2 order is also not optimal for this modiﬁed counter
example.
Remark 3. The converse in the last section continues to hold
for this modiﬁed setting. However, since Y3 is no longer an
essentially less noisy receiver than Y2 , the achievability of
the region depicted by Theorem 4 fails to hold.
A natural guess for the capacity region in this modiﬁed
counterexample would be given by the constraints in Equations (5).
III. C ONCLUSION
In this paper, we showed that superposition coding does
not achieve the capacity region for a three-receiver broadcast
channel. In fact, we presented a counterexample where
capacity could be achieved by treating the least capable
receiver as the intermediate receiver and the intermediatelycapable receiver the worst receiver. The main purpose of
this counterexample is to show that more capable is a very
weak ordering that does not preserve the nested decoding
properties a less noisy ordering would, at least in the three
receiver case. Then we produced a modiﬁed counterexample
to show that superposition coding (in whatever order one
wishes) cannot yield the capacity region for general threereceiver more capable broadcast channels.
5
5


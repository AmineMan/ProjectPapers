Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Tue May 15 12:47:29 2012
ModDate:        Tue Jun 19 12:54:11 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      624662 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564919

On the Uncertainty of Information Retrieval in
Associative Memories
Eitan Yaakobi∗† and Jehoshua Bruck∗
∗ Electrical Engineering Department, California Institute of Technology, Pasadena, CA 91125, U.S.A
† Electrical and Computer Engineering Department, University of California San Diego, La Jolla, CA 92093, U.S.A.

{yaakobi, bruck}@caltech.edu
Abstract—We (people) are memory machines. Our decision
processes, emotions and interactions with the world around us
are based on and driven by associations to our memories. This
natural association paradigm will become critical in future memory systems, namely, the key question will not be “How do I store
more information?” but rather, “Do I have the relevant information? How do I retrieve it?”
The focus of this paper is to make a ﬁrst step in this direction. We deﬁne and solve a very basic problem in associative
retrieval. Given a word W, the words in the memory that are
t-associated with W are the words in the ball of radius t around
W. In general, given a set of words, say W, X and Y, the words
that are t-associated with {W, X, Y } are those in the memory
that are within distance t from all the three words. Our main
goal is to study the maximum size of the t-associated set as a
function of the number of input words and the minimum distance of the words in memory - we call this value the uncertainty
of an associative memory. We derive the uncertainty of the associative memory that consists of all the binary vectors with an
arbitrary number of input words. In addition, we study the retrieval problem, namely, how do we get the t-associated set given
the inputs? We note that this paradigm is a generalization of the
sequences reconstruction problem that was proposed by Levenshtein (2001). In this model, a word is transmitted over multiple
channels. A decoder receives all the channel outputs and decodes
the transmitted word. Levenshtein computed the minimum number of channels that guarantee a successful decoder - this value
happens to be the uncertainty of an associative memory with two
input words.

(a)

(b)

(c)
Fig. 1. Three example of associated words: (a) associated words with a single
word x, (b) associated words with two words x and y, (c) associated words
with the six words x1 , . . . , x6 .

ball of radius t, see Fig. 1(a). We generalize this paradigm and
consider a set of words that are presented as an input to the
memory. For example, for two input words x, y, their set of
associated words are the ones that their distance from both x
and y is at most t, see Fig. 1(b). Clearly, this set of associations is strictly smaller then the ball of radius t. For more than
two input words, the set of associated words is getting even
smaller, and in general, the larger the set of input words is, the
smaller the set of associated words is, see Fig. 1(c). For example, assume the input word is “tall”, then many words can be
associated with it, such as “tree”, “mountain”, “tower”, “ladder”, etc. But if the input words are “tall” and “fruit”, then
out of these four associated words, only the word “tree” will
be associated with “tall” and “fruit”.
Assume that the memory is the set of all binary vectors,
M = {0, 1}n . For any input word x, it is immediate to see
that if its set of associated words are the ones of distance at
t
most t, then there are ∑i=0 (n) such words. However, for two
i
input words, x, y, the problem of ﬁnding the set of associated words of distance at most t from both x and y becomes
more complex. In fact, this problem was proposed and solved
by Levenshtein in [10], [11]. The motivation came from a
completely different scenario in the context of the sequences
reconstruction problem. In this model, a codeword x is transmitted through multiple channels. Then, a decoder receives all
channel outputs and generates an estimation on the transmitted word, while it is guaranteed that all channel outputs are
different from each other, see Fig. 2. If x belongs to a code C
with minimum distance d and in every channel there can be
at most t > d−1 errors, then Levenshtein studied the mini2
mum number of channels N that guarantees the existence of

I. I NTRODUCTION
One of the interpretations of the term association, especially in the context of psychology, is the connection between
two or more concepts. Throughout our life, we remember and
store an enormous amount of information. However, while we
are not aware of the method this information is stored, amazingly, it can be accessed and retrieved with relative ease. The
way we think and process information is mainly performed by
associations. Our memory content retrieval process is done by
stimulating it with an external or internal inputs. That is, the
knowledge of some information gives rise to related information that was stored earlier. Mathematically speaking, assume
the memory is a set of words M = {m1 , . . . , m N }. Then,
given an arbitrary word x as an input to the memory M, its
output is another word or a set of words from the memory
M that are related or close to the input word x. Here, the
term “close” can be interpreted as using any distance metric
between words, for example the Hamming distance. A word
is associated with another word or words which they again
can be associated with more words and so on, resulting in a
sequence of associations.
From the information theory perspective, we say that the
words associated with an input word x are those in distance
at most t (a prescribed value) from x. This set comprises a

1

Section III, we solve the problem stated in (2) for the case
d = 1. Extensions for arbitrary d are given in Section IV.
In Section V, we give efﬁcient decoders to the reconstruction
problem studied by Levenshtein. Finally, Section VI concludes
the paper.
II. D EFINITIONS AND BASIC P ROPERTIES
In this work, the words are binary vectors of length n. The
Hamming distance between two words x and y is denoted by
d H ( x, y) and the Hamming weight of a word x is denoted
n
by w H ( x). For a word x ∈ {0, 1}n , Bt ( x) is its surrounding
n ( x ) = { y ∈ { 0, 1 }n : d ( x, y )
ball of radius t, Bt
t }.
H
n
The size of Bt ( x), comprising of length-n words, is bt,n =
n
t
∑i=0 ( i ). If the length of the words is clear from the context,
we use the notation Bt ( x). For 1 i n, ei is the unit vector
where only its i-th bit is one, and 0 is the all-zero vector. Two
words x, y ∈ {0, 1}n are called t-associated if d H ( x, y) t.
Deﬁnition. The t-associated set of the words x1 , . . . , xm is denoted by St ({ x1 , . . . , xm }) and is deﬁned to be the set of all
words y that are t-associated with x1 , . . . , xm ,
m

Fig. 2. Channel model of the sequences reconstruction problem.

a successful decoder. This number has to be greater than
N=

max

x1 ,x2 ∈C ,x1 = x2

| Bt ( x1 ) ∩ Bt ( x2 )|,

(1)

where Bt ( x) is the ball of radius t surrounding x. To see that,
notice that if the intersection of the radius-t balls of x1 and x2
contains N words and the channel outputs are these N words,
then a decoder cannot determine what the transmitted word
is. However, if the number of channel outputs is greater than
the maximum size of the intersection of two balls, then there
is only one codeword of distance at most t from all received
channel outputs.
The motivation to the model studied by Levenshtein came
from ﬁelds such as chemistry and biology, where the redundancy in the codewords is not sufﬁcient to construct a successful decoder. Thus, the only way to combat errors is by repeatedly transmitting the same codeword. Recently, this model was
shown to be also relevant in storage technologies [3], [16]. Due
to the high capacity and density of today’s and future’s storage
medium, it is no longer possible to read individual memory
elements, but, rather, only a multiple of them at once. Hence,
every memory element is read multiple times, which is translated into multiple estimations of the same information stored
in the memory.
Finding the maximum intersection problem in (1) was studied in [11] with respect to the Hamming distance and other
metric distances. In [7]–[9], it was analyzed over permutations, and in [13], [14] for error graphs. In [12], the equivalent
problem for insertions and deletions was studied and reconstruction algorithms for this model were given in [2], [5], [15].
The case of deletions only was studied in the context of trace
reconstruction in [4].
Returning to our original problem, the set of associated
words with x and y is Bt ( x) ∩ Bt ( y) and the maximum intersection is the value N in (1). The generalized problem
of ﬁnding the maximum size of associated words of m
2
input words with mutual distance d is expressed as
Nt (m, d) =

max

x1 ,...,xm ,d H ( xi ,x j ) d

{|∩im 1 Bt ( xi )|} .
=

St { x1 , . . . , xm } ={ y : d H ( y, xi )

t, 1 i

m}=

Bt ( xi ).
i =1

Note that for a single word x, we have St { x} = Bt ( x).
Given an associative memory M, we deﬁne the maximum
size of a t-associated set of any m words from the memory.
Deﬁnition. Let M be an associative memory and m, t be two
positive integers. The uncertainty of the associative memory
M for m and t, denoted by Nt (m, M), is the maximum size of
a t-associated set of m different input words from M. That is,
Nt (m, M) =
max
St { x1 , . . . , xm }
. (3)
x1 ,...,xm ∈M,xi = x j

In case the associative memory M is a code with minimum distance d, we will use the notation Nt (m, d) instead
of Nt (m, M). Then, the value in Equation (3) becomes
Nt (m, d) =
max
St { x1 , . . . , xm }
. (4)
x1 ,...,xm ,d H ( xi ,x j ) d

For example, Nt (m, 1) refers to Nt (m, {0, 1}n ).
We now give the deﬁnitions that establish the connection with the channel model by Levenshtein [11]. Assume
a codeword x is transmitted over N channels. The channel
outputs, denoted by y1 , . . . , y N , are all different from each
other (Fig. 2). A list decoder DL receives the N channel outputs and returns a list of at most L words x1 , . . . , x , where
L. We call it an L-decoder DL . The L-decoder DL is
said to be successful if and only if the transmitted word x
belongs to the decoded output list, i.e.,
x ∈ DL ( y1 , . . . , y N ) = { x1 , . . . , x }.
In case L = 1 then the decoder output is a single word and
this is the model studied by Levenshtein [11].
The next Lemma shows the connection between the value
of Nt (m, d) and the decoding success of an L-decoder.
Lemma 1. Assume the transmitted word x belongs to a code
C of minimum distance d. Then, there exists a successful Ldecoder with N channels if and only if
N Nt (L + 1, d) + 1.
Proof: Assume to the contrary that the number of channels is Nt (L + 1, d) and let x1 , . . . , xL+1 be L + 1 words

(2)

The main goal in this paper it to analyze the value of
Nt (m, d) with respect to the Hamming distance for different values of t, m, d. In particular, we show that if A( D )
is the size of a maximal anitcode of diameter D, that is,
the largest set of words with maximum distance D, then
Nt ( A( D ), 1) = A(2t − D ).
The rest of the paper is organized as follows. In Section II,
we deﬁne the concept of associative memories and describe
the connection to the sequences reconstruction problem. In

2

such that the set St ({ x1 , . . . , xL+1 }) contains Nt (L + 1, d)
words. If one of these L + 1 words is the transmitted one
and the received channel outputs are the Nt (L + 1, d)
words in St ({ x1 , . . . , xL+1 }), then any of the L + 1 words
x1 , . . . , xL+1 could be the transmitted one and thus can belong to the decoder’s output list. Hence, the transmitted word
may not belong to the output list.
On the other hand, if there are Nt (L + 1, d) + 1 channels,
then for any transmitted word x, there are at most L words
in C , all of distance at least d from each other, such that the
Nt (L + 1, d) + 1 channel outputs are located in the intersection of their radius-t balls.
For L = 1, the value Nt (2, d) was studied by Levenshtein [11] and was shown to be
t−

Nt (2, d) =

d
2

∑

i =0

n−d
i

t −i

d
.
k

∑

k = d − t +i

where i1 , i2 , i3 , i4 satisfy the following constraints:
d
1) 0 i1 t − 2 ,
d
d
2) i1 + 2 − t i4 t − 2 − i1 ,
3) d − t + i1 i3 t − (i1 + i4 ),
4) max{i1 − i3 − i4 + 3d − t, i1 + i3 + i4 +
2
i 2 t − ( i 1 + i 4 + d − i 3 ).
2

In this section, we analyze the value of Nt (m, d) for d = 1.
A ﬁrst observation on the value of Nt (m, 1) is stated in the
next lemma.
Lemma 4. For m, t 1, if Nt (m, 1)
and Nt (m + 1, 1) <
, then Nt ( , 1) = m.
Proof: Since Nt (m, 1)
, there exist m different words
x1 , . . . , xm such that
| St ({ x1 , . . . , xm })| = | Bt ( x1 ) ∩ · · · ∩ Bt ( xm )|
and assume y1 , . . . , y are words which belong to this intersection. Therefore, d H ( xi , y j )
t for all 1
i
m and
1 j
, and thus
{ x1 , . . . , xm } ⊆ St ({ y1 , . . . , y }) = Bt ( y1 ) ∩ · · · ∩ Bt ( y ),
and hence Nt ( , 1) m.
Assume to the contrary that Nt ( , 1)
m + 1 and let
z1 , . . . , z be words such that
| St ({ z1 , . . . , z })| = | Bt ( z1 ) ∩ · · · ∩ Bt ( z )| m + 1.
As in the ﬁrst part, we get that Nt (m + 1, 1)
, which is a
contradiction. Hence, Nt ( , 1) = m.
In general, for a given set of words x1 , . . . , xm , the closer
the words are, the larger the size of the set St ({ x1 , . . . , xm })
is. In case d = 1, we look for a set of words that are all close
to each other, or equivalently - the maximum distance between
all pairs of words is minimized.
An anticode of diameter D is a set A ⊆ {0, 1}n of words
such that the maximum distance between every two words in
A is at most D. That is, for all x, y ∈ A, d H ( x, y) D. For
D
1, A( D ) is the size of the largest anticode of diameter
D. It was shown in [6] that the value of A( D ) is given by
b D ,n
if D is even,
2
A( D ) =
2b D−1 ,n−1 if D is odd.

Let us ﬁrst remind how this value was calculated. Assume
d H ( x, y) = d and the goal is to ﬁnd the cardinality of the set
St ({ x, y}) = { z ∈ {0, 1}n : d H ( z, x), d H ( z, y) t}.
For any word z ∈ St ({ x, y}), let S0,0 , S0,1 , S1,0 , S1,1 be the
following four sets:
S0,1 = {i : yi = xi , zi = xi },

S1,0 = {i : yi = xi , zi = xi },

S1,1 = {i : yi = zi = xi }.

Note that | S0,0 | + | S0,1 | = n − d and | S1,0 | + | S1,1 | = d.
Since d H ( z, x) t and d H ( z, y) t we get that
| S0,1 | + | S1,1 | t, | S0,1 | + | S1,0 | t,
or
| S0,1 | + | S1,1 | t, | S0,1 | + d − | S1,1 | t.
Denote | S0,1 | = i and | S1,1 | = k so we get
i + k t, i + d − k t,
or
0 i t − d/2 , i + d − t k t − i.
Therefore, the number of words in the intersection of these
two spheres is given by
t− d/2
t −i
n−d
d
| St ({ x, y})| = Nt (2, d) = ∑
∑ k .
i
i =0
k =i + d − t
a
where (b) = 0 if b < 0 or b > a.
If we substitute the order of i, k in the last term, we get
0 k min{d, t}, 0 i t − max{k, d − k}, and
min{d,t}

| St ({ x, y})|= Nt (2, d)=

∑

k=0

d
k

t−max{k,d−k}

∑

i =0

n−d
.
i

2

Our next goal is to show that for all D 1,
Nt ( A( D ), 1) = A(2t − D ).
That is, the t-associated set of a maximum anticode of diameter D is a maximum anticode of diameter 2t − D.
Lemma 5. For all 0 D 2t n,
Nt ( A( D ), 1) A(2t − D ).

(6)

This last representation of Nt (2, d) will be helpful in showing
the following property. Due to the lack of space, we omit the
proof of the next lemma as well as the following one, which
extends the solution of Nt (2, d) for m = 3.

Proof: Assume that D is even. We take the A( D ) words
in B D (0) and consider the set
2

Lemma 2. Let t, d be two positive integers such that d is even,
then
Nt (2, d) = Nt (2, d − 1).
Lemma 3. Let t, d be such that t >
The value of Nt (3, d) is given by
n − 3d
2
Nt (3, d) = ∑
i1
i ,i ,i ,i
1 2 3 4

d−1
2

d
2

d
2

i2

i3

i4

Bt ( x).

S t B D (0) =
2

x ∈ B D (0)
2

, and n large enough.

d
2

− t}

III. T HE C ASE d = 1

(5)

S0,0 = {i : yi = zi = xi },

d
2

Then, Bt− D (0) ⊆ St B D (0) and hence Nt ( A( D ), 1)
2
2
A(2t − D ) for even D.
In case that D is odd, let i = ( D − 1)/2. Let us start with
a maximal anticode of diameter 2i + 1. Let X be the set

,

3

X = Bi (0) ∪ Bi (e1 )={ aw : a ∈ {0, 1}, w ∈ Bin−1 (0)},
and let
n−
Y = Bt−i−1 (0) ∪ Bt−i−1 (e1 )={bu : b ∈{0, 1}, u ∈ Bt−i1 1 (0)}.
−
Then, for every x ∈ X, y ∈ Y, d H ( x, y)
t. Therefore,
Nt ( A( D ), 1) A(2t − D ) for odd D as well.
The equivalent upper bound is proved in the next two lemmas.
Lemma 6. For all 0
D
2t and n
(t − D )(2 D+1 + 1),
2
where D is even,
Nt ( A( D ) + 1, 1) < A(2t − D ).
Proof: Let X = { x1 , . . . , x A( D)+1 } be a set of A( D ) + 1
words. Since the largest anticode with diameter D has size
A( D ), there exist two words, say x1 , x2 , where d H ( x1 , x2 )
D + 1. Hence, the size of St ( X ) is no greater than the size
of St ({ x1 , x2 }), which, according to (5), is at most
t−( D +1)
2

M=

∑

j=0

n−D−1
j

Note that

t−( D +1)
2

M<

∑

j=0

t− j

∑

k= D +1−t+ j

t−( D +1)
2

∑

j=0

n
j+1

t− D
2

<

∑

j=0

D
D +1
2 )(2

n
j

Proof: We take the code C in Theorem 9 to be a code Cd
of minimum distance d and maximal rate ρd . Then, we get
| L( B)|
|Cd |
= ρd .
| B|
2n
Now, we can derive a connection with Nt (m, d).
Lemma 11. For all m, t, d and n large enough,
Nt ( ρd m , d) Nt (m, 1).
Proof: Assume that X is a set of m words such that
St ( X ) has size A. According to Lemma 10, let L( X ) be a
code in X of minimum distance d and size ρd m . Then,
St ( X ) ⊆ St ( L( X )) and thus Nt ( ρd m , d) Nt (m, 1).
Finally, in case m, t, d are ﬁxed we derive the following.
Lemma 12. For any ﬁxed m, t, d and n large enough, such that
d
t− d
2 ).
m 3 and t
2 , Nt (m, d ) = Θ (n
Proof: Since Nt (m, d)
Nt (2, d) and Nt (2, d) =
d
d
Θ(n 2 ), then Nt (m, d) is at most O(nt− 2 ).
To show the other direction of this equality, we show
an example of a set X such that the cardinality of St ( X )
d
j
is O(nt− 2 ). Let ei be the vector which its -th bit is
one if and only if
∈ {i, . . . , j}. For 1
i
m, let
i
i0 = (i − 1) d + 1 and i1 = i d , and xi = ei1 . Then,
2
2
0
d
for all i = j, d H ( xi , x j ) = 2 2
2d. For any vector y of
weight at most t − d , such that its ﬁrst m d bits are zero,
2
2
t−

n D +1
2
.
j

For 0
j
t − ( D + 1) and (t −
2
n D +1
n
( j )2
( j+1) and hence,
M<

D+1
.
k

Lemma 10. Let B be a set and let L( B) be a maximal code in
B with minimum distance d, then
| L( B)| ρd · | B|.

+ 1), we have

= A(2t − D ).

An equivalent property can be shown for D odd. We skip its
details due to its long proof and the lack of space.
Lemma 7. For all 0
D
2t, where D is odd, and n large
enough,
Nt ( A( D ) + 1, 1) < A(2t − D ).

t−

d

n−m

d

2 ) such vectors we
y ∈ St ( X ). Since there are ∑ =0 2 (
d
get that for n large enough, Nt (m, d) is at least O(nt− 2 ).
d
Together we conclude that Nt (m, d) = Θ(nt− 2 ).
V. S EQUENCES R ECONSTRUCTION D ECODERS

We summarize this result in the following corollary.
Corollary 8. For all 0 D 2t and n large enough,
Nt ( A( D ), 1) = A(2t − D ).
Proof: From Lemma 5 we get that Nt ( A( D ), 1)
A(2t − D ) and from Lemma 6 and Lemma 7 Nt ( A( D ) +
1, 1) < A(2t − D ). The conditions of Lemma 4 hold and thus
Nt ( A(2t − D ), 1) = A( D ), or Nt ( A( D ), 1) = A(2t − D ).

The main goal in [11] was to ﬁnd the necessary and sufﬁcient number of channels in order to have a successful decoder
for a code with minimum distance d. This number was studied
for different error models in [7]–[14], however the only decoder constructions, which we are aware of, were given in [2],
[5], [15] for channels with insertion and deletions, and in [4]
for deletions only. In this section, we show how to construct
decoders for substitution errors, where the decoder has to output the transmitted word (and not a list of words).
The case d = 1 was solved in [11] where the majority algorithm on each bit successfully decodes the transmitted word.
According to Lemma 2, this algorithm works for d = 2 as
well since the number of channels has to be the same. However, if d is greater than two, then the majority algorithm on
each bit no longer works. In general, according to Lemma 2,
if d is even then the number of channels for a code with minimum distance d or d − 1 is the same. Hence, we only need
to solve here the case of odd minimum distance.
For the rest of this section, we assume that the transmitted word belongs to a code C with odd minimum distance d,
there are at most t > d−1 errors in every channel, and the
2
number of channels is N = Nt (2, d) + 1. The N channel outputs are denoted by y1 , . . . , y N . Furthermore, the code C has
a decoder DC , which can successfully correct d−1 errors.
2

We note that the result shown by Levenshtein for d = 1 is a
special case of Corollary 8 for D = 1.
IV. E XTENSIONS FOR A RBITRARY d
Our goal in this section is to use the results found in Section III in order to derive bounds on Nt (m, d) for arbitrary d.
First, we state a useful Theorem from [1].
Theorem 9. [1] Let C be a code in the Hamming graph Γ with
distances from D = {d1 , . . . , ds } ⊆ {1, . . . , n}. Further let
LD ( B) be a maximal code in B ⊆ Γ with distances from D .
Then, one has
| LD ( B)|
|C|
.
|Γ |
| B|
For all d 1, we denote by ρd to be the maximal rate of
a code Cd of minimum distance d and length n, that is,
|Cd |
ρd = max
.
2n
Cd
Theorem 9 will serve us to prove the next lemma.

4

d−1
i.e. Ie1 ⊆ I. Hence, d H (c, y1 + e1 )
2 , so the decoder in
Step 2.b succeeds. Hence the algorithm succeeds and c = c.

A ﬁrst observation in constructing a decoder is that we can
always detect whether the output word is the transmitted one.
This can simply be done by checking if the maximum distance
from all channel outputs is at most t.
Lemma 13. For any c ∈ C , c = c if and only if
max {d H (c, yi )} t.

The complexity of Algorithm 15 is signiﬁcantly better than
the naive approach. However, the larger the value of ρ is, the
larger the algorithm’s complexity is. We report on another algorithm with better complexity, O(nN ), for the case d = 3.
VI. C ONCLUSION
This paper proposed a model of an associative memory:
Two words are associated if their Hamming distance is no
greater than some prescribed value t. Our main goal was to
study the maximum size of the associative memory output as
a function of the number of input words and their minimum
distance. We observed that this problem is a generalization of
the sequences reconstruction problem that was proposed by
Levenshtein. Finally, we presented a decoding algorithm for
the sequences reconstruction problem.
VII. ACKNOWLEDGEMENT
The authors thank Tuvi Etzion for helpful discussions on
anticodes. This research was supported in part by the ISEF
Foundation, the Lester Deutsch Fellowship, and the NSF Expeditions in Computing Program under grant CCF-0832824.
R EFERENCES

1 i N

Proof: If c = c then every channel suffers at most t errors
and thus max1 i N {d H (c, yi )}
t. In case c = c, let us
assume to the contrary that max1 i N {d H (c, yi )} t. Then
the set St ({c, c}) contains at least N = Nt (2, d) + 1 words
in contradiction to the deﬁnition of Nt (2, d).
A naive algorithm can choose any of the channel outputs
and add all error vectors of weight at most t − d−1 . For one
2
of these error vectors we will get a word with at most d−1
2
errors which can be decoded by the decoder of the code C . We
will show how to modify and improve the complexity of this
algorithm. Assume for example that t = d−1 + 1. Then, there
2
are two channel outputs, say y1 and y2 , that are different in
at least one bit location. If we ﬂip this bit in both y1 and y2 ,
then in exactly one of them the number of errors reduces by
one and thus is at most d−1 , which can be decoded by DC .
2
We show how to generalize this idea for arbitrary t. We let
ρ = t − d−1 . First, we prove the following Lemma.
2
Lemma 14. There exist two channel outputs yi , y j such that
d H ( yi , y j ) 2ρ − 1.

[1] R. Ahlswede, H.K. Aydinian, and L.H. Khachatrian, “On perfect codes
and related concepts,” Designs, Codes and Cryptography, vol. 22,
pp. 221–237, 2001.
[2] T. Batu, S. Kannan, S. Khanna, and A. McGregor, “Reconstructing
strings from random traces,” Proceedings of the Fifteenth Annual ACMSIAM Symposium on Discrete Algorithms, pp. 903–911, 2004.
[3] Y. Cassuto and M. Blaum, “Codes for symbol-pair read channels,” IEEE
Trans. on Information Theory, vol. 57, no. 12, pp. 8011–8020, Dec. 2011.
[4] T. Holenstein, M. Mitzenmacher, R. Panigrahy, and U. Wieder, “Trace reconstruction with constant deletion probability and related results,” Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete
Algorithms, pp. 389-398, 2008.
[5] S. Kannan and A. McGregor, “More on reconstructing strings from random traces: insertions and deletions,” Proc. IEEE International Symposium on Information Theory, pp. 297–301, Australia, Sep. 2005.
[6] D.J. Kleitman, “On a combinatorial conjecture of Erd˝ s,” J. Combin.
o
Theory, vol. 1, pp. 209–214, 1966.
[7] E. Konstantinova, “On reconstruction of signed permutations distorted
by reversal errors,” Discrete Mathematics, vol. 308, pp. 974–984, 2008.
[8] E. Konstantinova, “Reconstruction of permutations distorted by single
reversal errors,” Discrete Applied Math., vol. 155, pp. 2426–2434, 2007.
[9] E. Konstantinova, V.I. Levenshtein, and J. Siemons, “Reconstruction of permutations distorted by single transposition errors,”
arXiv:math/0702191v1, February 2007.
[10] V.I. Levenshtein, “Reconstructing objects from a minimal number of distorted patterns”, (in Russian), Dokl. Acad. Nauk 354 pp. 593-596; English translation, Doklady Mathematics, vol. 55 pp. 417–420, 1997.
[11] V.I. Levenshtein, “Efﬁcient reconstruction of sequences,” IEEE Trans.
on Information Theory, vol. 47, no. 1, pp. 2–22, January 2001.
[12] V.I. Levenshtein, “Efﬁcient reconstruction of sequences from their subsequences or supersequences”, Journal of Combin. Theory, Ser. A, vol. 93,
no. 2, pp. 310–332, 2001.
[13] V.I. Levenshtein, E. Konstantinova, E. Konstantinov, and S. Molodtsov,
“Reconstruction of a graph from 2-vicinities of its vertices,” Discrete
Applied Mathematics, vol. 156, pp. 1399–1406, 2008.
[14] V.I. Levenshtein and J. Siemons, “Error graphs and the reconstruction
of elements in groups,” Journal of Combin. Theory, Ser. A, vol. 116,
pp. 795–815, 2009.
[15] K. Viswanathan and R. Swaminathan, “Improved string reconstruction
over insertion-deletion channels,” Proceedings of the Nineteenth Annual
ACM-SIAM Symposium on Discrete Algorithms, pp. 399–408, 2008.
[16] E. Yaakobi, J. Bruck, and P.H. Siegel, “Decoding of cyclic codes over
symbol-pair read channels,” IEEE International Symposium on Information Theory, Cambridge, MA, July 2012.

Proof: Assume to the contrary that there do no exist such
words. Then, the words y1 , y2 , . . . , y N form an anticode of
diameter 2ρ − 2. According to [6], the maximum size of such
−1
an anticode is bρ−1,n = ∑ρ=0 (n), while according to (5) the
i
i
value of N satisﬁes t− d+1
2

N > Nt (2, d) =

∑

i =0
ρ−1

=

∑

i =0

n−d
i

t −i

∑

k = d − t +i

n−d
i
d
k

t −i

∑

k = d − t +i

d
k

> bρ−1,n .

Algorithm 15. The input to the decoder are the N words
y1 , . . . , y N and it returns an estimation c on c.
Step 1. Find two words yi , y j such d H ( yi , y j ) 2ρ − 1, and
let i1 , i2 , . . . , i2ρ−1 be 2ρ − 1 different indices that the
two vectors are different from each other.
Step 2. For all vectors e of weight ρ on these 2ρ − 1 indices,
a) D( y1 + e) = c1 , D( y2 + e) = c2 .
b) If max1 i N {d H (c1 , yi )} t, c = c1 .
c) If max1 i N {d H (c2 , yi )} t, c = c2 .
Theorem 16. The output of Algorithm 15 satisﬁes c = c.
Proof: The success of Step 1 is guaranteed according to
Lemma 14. For every index i j , 1
j 2ρ − 1, exactly one
of the channel outputs y1 or y2 has an error. Therefore, either
y1 or y2 has at least ρ errors on these indices. Without loss
of generality assume it is y1 and let I ⊆ {i1 , . . . , i2ρ−1 } be
a subset of its error locations, where | I | = ρ. In Step 2 we
exhaustively search over all error vectors e of weight ρ on
these 2ρ − 1 indices. For every error vector e let Ie = {i :
ei = 1}. Therefore, there exists an error vector e1 such that
its the set of indices with value one is covered by the set I,

5


Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 16:24:43 2012
ModDate:        Tue Jun 19 12:56:39 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      389540 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565029

Coordination via a Relay
Farzin Haddadpour, Mohammad Hossein Yassaee, Amin Gohari, Mohammad Reza Aref
Information Systems and Security Lab (ISSL)
Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran
Email: {haddadpour,yassaee}@ee.sharif.edu,{aminzadeh,aref}@sharif.edu

Abstract—In this paper, we study the problem of coordinating
two nodes which can only exchange information via a relay at
limited rates. The nodes are allowed to do a two-round interactive
two-way communication with the relay, after which they should
be able to generate i.i.d. copies of two random variables with
a given joint distribution within a vanishing total variation
distance. We prove inner and outer bounds on the coordination
capacity region for this problem. Our inner bound is proved using
the technique of “output statistics of random binning" that has
recently been developed by Yassaee, et al.

Rb1

Rb2

A1

A0

A2

A1

A0

A2

Rf1

I. I NTRODUCTION
Coordination is the problem of producing dependent random
variables over a network [1]. This problem differs from
traditional coding problems where the goal is to distribute
explicit messages. The problem of coordination for a joint
action has applications in distributed control and game theory
[2], [4]. Two notions of coordination have been deﬁned in
[1], namely empirical coordination and strong coordination. In
empirical coordination we want the empirical joint distribution
of the actions to be close to the desired distribution, whereas
in the strong coordination we want the total variation distance
between the joint probability distribution of the actions, and
the i.i.d. copies of the given distribution to be negligibly
small. In other words, the generated distribution and the i.i.d.
distribution should be statistically indistinguishable. These are
two different notions of coordination. In this paper we study
the strong notion of coordination.
As discussed in [1], nodes in a network can cooperate
arbitrarily without any communication if they are provided
with sufﬁcient common randomness. However [1] argues that
problem becomes nontrivial if the action of some of the nodes
is speciﬁed by nature. We believe that this is not the only situation where the problem becomes nontrivial. Consider a large
network of users, connected to each other through a proxy
(relay). The relay wants to make two nodes in the network
to cooperate with each other while making sure they do not
learn the identity of each other. Anonymity can be obtained
through the proxy who can privately exchange messages with
them. Since the two nodes cannot directly talk to each other
(as they do not know the identity of each other), they will
not be able to directly share randomness. However common
randomness can be created indirectly through the relay. But the
rate of this common randomness will be bounded from above
by the communication rate constraints between the nodes
and the relay. Furthermore creating common randomness for

Rf2

Figure 1. The model for coordination via a relay. In the ﬁrst step nodes
A1 and A2 communicate to the relay node A0 (as in the top subﬁgure).
In the second step the relay communicates to A1 and A2 (as in the bottom
subﬁgure).

later use may not be the optimal strategy if the ﬁnal goal
is coordination. The communication links between the nodes
and the relay are rate limited, and hence there may exist
more economic ways of using this resource. Inspired by this
discussion, we propose the following model as an attempt to
understand the use of a relay in cooperation of two nodes
whose actions are not speciﬁed by nature.
As shown in Fig. 1, we assume that there are four links
between the relay (A0 ) and the two nodes (A1 and A2 ). The
noiseless forward links from the relay to the the ﬁrst and
second nodes have rates Rf1 and Rf2 respectively. The backward links have rates Rb1 and Rb2 . As can be seen from the
ﬁgure, the nodes use the backward links ﬁrst to communicate
to the relay, after which the relay communicates back to the
nodes using the forward links. The goal of the two parties
is to generate i.i.d. copies of Y1 and Y2 jointly distributed
according to a given p(y1 , y2 ) within a vanishing total variation
distance. We don’t assume any common randomness shared
between A1 and A2 since the two nodes don’t share any
resources beyond private communication links with the proxy.
However, private randomization is allowed at all the three
nodes. Further we could have added a separate rate limited
public forward link from the proxy to all the nodes, where all
the bits put on this link will become available to all the parties.
Adding this link would make our model to resemble the model
proposed by Wyner [3] where a set of random bits were being
simultaneously transmitted to two parties. However, we have
excluded this from our model for simplicity.
Since the two nodes are initially communicating at rates
Rb1 and Rb2 , the nodes can use these only to generate pairwise
common randomness between themselves and the proxy. Thus

This work was supported by Iran-NSF under grant No. 88114.46.

1

three nodes A1 , A0 and A2 . They do not share any common
randomness, but private randomization is allowed. Let Mi be
the private randomness at node Ai . A (n, Rf1 , Rb1 , Rf2 , Rb2 )
coordination code consists of
• Two encoders at nodes Ak , k = 1, 2, that map Mk to
[1 : 2nRbk ].
• Two encoders at the relay node A0 , that map M0 × B1 ×
B2 to [1 : 2nRfk ] for k = 1, 2.
• Two decoders at nodes Ak , k = 1, 2, that map Mk ×
n
Bk × Fk to Yk .
Deﬁnition 1: A joint distribution q(y1 , y2 ) is said to be in
the admissible region of the rate tuple (Rf1 , Rb1 , Rf2 , Rb2 ) if
one can ﬁnd a sequence of (n, Rf1 , Rb1 , Rf2 , Rb2 ) coordination codes for n = 1, 2, ... whose induced joint distributions
n n
have marginal distributions p(y1 , y2 ) that satisfy

one can reinterpret the model as a one-way communication
problem from the relay to the two nodes in the presence of
pairwise common randomness. This has been the motivation
for naming Rf1 and Rf2 as forward links although they are
being used in the second step of the protocol.
It is noteworthy that to see when Rf1 = 0 and Rb1 = ∞
our model reduces to the one considered by Cuff in [4]. If
Rf1 = 0, the ﬁrst node does not receive any feedback and has
to create the i.i.d. copies of Y1n by itself. Since Rb1 = ∞,
the ﬁrst node can send Y1n completely to the relay. The
relay is receiving Rb2 bits from the second node which can
be understood as a common randomness shared between A0
and A2 . Thus, our problem reduces to the problem of [4].
If Rf1 = ∞, the problem reduces to a special case of the
problem studied in [6]. In this case the relay is effectively
coordinating with the second node because the relay can send
its reconstruction of Y1n to the ﬁrst node using the forward
link Rf1 of inﬁnite capacity. Thus this would be the problem
of generating Y1n and Y2n using a two-round communication
scheme when the two node share no common randomness.
When Rb1 = Rb2 = ∞, the problem reduces to that of
coordinating A1 and A2 when there are pairwise common
randomness shared between (A0 , A1 ) and (A0 , A2 ) but no
common randomness shared among the three. Finally when
Rb1 = Rb2 = 0 the problem reduces to a problem that
resembles Wyner’s model [3].
We prove an inner and an outer bound on our model. We
show that the inner and the outer bound match in certain
special cases, two of which are of special interest: one is
when Rb1 = Rb2 = ∞, i.e. an inﬁnite pairwise common
randomness, the other is when Rb1 = Rb2 = 0, i.e. no pairwise
common randomness. We show that when Rb1 = Rb2 = ∞,
the capacity region is the one where Rf1 + Rf2 is greater than
or equal to the mutual information between Y1 and Y2 . In the
other extreme case both Rf1 and Rf2 have to be larger than
Wyner’s common information. This provides insights on the
role of pairwise common randomness.
This paper is organized as follows: in Section II, we
introduce the basic notations and deﬁnitions used in this paper.
Section III contains the main results of the paper, and Section
IV and V includes the proofs.

n

lim

n→∞

n n
p(y1 , y2 )−

= 0.

q(y1,i , y2,i )
i=1

1

Deﬁnition 2: Given a joint distribution q(y1 , y2 ), the coordination rate region is the closure of the set of rate tuples
(Rf1 , Rb1 , Rf2 , Rb2 ) that admit the channel q(y1 , y2 ).
III. M AIN R ESULTS
Theorem 1 (Inner bound): The following region forms an
inner bound to the coordination rate region for q(y1 , y2 ): Rin
is the set of all non-negative rate tuples (Rf1 , Rb1 , Rf2 , Rb2 ),
for which there exists p(u, v, w, y1 , y2 ) ∈ Tin such that
Rb1 + Rf1 + Rb2 + Rf2 ≥ I(Y1 Y2 ; V U W ) + I(U ; V |W )
+ I(W ; Y1 Y2 ),
Rb1 + Rf1 ≥ I(Y1 Y2 ; V W ),
Rb2 + Rf 2 ≥ I(Y1 Y2 ; U W ),
Rf2 + Rf1 ≥I(U ; V |W ) + I(W ; Y1 Y2 ),

(1)

where
Tin = {p(u, v, w, y1 , y2 ) :(Y1 , Y2 ) ∼ q(y1 , y2 ),
Y2 − U W − V W − Y1 }.
Theorem 2 (Outer bound): Take a desired distribution
q(y1 , y2 ). Then the coordination rate region is contained in
the region Rout which is the closure of the set of all nonnegative rate tuples (Rf1 , Rb1 , Rf2 , Rb2 ), for which there
exists p(u, v, y1 , y2 ) ∈ Tout such that

II. D EFINITIONS
A. Notation
In this paper, we use pU to denote the uniform distribution
A
n
over the set A and p(xn ) to denote the i.i.d. pmf i=1 p(xi ),
unless otherwise stated. Also we use XS to denote (Xj : j ∈
S). The total variation between two pmf’s p and q on the
same alphabet X , is denoted by p(x) − q(x) 1 . When a pmf
itself is random, we use capital letter, e.g. PX . For simplicity
of notation, we use PX ≈ QX if E PX − QX 1 < , where
PX and QX are random pmf

Rb1 + Rf1 ≥ I(Y1 Y2 ; V ),
Rb2 + Rf2 ≥ I(Y1 Y2 ; U ),
Rf2 + Rf1 ≥ max{I(U ; Y1 ), I(V ; Y2 )},
where
Tout = {p(u, v, y1 , y2 ) :(Y1 , Y2 ) ∼ q(y1 , y2 ),
Y2 − U − Y1 ,
Y2 − V − Y1 ,

B. Problem Statement

|U| ≤ |Y1 | × |Y2 | + 1,

Consider the problem of strong coordination over a network
with a relay, as depicted in Figure 1. In this setting, there are

|V| ≤ |Y1 | × |Y2 | + 1}.

2

(2)

n
for reconstructing X[1:T ] in the presence of (B1:T , Y n ) at the
decoder. As in the achievability proof of the [7, Theorem
15.4.1], we can deﬁne a decoder with respect to any ﬁxed
distributed binning. We denote the decoder by the random
conditional pmf P SW (ˆn ] |y n , b[1:T ] ) (note that since the
x[1:T
decoder is a function, this pmf takes only two values, 0 and
1). Now we write the Slepian-Wolf theorem in the following
equivalent form. See [9] for details.
Lemma 1: If for each S ⊆ [1 : T ], the following constraint
holds
Rt > H(XS |XS c , Y ),
(5)

Corollary 1: The inner bound and the outer bound match
when Rb1 = Rb2 = ∞, both reducing to Rf2 + Rf1 ≥
I(Y1 ; Y2 ). This corresponds to the case of inﬁnite pairwise
common randomness and has not been considered (to best of
our knowledge) in the previous works. When Rf1 = ∞, the
inner and outer bound reduce to Rb2 + Rf2 being greater than
or equal to Wyner’s common information. The inner and outer
bound also match when Rf1 = 0 and Rb1 = ∞. To see this
let V = Y1 and W = cont. in the inner bound. On the other
hand the optimal choice for V in the outer bound is V = Y1 .
Thus both regions reduce to the following region that matches
the one given in [4].

t∈S

then as n goes to inﬁnity, we have

Rb2 + Rf2 ≥ I(Y1 Y2 ; U ),
Rf2 ≥ I(U ; Y1 ).

ˆ[1:T
x[1:T
E P (xn ] , y n , xn ] ) − p(xn ] , y n )1{ˆn ] = xn ] }
[1:T
[1:T
[1:T

Another extreme case is when Rb1 = Rb2 = 0. Here we take
U = V = cont. in the inner bound. It is easy to see that both
the inner and outer bound reduce to Rf1 and Rf2 being greater
than or equal to Wyner’s common information. Comparing this
case with Wyner’s model, we see that an optimal strategy is to
send the same message to both A1 and A2 (which is expected
when Rb1 = Rb2 = 0). The inner and outer bound also match
when Y1 = (A, B), Y2 = (A, C) for mutually independent
random variable A, B and C.

Lemma 2: We have
1) pX pY |X − qX pY |X 1 = pX − qX 1
pX − qX 1 ≤ pX pY |X − qX qY |X 1
2) If pX pY |X ≈ qX qY |X , then there exists x ∈ X such
2

that pY |X=x ≈ qY |X=x .
3) If PX

B. Proof of Theorem 1
The proof is divided into three parts. In the ﬁrst part
we introduce two protocols each of which induces a pmf
on a certain set of r.v.’s. The ﬁrst protocol has the desired
i.i.d. property on Y1n and Y2n , but leads to no concrete
coding algorithm. However the second protocol is suitable
for construction of a code, with one exception: the second
protocol is assisted with an extra common randomness that
does not really exist in the model. In the second part we
ﬁnd conditions on Rb1 , Rb2 , Rf1 , Rf2 implying that these two
induced distributions are almost identical. In the third part of
the proof, we eliminate the extra common randomness given
to the second protocol without disturbing the pmf induced on
the desired random variables (Y1n and Y2n ) signiﬁcantly. This
makes the second protocol useful for code construction.
Part (1) of the proof: We deﬁne two protocols each of
which induces a joint distribution on random variables that
are deﬁned during the protocol.
Protocol A. Let (W n , U n , V n , Y1n , Y2n ) be i.i.d. and distributed according to p(w, v, u, y1 , y2 ) such that the marginal
pmf of (Y1 , Y2 ) satisﬁes p(y1 , y2 ) = q(y1 , y2 ). Consider the
following random binning:
n
• To each sequence w , assign a random bin index g0 ∈
˜
nR 0
[1 : 2
].
n n
• To each pair (w , v ), assign three random bin indices
˜
nR 1
g1 ∈ [1 : 2
], b1 ∈ [1 : 2nRb1 ] and f1 ∈ [1 : 2nRf1 ].
n
n
• To each pair (w , u ), assign three random bin indices
˜2
g2 ∈ [1 : 2nR ], b2 ∈ [1 : 2nRb2 ] and f2 ∈ [1 : 2nRf2 ].
• We use a Slepian-Wolf decoder to recover w1 , v from
ˆ n ˆn
(g0 , g1 , b1 , f1 ), and another Slepian-Wolf decoder to recover w2 , un from (g0 , g2 , b2 , f2 ). The rate constraints
ˆn ˆ

A. Review on probability approximation via random binning
[9]
Let (X[1:T ] , Y ) be a DMCS distributed according to a
T
joint pmf pX[1:T ] ,Y on a countably inﬁnite set i=1 Xi × Y.
A distributed random binning consists of a set of random
mappings Bi : Xin → [1 : 2nRi ], i ∈ [1 : T ], in which Bi
maps each sequence of Xin uniformly and independently to
n
[1 : 2nRi ]. We denote the random variable Bt (Xt ) by Bt . A
random distributed binning induces the following random pmf
T
n
on the set X[1:T ] × Y n × t=1 [1 : 2nRt ],
T

1{Bt (xn ) = bt }.
t
t=1

Theorem 3 ([9]): If for each S ⊆ [1 : T ], the following
constraint holds
Rt < H(XS |Y ),

(3)

t∈S

then as n goes to inﬁnity, we have
T

pU nRt ] (bt )
[1:2
t=1

δ

≈ PX QY |X , then

+δ

We apply the techniques of [9] to prove the achievability
of the theorem. We begin by a providing a summary of the
lemmas we need. In the following subsection we provide the
proof.

E P (y n , b[1:T ] ) − p(y n )

≈ QX and PX PY |X

PX PY |X ≈ QX QY |X .

IV. ACHIEVABILITY

P (xn ] , y n , b[1:T ] ) = p(xn ] , y n )
[1:T
[1:T

→ 0.
1

→ 0.

(4)

1

We now consider another region for which we can approximate a speciﬁed pmf. This region is the Slepian-Wolf region

3

for the success of these decoders will be imposed later,
although these decoders can be conceived even when
there is no guarantee of success.

ˆ
P (g[0:2] , b1 , b2 , wn , v n , un , w1 , v n , w2 , un )
ˆn ˆ ˆn ˆ

The random1 pmf induced by the random binning, denoted by
P , can be expressed as follows:

(n)
0

≈ P (g[0:2] , b1 , b2 , wn , v n , un , w1 , v n , w2 , un )
ˆn ˆ ˆn ˆ

P (g0 |wn )P (g1 b1 f1 |wn v n )P (g2 b2 f2 |wn un )p(wn , v n , un ) ×
n
n
p(y1 |wn un )p(y2 |wn v n ).

˜
R1 + Rb1 + Rf1
˜
˜
R0 + R1 + Rb1 + Rf1
˜ 2 + Rb + Rf
R
2
2
˜ 0 + R2 + Rb + Rf
˜
R

Protocol B. In this protocol we assume that the nodes
have access to the extra common randomness (G0 , G1 , G2 )
where G0 , G1 , G2 are mutually independent random variables
˜
˜
distributed uniformly over the sets [1 : 2nR0 ], [1 : 2nR1 ] and
˜2
[1 : 2nR ], respectively. Now, we use the following protocol:

2

≥ H(V |W ),
≥ H(W V ),
≥ H(U |W ),

(9)

≥ H(W U ),
(n)
1 ,

we have

P (g[0:2] , b1 , b2 , wn , v n , un , w1 , v n , w2 , un )
ˆn ˆ ˆn ˆ
(n)
1

≈ P (g[0:2] , b1 , b2 , wn , v n , un )
ˆ
× 1{w1 = wn , v n = v n , w2 = wn , un = un }.
ˆn
ˆn
ˆ
Using equation (8) we have
ˆ
P (g[0:2] , b1 , b2 , wn , v n , un , w1 , v n , w2 , un )
ˆn ˆ ˆn ˆ
(n)
(n)
0 + 1

P (g[0:2] , b1 , b2 , wn , v n , un )
ˆ
× 1{w1 = wn , v n = v n , w2 = wn , un = un }.
ˆn
ˆn
ˆ

≈

The third part of Lemma 2 implies that
ˆ
P (g[0:2] , b1 , b2 , wn , v n , un , w1 , v n , w2 , un )
ˆn ˆ ˆn ˆ
n
n
× p(y1 |w1 , v n )p(y2 |w2 , un )
ˆn ˆ
ˆn ˆ
(n)
(n)
0 + 1

P (g[0:2] , b1 , b2 , wn , v n , un )
ˆ
× 1{w1 = wn , v n = v n , w2 = wn , un = un }
ˆn
ˆn
ˆ

≈

pU (g[0:2] )pU (b1 )pU (b2 )P (wn , v n , un , f[1:2] |g[0:2] b[1:2] )×

n
n
× p(y1 |w1 , v n )p(y2 |w2 , un )
ˆn ˆ
ˆn ˆ

P SW (w1 , v n |g0 , g1 , b1 , f1 )P SW (w2 , un |g0 , g2 , b2 , f2 )×
ˆn ˆ
ˆn ˆ

P (g[0:2] , b1 , b2 , wn , v n , un )
ˆ
× 1{w1 = wn , v n = v n , w2 = wn , un = un }
ˆn
ˆn
ˆ

=

(6)

n
n
n
n
× p(y1 |w1 , v n )p(y2 |w2 , un ).

Part (2) of the proof: Sufﬁcient conditions that make the
induced pmfs approximately the same: To ﬁnd the constraints
ˆ
that imply that the pmf P is close to the pmf P in total
ˆ
variation distance, we start with P and make it close to P
in a few steps. The ﬁrst step is to observe that g0 , (g1 , b1 ) and
(g2 , b2 ) are the bin indices of wn , (wn , v n ) and (wn , un ),
respectively. Substituting T = 3, X1 = W , X2 = W V ,
X3 = W U and Y = ∅ in Theorem 3, implies that if

Thus,
n n
ˆ
P (g[0:2] , b1 , b2 , wn , v n , un , w1 , v n , w2 , un , y1 , y2 )
ˆn ˆ ˆn ˆ
(n)
(n)
0 + 1

n n
P (g[0:2] , b1 , b2 , wn , v n , un , y1 , y2 )
ˆ
× 1{w1 = wn , v n = v n , w2 = wn , un = un }.
ˆn
ˆn
ˆ

≈

Using the second item in part 1 of Lemma 2 we conclude that

˜
R0 < H(W ),

1

2

then for some vanishing sequence

At the ﬁrst stage, the node A1 chooses an index b1 ∈
[1 : 2nRb1 ] uniformly at random and sends it to the node
A0 . Also the node A2 independently chooses an index
b2 ∈ [1 : 2nRb2 ] uniformly at random and sends it to the
node A0 .
• In the second stage, knowing (g0 , g1 , g2 , b1 , b2 ), the node
A0 generates sequences (wn , v n , un ) according to the
conditional pmf P (wn , v n , un |g0 , g1 , g2 , b1 , b2 ) of the
protocol A. Then it sends the bin indices f1 (wn , v n ) and
f2 (wn , un ) to the nodes A1 and A2 , respectively.
• At
the ﬁnal stage, the node A1 , knowing
(g0 , g1 , b1 , f1 ) uses the Slepian-Wolf decoder
P SW (w1 , v n |g0 , g1 , b1 , f1 ) to obtain an estimate of
ˆn ˆ
n n
n
(w , v ). Then, it generates a sequence y1 according
n
n n
to pY n |W n V n (y1 |w1 , v ). The node A2 proceeds in a
ˆ ˆ
similar way.
ˆ
The random pmf induced by the protocol, denoted by P ,
factors as
•

˜
˜
R0 + R1 + Rb1 < H(W V ),
˜
˜
R0 + R2 + Rb2 < H(W U ),
˜
˜
˜
R0 + R1 + R2 + Rb + Rb < H(W V U ),

(8)

The next step is to see that for the Slepian-Wolf decoders
of the ﬁrst protocol to work well, Lemma 1 requires imposing
the following constraints:

P SW (w1 , v n |g0 , g1 , b1 , f1 )P SW (w2 , un |g0 , g2 , b2 , f2 )×
ˆn ˆ
ˆn ˆ

n
n
p(y1 |w1 , v n )p(y2 |w2 , un )
ˆn ˆ
ˆn ˆ

(n)
0

(n)

then there exists 0 → 0 such that P (g[0:2] , b1 , b2 ) ≈
ˆ
pU (g[0:2] )pU (b1 )pU (b2 ) = P (g[0:2] , b1 , b2 ). This implies

n n
ˆ
P (g[0:2] , y1 , y2 )

(7)

(n)
(n)
0 + 1

≈

n n
P (g[0:2] , y1 , y2 ).

In particular, the marginal pmf of (Y1n , Y2n ) of the RHS of
n n
this expression is equal to p(y1 , y2 ) which is the desired pmf.
Part (3) of the proof: In the protocol we assumed that the
nodes have access to an external randomness G[0:2] which is
not present in the model. Nevertheless, we can assume that

2

1 The pmf is random because we are doing a random binning assignment
in the protocol.

4

the nodes agree on an instance g[0:2] of G[0:2] . In this case,
ˆ n n
the induced pmf P (y1 , y2 ) changes to the conditional pmf
n n
ˆ
P (y1 , y2 |g[0:2] ). But if G[0:2] is independent of (Y1n , Y2n ),
ˆ n n
then the conditional pmf P (y1 , y2 |g[0:2] ) is also close to the
desired distribution. To obtain the independence, we again
use Theorem 3. Substituting T = 3, X1 = W , X2 = W V ,
X3 = W U and Y = Y1 Y2 in Theorem 3, asserts that if
˜
R0 < H(W |Y1 Y2 ),
˜
˜
R0 + R1 < H(W V |Y1 Y2 ),
˜
˜
R0 + R2 < H(W U |Y1 Y2 ),

where gi ( ) stands for functions that converge to zero as
converges to zero. Equations (11) and (12) hold, due to Lemma
20 and Lemma 21 of [5]. In the same way one can show that
n(Rf2 + Rf1 ) ≥ nI(V ; Y2,Q ) − g1 ( ) − g2 ( ).
Next in a similar fashion we have
n(Rf1 + Rb1 ) ≥ H(F1 B1 )
≥ I(F1 B1 ; Y2n Y1n )
n

(10)

I(F1 B1 ; Y1,q Y2,q | Y1q−1 Y2q−1 )

=
q=1
n

˜
˜
˜
R0 + R1 + R2 < H(W V U |Y1 Y2 ),

I(F1 B1 Y1q−1 Y2q−1 ; Y1,q Y2,q )

=
(n)
2

q=1

n n
n n
then P (y1 , y2 , g[0:2] ) ≈ pU (g[0:2] )p(y1 , y2 ), for some van(n)
ishing sequence 2 . Using triangular inequality for total
(n)
ˆ
variation, we have P (y n , y n , g[0:2] ) ≈ pU (g[0:2] )p(y n , y n ),
1
(n)
2
i=0 i .

2

1

− I(Y1q−1 Y2q−1 ; Y1,q Y2,q )
n

2

q=1

≥ nI(V ; Y1,Q Y2,Q ) − ng3 ( ) − ng4 ( ). (15)
A similar statement can be proved for n(Rf2 + Rb2 ).
In summary, we have proved that for every , any achievable rate tuple must belong to the set Rout, deﬁned as the
set of all tuples (Rf1 , Rf2 , Rb1 , Rb2 ) such that there exists
p(u, v, y1 , y2 ) ∈ Tout, for which (Rf1 , Rf2 , Rb1 , Rb2 ) satisﬁes
the inequalities (13), (14) and (15) where Tout, is the set
of p(u, v, y1 , y2 ) satisfying the Markov relations as in the
deﬁnition of Tout and

(n)

n n
p(y1 , y2 , g[0:2] ) ≈ pU (g[0:2] )p(y1 , y2 ). Now, the second
ˆ n n
part of Lemma 2 shows that there exists an instance g[0:2]
(n)

n n
such that p(y1 , y2 |g[0:2] ) ≈ p(y1 , y2 ). Finally, eliminating
ˆ n n
˜ 0 , R1 , R2 ) from (7), (9) and (10) by using Fourier-Motzkin
˜ ˜
(R
elimination results in the rate region (1).

V. C ONVERSE

p(y1 , y2 ) − q(y1 , y2 )

Let Q denote a uniform random variable over [1 : n]
and independent of all previously deﬁned random variables.
We choose single-letter auxiliary random variables as follows:
U = (F2 , B2 , Y1Q−1 , Q) and V = (F1 , B1 , Y2Q−1 , Q). Using
the fact that I(B2 ; B1 ) = 0 that comes from the model
(because A1 and A2 are creating these random variables at
the beginning) we have:

[1] P. Cuff, H. Permuter and T. M. Cover, “Coordination capacity," IEEE
Trans. Inform. Theory, 56(9): 4181–4206, 2010.
[2] V. Anantharam and V. Borkar, “Common Randomness and Distributed
Control: A Counterexample," Systems and Control Letters, vol. 56, no.
7-8, July 2007.
[3] A. Wyner, “The Common Information of Two Dependent Random
Variables," IEEE Trans. Inform. Theory, 21 (2), pp. , 1975.
[4] P. Cuff, “Communication requirements for generating correlated random
variables," in Proc. IEEE Int. Symp. Inform. Theory (ISIT), 2008,
pp.1393-1397.
[5] P. Cuff. “Communication in networks for coordinating behavior," Ph.D
dissertation, Stanford Univ., CA. Jul. 2009.
[6] A. Gohari and V. Anantharam, “Generating dependent random variables
over networks," in Proc. IEEE Inform. Theory Workshop(ITW), 2011,
pp.698-672.
[7] T. M. Cover and J. A. Thomas, “Elements of Information Theory,"
Second edition, John Wiley & Sons, Inc, 2006.
[8] A. El Gamal and Y.-H. Kim, “Lecture notes on network information
theory", available online, Arxiv:1001.3404, 2010.
[9] M. H. Yassaee, M. R. Aref and A. Gohari, “Achievability proof
via output statistics of random binning," to appear in ISIT 2012,
arXiv:1203.0730.

= I(F2 B2 ; F1 B1 )
≥ I(F2 B2 ; Y1n )
n

I(F2 B2 ; Y1,q | Y1q−1 )
q=1
n

[I(F2 B2 Y1q−1 ; Y1,q ) − I(Y1q−1 ; Y1,q )]
q=1
n

I(F2 B2 Y1q−1 ; Y1,q ) − ng1 ( )

≥

(11)

q=1

= nI(F2 B2 Y1Q−1 ; Y1,Q |Q) − ng1 ( ),
≥ nI(F2 B2 Y1Q−1 , Q; Y1,Q )
− ng1 ( ) − ng2 ( )
= nI(U ; Y1,Q ) − ng1 ( ) − ng2 ( ),

< .

R EFERENCES

≥ I(F2 ; F1 B1 | B2 ) + I(B2 ; F1 | B1 )

=

1

The proof continues by showing that ∩ >0 Rout, = Rout .
Note that the cardinality bounds can be proved using the
standard Fenchel extension of the Caratheodory theorem [8].
This completes the proof for the converse.

n(Rf2 + Rf1 ) ≥ H(F2 ) + H(F1 )

≥

[I(F1 B1 Y2q−1 ; Y1,q Y2,q ) − g3 ( )]

≥

where (n) =
Thus, there exists a ﬁxed binning
with the corresponding pmf p such that if we replace P
¯
with p in (6) and denote the resulting pmf with p, then
¯
ˆ

2

(14)

(12)
(13)

5


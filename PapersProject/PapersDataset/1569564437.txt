Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May 16 18:11:21 2012
ModDate:        Tue Jun 19 12:56:11 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      335187 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564437

Mutual Information for a Deletion Channel
Michael Drmota

Wojciech Szpankowski

Krishnamurthy Viswanathan

TU Wien
A-1040 Wien, Austria
Email: michael.drmota@tuwien.ac.at

Purdue University
West Lafayette, IN, USA
Email: spa@cs.purdue.edu

Hewlett-Packard Laboratories
Palo Alto, CA, USA
Email: krishnamurthy.viswanathan@hp.com

where IA = 1 if A is true and zero otherwise. The problem of
counting subsequences in a text is known as the hidden pattern
matching problem and was studied in [1], [7]. In this paper,
to derive our results we ﬁrst represent the mutual information
between the input and output of a deletion channel in terms
of the count ΩX (w) for a random sequence X.

Abstract—We study the binary deletion channel where each
input bit is independently deleted according to a ﬁxed probability.
We relate the conditional probability distribution of the output
of the deletion channel given the input to the hidden pattern
matching problem. This yields a new characterization of the
mutual information between the input and output of the deletion
channel. Through this characterization we are able to comment
on the the deletion channel capacity, in particular for deletion
probabilities approaching 0 and 1.

n
Theorem 1. For any random input X1 , the mutual information satisﬁes

I. I NTRODUCTION

w
n
n
− E[ΩX1 (w)] log E[ΩX1 (w)] ,

n
n
From Theorem 1, we have I(X1 ; Y (X1 ))
n
n
n
n
S1 (X1 , Y (X1 )) − S2 (X1 , Y (X1 )) := S1 − S2 where
n
n
dn−|w| (1 − d)|w| E[ΩX1 (w) log ΩX1 (w)],

S1 =

=
(3)

w
n
n
dn−|w| (1 − d)|w| E[ΩX1 (w)] log E[ΩX1 (w)]. (4)

S2 =
w

n
In this paper, we focus on memoryless distributions on X1 ,
however, it appears that most of our results extend to larger
classes (Markovian). Suppose that X1 X2 . . . is an i.i.d. sequence of Bernoulli random variables with parameter p. For
1
n
n
such sequences, let I(d, p) = lim I(X1 ; Y (X1 )), and
n→∞ n
1
n
n
λ(d, p) = limn→∞ n S1 (X1 , Y (X1 )).

Theorem 2. For all 0 ≤ d ≤ 1, and 0 ≤ p ≤ 1, the limit
I(d, p) as well as the non-negative limits λ(d, p) and

1
n
n
sup I(X1 ; Y (X1 )),
n→∞ n P n
X

lim

n→∞

1

1
n
n
S2 (X1 , Y (X1 )) = H(1 − d) − (1 − d)H(p)
n

exist, and

n
n
n
n
where PX1 is the distribution of X1 , and I(X1 ; Y (X1 ))
is the mutual information between the input and output of
the deletion channel. Many bounds have been derived for the
capacity (see the survey article by Mitzenmacher [11]).
Let x = xn ∈ {0, 1}n and w = w1 w2 . . . wm ∈ {0, 1}m ,
1
m ≤ n, be binary sequences. Let Ωx (w) denote the number
of occurrences of w as a subsequence (i.e., not consecutive
symbols) of x, that is,

I[xi1 =w1 ] I[xi2 =w2 ]
1≤i1 <i2 <···<im ≤n

(2)

where the sum is over all binary sequences of length ≤ n.

C(d) = lim

Ωx (w) =

n
n
dn−|w|(1 − d)|w| E[ΩX1 (w)log ΩX1 (w)]

n
n
I(X1 ;Y (X1 )) =

A deletion channel with parameter d takes a binary sequence x := xn = x1 · · · xn where xi ∈ A = {0, 1} as
1
input and deletes each symbol in the sequence independently
with probability d. The output of such a channel is then
a subsequence Y = Y (x) = xi1 ...xiM of x, where M
follows the binomial distribution Bi(n, (1 − d)), and the
indices i1 , ..., iM correspond to the bits that are not deleted.
Despite signiﬁcant effort [2], [3], [5], [9], [10], [11], [12], [14]
the mutual information between the input and output of the
deletion channel and its capacity are still unknown. Our goal
is to provide a more detailed characterization of the mutual
information for memoryless sources (extensions to strongly
mixing sources or Markovian sources seem likely). Through
this characterization we are able to comment on the channel
capacity for two special cases: d → 1 and d → 0. The latter
case was already discussed in [10], [9]. We derive our results
by relating the the conditional probability distribution of the
output of the deletion channel given the input to the so called
hidden pattern matching analyzed recently in [1], [7].
Following [4], the channel capacity of the deletion channel
with deletion probability d is

I(d, p) = λ(d, p) + (1 − d)H(p) − H(1 − d)
where, H(·) is the binary entropy function. Further1
n
n
more, I(d, p) = inf n≥1 n I(X1 ; Y (X1 )), and λ(d, p) =
1
n
n
supn≥1 n S1 (X1 , Y (X1 )).
1
1
From Theorem 2, I(d, p) ≤ I(X1 ; Y (X1 )) = H(p)(1 − d).
When optimized over p, this upper bound matches the capacity
asymptotically for d → 0 but not for d → 1, as our next result
(Theorem 3) shows. This also implies that λ(d, p) ≤ H(1−d).
Note that for d → 1 it is just known that C(d) = Θ(1−d) [2],

· · · I[xim =wm ] , (1)

1

[11], [12]. Our next result is a bound on I(d, p) that implies
that, in contrast to the case d → 0, i.i.d. distributions over the
n
inputs X1 do not asymptotically achieve capacity as d → 1.

with |w|=m P n (w) = 1. In particular, if X is memoryless,
then P n (w) = P (w) where P (w) is the probability that
X1 X2 ...X|w| = w (see [1] for dynamic X).

Theorem 3. For all p ≥ 0, as d → 1

Proof: Taking expectation on both sides of (1) we have

1
I(d, p) ≤ K(1 − d)4/3 log
1−d
where the constant K > 0 is absolute.

P (Xi1 = w1 , · · · , Xim = wm ) =
1≤i1 <···<im ≤n

proving the lemma.

Finally we demonstrate the strength of our method by reproving Kanoria and Montanari’s [10] expansion for I(d, p)
for d → 0 leading to C(d) = I(d, 1/2) + O(d3/2−ε ) =
1 + d log d − Ad + O(d3/2−ε ) (cf. Theorem 4), where A =
− −1
log(2e) −
log . Note that the symmetric mem≥1 2
oryless distribution is asymptotically optimal in this regime.

Lemma 2. For any distribution on the input binary rann
dom sequence X1 , and and deletion probability d ≥ 0,
n
n
I(X1 ; Y (X1 )) ≤ n(1 − d).
Proof: Following Theorem 1, we can write
n
n
I(X1 ; Y (X1 )) = S1 − S2 where S1 and S2 are deﬁned in
n
(3)–(4). Since ΩX (w) ≤ |w| we ﬁrst have

II. P ROOF OF T HEOREM 1 AND C APACITY B OUND
In this section, we ﬁrst prove Theorem 1 and then present
a simple proof of the fact that C(d) ≤ 1 − d.
A. Proof of Theorem 1
To prove Theorem 1, we relate hidden pattern matching to
the deletion channel through the following observation. For all
xn ∈ An
1

w

n
n
I(X1 ; Y (X1 ))

w
n

n
(X1 )

dn−m (1 − d)m

=−
m=0

+ E[ΩX (w)] log(d

(1 − d)

dn−m (1 − d)m
)).

(6)

m=0 |w|=m

III. M EMORYLESS I NPUT D ISTRIBUTIONS
We now restrict the channel input distributions to be memoryless over A with p denoting the probability of “0”. We
prove Theorems 2 and 3 in this section.

w

+ E[ΩX (w)] log dn−|w| (1 − d)|w| ).

(7)

The theorem follows from (6) and (7).

A. Proof of Theorem 2
The next lemma follows from the deﬁnition ΩX (w).

B. Upper Bound for the Capacity

Lemma 3. For all binary sequences w, and all xn+k ∈ An+k

It is well known that the capacity C(d) of a deletion channel
with deletion probability d can be bounded from above by the
capacity of an erasure channel with the erasure probability d
(e.g., see [3]). We provide a direct proof of this fact. To do
so, we ﬁrst compute the expectation of ΩX (w).

Ωxn+k (w) =

n+1

(9)

w1 w2 =w

where the sum is taken over all pairs w1 , w2 such that their
concatenation w1 w2 equals w.
We also require the following lemma.

n
P n (w),
|w|

Lemma 4. Let zm and am , 1 ≤ m ≤ M , be non-negative
numbers. Then we have

where
1

Ωxn (w1 )Ωxn+k (w2 ),
1

1

n
Lemma 1. For any random X1 , and all binary sequences w

P n (w) =

n
m = n · (1 − d).
m

Substituting this in (8) completes the proof, and also establishes an upper bound of C(d) ≤ 1 − d for the capacity.

dn−|w| (1 − d)|w| (E[ΩX (w) log ΩX (w)]

n
E ΩX1 (w) =

P n (w) log(P n (w)). (8)
|w|=m

n

Next, we compute the conditional entropy H(Y |X). Notice
that for x ∈ An and y ∈ Am we have P (x, y) =
P (x)Ωx (y)dn−m (1 − d)m . Combining this with (5) we obtain
H(Y |X) = −

n
m

Since for all m ≥ 0, P n (w) is a probability distribution over
w ∈ Am , we have |w|=m P n (w) log(1/P n (w)) ≤ log 2m =
m, and consequently

w
|w|

n
P n (w) log P n (w)
|w|

dn−|w| (1 − d)|w|

≤−

n
dn−|w| (1 − d)|w| E[ΩX1 (w)] log E[ΩX (w)]

n−|w|

n
E[ΩX (w)]
|w|

and this in combination with Lemma 1 gives us

We use X and Y to abbreviate
and Y
respectively.
Using (5), we will compute H(Y ) and H(Y |X) and use
I(X; Y ) = H(Y ) − H(Y |X) to prove the theorem. We
ﬁrst compute H(Y ). Observe that, from (5) P (Y = w) =
n−|w|
(1 − d)|w| which leads to
x∈An P (X = x)Ωx (w)d
H(Y ) = −

dn−|w| (1 − d)|w| log

S1 ≤

n
n
P (Y (X1 ) = w|X1 = xn ) = Ωxn (w)dn−|w| (1 − d)|w| . (5)
1
1
n
X1

n
P n (w).
|w|

M

P (Xi1 = w1 , Xi2 = w2 , ..., Xim = wm )

zm log

n
|w| i1 <···<im

m=1

2

M
m=1 zm
M
m=1 am

M

≤

zm log
m=1

zm
.
am

(10)

n
Lemma 6. If X1 is a memoryless binary sequence with
parameter p, then S2 ∼ n · (H(1 − d) − (1 − d)H(p)) as
n → ∞.

Proof: Apply the inequality log x ≤ x − 1.
Lemma 5. Let X1 X2 . . . be a memoryless random binary
sequence. Then

Proof: By Theorem 1 and Lemma 1, and by the trivial
observation |w|=m P (w) = 1, we have

n+k
n+k
n
n
k
k
I(X1 ; Y (X1 )) ≤ I(X1 ; Y (X1 )) + I(X1 ; Y (X1 )).
n
Proof: We abbreviate ΩX1 (w1 ) by α(w1 ) and
ΩX n+k (w2 ) by β(w2 ). Applying (9) and (10) we obtain

1

1

=

1

α(w1 )β(w2 )
E [α(w1 )β(w2 )]
w1 w2 =w

w1 w2 =w

≤

α(w1 )β(w2 ) log
w1 w2 =w

=
w1 w2

n
P (w) log P (w)
|w|

w
n

α(w1 )β(w2 )
E [α(w1 )β(w2 )]

dn−m (1 − d)m

+

n

dn−m (1 − d)m
m=0

n
m

P (w) log P (w)
|w|=m

n

1

1

P (w) log P (w).
|w|=m

The second term above can be computed directly. By the
deﬁnition of the entropy we have |w|=m P (w) log P (w) =
−mH(p). Consequently,

dn+k−|w| (1 − d)|w| E ΩX n+k (w)log ΩX n+k (w)
w

.

n
mH(p) = −n(1 − d)H(p).
m

dn−m (1 − d)m

=−
m=0

1

1

In order to evaluate the ﬁrst term we apply the results of [6],
[8] about the so called binomial sums. Notice that

Hence by taking expectations of (11) and using the relation
n
dn−|w1 | (1 − d)|w1 | E[ΩX1 (w1 )]

n

dn−m (1 − d)m

w1

dn−|w| (1 − d)|w|

n
m

m=0

where the last equality follows holds as α(w1 ) and β(w2 )
n
n
are independent. Let now cn = I(X1 ; Y (X1 )). Then, by
Theorem 1

−E ΩX n+k (w) log E ΩX n+k (w)

n
n
log
m
m

m=0
n

α(w1 )
β(w2 )
α(w1 )β(w2 ) log
+ log
E [α(w1 )]
E [β(w2 )]
=w

cn+k =

dn−m (1 − d)m

=

(11)

w

dn−|w| (1 − d)|w|

+

1

w1 w2 =w

α(w1 )β(w2 ) log

=

n
n
P (w) log
|w|
|w|

w

ΩX n+k (w) log ΩX n+k (w) − ΩX n+k (w) log E ΩX n+k (w)

1=

dn−|w| (1 − d)|w|

S2 =

n+1

n
P n (w) =
|w|

n

dn− (1 − d)

n

m=0

n
n
log
m
m

∼ nH(1 − d).

This completes the proof of the lemma.
The next step is to show a similar property for S1 , namely
that S1 ∼ n · λ(d, p), where λ(d, p) is a non-negative constant.
The problem is to obtain some information about λ(d, p), but
for this we would need precise information about the behavior
of ΩX (w).

=0

(and a similar relation for the sum over w2 ) we immediately
derive cn+k ≤ cn + ck . Note that we have used the property
n+k
n+k
n
that X1 and Xn+1 are independent and that Xn+1 has the
k
same distribution as X1 .
By Fekete’s lemma [13] the following corollary follows.

Lemma 7. Suppose that X1 X2 . . . is a binary memoryless
n
n
sequence and an = S1 (X1 , Y (X1 )). Then an+k ≥ an + ak .

1
n
n
Corollary 1. I(d, p) = inf n≥1 n I(X1 ; Y (X1 )).
1
n
n
In particular, I(d, p) ≤ n I(X1 ; Y (X1 )) for all n ≥ 1. If
we apply this for n = 1, 2 we ﬁnd

Proof: We have

I(d, p) ≤ (1 − d)H(p), and

ΩX n+k (w) log ΩX n+k (w) =
1

I(d, p) ≤ d(1 − d)(H(p) + p2 + q 2 − 1) + (1 − d)2 H(p),

n
ΩX1 (w1 )ΩX n+k (w2 )

1

n+1

w1 w2 =w

where q = 1−p. For example, by looking at the second bound
we observe that sup0≤p≤1 I(d, p) ≤ 1−d + (1 − d)2 which
2
implies that memoryless input distributions do not meet the
general upper bound 1−d when d → 1. Actually we will show
that sup0≤p≤1 I(d, p) is much smaller as d → 1 (Theorem 3).
We now prove Theorem 2. As above, we write
n
n
I(X1 ; Y (X1 )) = S1 − S2 . Also, given two sequences an
and bn , an ∼ bn if an /bn → 1 as n → ∞.

× log

n ˜
ΩX1 (w1 )ΩX n+k (w2 )
˜
n+1

w1 w2 =w
˜ ˜

≥

n
n
ΩX1 (w1 )ΩX n+k (w2 ) log ΩX1 (w1 )ΩX n+k (w2 )
n+1

n+1

w1 w2 =w

=

n
n
ΩX1 (w1 )ΩX n+k (w2 ) log ΩX1 (w1 )
n+1

w1 w2 =w

+

n
ΩX1 (w1 )ΩX n+k (w2 ) log ΩX n+k (w2 )
n+1

w1 w2 =w

3

n+1

is completely symmetric if w = 1. Hence the contribution of
words of length 1 to I(X n ; Y (X n )) is

and consequently
dn+k−|w| (1 − d)|w| E ΩX n+k (w)log ΩX n+k (w)

an+k =

1

n

1

w
n+k−|w1 |−|w2 |

≥

d

T1 :=dn−1 (1 − d)

|w1 |+|w2 |

(1 − d)

m=1

n
n
× E ΩX1 (w1 )ΩX n+k (w2 ) log ΩX1 (w1 )

pm q n−m +pn−m q m

where q = 1 − p. By using the inequality

n+1

n+k−|w1 |−|w2 |

d

|w1 |+|w2 |

(1 − d)

log m = log(np) + log 1 +

w w1 w2 =w
n
× E ΩX1 (w1 )ΩX n+k (w2 ) log ΩX n+k (w2 ) .
n+1

n

m log m
m=1

Lemma 8. If X =
is a binary memoryless sequence, then
there exists a non-negative constant λ(d, p) ≤ H(1 − d) such
that S1 ∼ n · λ(d, p) as n → ∞.

w

m log np +
m=1

= log(np)np +

Proof: Since ΩX (w) is a non-negative integer we cern
tainly have S1 ≥ 0. Furthermore, since ΩX (w) ≤ |w| it
follows (as in the proof of Lemma 6) that
n
|w|

≤ log(np) +

m − np
np

n m n−m
p q
m

n

≤

n
X1

dn−|w| (1−d)|w| E[ΩX (w)] log

m − np
np

we obtain that

n+1

Hence, as in Lemma 5, we obtain that an+k ≥ ak + an .
The superadditivity property of Lemma 7 provides the
following convergence result.

S1 ≤

n
m

− dn−1 (1 − d) (np log(np) + nq log(nq))

w w1 w2 =w

+

mlog m

m − np
np

n m n−m
p q
m

npq
= np log(np) + q.
np

Putting all parts together we obtain that T1 ≤ dn−1 (1 − d) ≤
(1 − d).
Let T2 denote the subsum of (2) corresponding to those
terms with |w| ≥ 2. By using the trivial estimate ΩX (w) ≤
n
|w| and taking absolute values we obtain the upper bound

∼ nH(1−d).

n

n
n
Hence (using the notation an = S1 (X1 , Y (X1 )))
an
≤ H(1 − d).
0 ≤ λ(d, p) := sup
n≥1 n

dn− (1 − d)

T2 ≤ 2
=2
n

dn− (1 − d)

≤2

By another application of Fekete’s lemma [13] the sequence
an /n has a limit that equals the supremum sup(an /n). We
have used the property an+k ≥ an + ak here.
The proof of Theorem 2 is a combination of Lemma 6 and
Lemma 8. The lower bound on λ(d, p) follows from the fact
that I(d, p) ≥ 0.

=2

= 2dn log n
≥2

n

log

n

n
log n
!

n(1 − d)
d

1
( − 1)!

n(1 − d) n(1−d)/d
e
−1 .
≤ 2dn log n
d
If n(1 − d) = o(1) this leads to T2 ≤ C1 n2 (1 − d)2 log n
for some absolute constant C1 > 0. Summing up and using
Corollary 1, we obtain that

Remark (Extension to Mixing Sources): Most results of this
section hold for more general distributions. For example, from
the proof of Lemma 6 we conclude that
¯
S2 ∼ n · H(1 − d) − (1 − d)H(P )

I(d, p) ≤

¯
¯
where P is the limit of Pn which was deﬁned in Lemma 1
n
(provided the limit exists). A distribution P (X1 ) is said to
correspond to a strongly mixing source [13] if for all m ≤ n,
there exist constants c1 , c2 such that

1
1−d
n
n
I(X1 ; Y (X1 )) ≤
+ C1 n(1 − d)2 log n.
n
n

Finally by choosing n = (1 − d)−1/3 we derive the upper
bound
1
I(d, p) ≤ K (1 − d)4/3 log
1−d
for an absolute constant K > 0.

m
n
n
m
n
c1 P (X1 )P (Xm+1 ) ≤ P (X1 ) ≤ c2 P (X1 )P (Xm+1 ).

C. Lower Bound for d → 0

For such distributions, Lemma 7 generalizes to an+k ≥ an +
ak + K1 for some constant K1 , hence Lemma 8 holds as well.

Finally, we comment on the case d → 0 that has been
already solved in [10] and [9] where it is shown that
I(d, 0.5) = 1 + d log d − Ad + O(d2−ε ) as d → 0 and
C(d) = I(d, 0.5)+O(d3/2−ε ). The approach presented in [10]
is quite different from ours. However, we can use our methods
to obtain corresponding bounds. In particular, we easily obtain
the following lower bound for I(d, p).

B. Proof of Theorem 3: d → 1
We consider the expression in (2). We ﬁrst note that the
empty word does not contribute to the sum (2). Next we
n
consider words of length 1. If w = 0 and if X = X1 consists
of m zeroes and n − m ones then ΩX (w) = m. The situation

4

Now we choose n = d−ε which ensures that (1−d)n−1 =
1 + O(d1−ε ). From the deﬁnition of λ(d, p) and (3), this
implies that

Theorem 4. As d → 0,
I(d, p) ≥(1 − d)H(p) + d log d − d log(e)
+ d(q 2 f (p) + p2 f (p)) + O d2−ε

(12)

λ(d, p) ≥ d(q 2 f (p) + p2 f (q)) + O(d2−ε ).

for every ε > 0, where f (x) denotes the function f (x) =
log and q = 1 − p. Furthermore, as d → 0,
≥2 x
I(d, p) ≤ H(p) + d log d + O(d log log(1/d)).

Since H(1 − d) = −d log d − (1 − d) log(1 − d) = −d log d +
d log(e) + O(d2 ) we obtain the lower bound (12).
For the upper bound we proceed as in the proof of Theorem 3. We start with S1 . Let S1,n−1 denote the subsum of S1
corresponding to words of length n − 1. Then it follows from
the above calculations that S1,n−1 = O(nd) (actually we can
be much more precise). Furthermore, it follows as in the proof
of Theorem 3 that S1 − S1,n−1 = O log n d2 n2 if dn → 0.
Finally, for S2 we have (see Lemma 6)

(13)

Proof: The lower bound for I(d, p) follows from ideas
similar to those in the proof of Theorem 2. Instead of taking
the limit of an /n deﬁned in Lemma 7 we derive lower bounds
for an /n for certain n. We will only consider words w with
|w| = n − 1. Then
an ≥ d(1 − d)n−1

E[ΩX (w) log ΩX (w)].

S2 = −n(1 − d)H(p) + d(1 − d)n−1 n log n + O log n d2 n2 .

|w|=n−1

Consequently, we obtain for n = n(d) = d−1 / log d−1

Suppose for the moment that w has the form w =
0i1 1j1 0i2 1j2 · · · 0iK 1jK , where ir , jr ≥ 1; this means that
w1 = 0 and wn−1 = 1 (the other cases can be handled in
completely the same way). If |w| = n − 1, then we have
ΩX (w) = (for some > 2) if and only if there exists r with
ir = − 1

jr−1 ir +1 jr

ir jr +1 ir+1

iK jK

= H(p) + d log d + O(d log log(1/d)).

iK jK

i1 j1

and

i1 j 1

S1 − S2
n
= (1 − d)H(p)−(1 − d)n−1 d log n + O(d) + O log n d2 n

I(d, p) ≤

X = 0 1 ···1

1 ···0 1

0

This completes the proof of the theorem.

or there exists r with
jr = − 1

ACKNOWLEDGMENT

and X = 0 1 · · · 0 1

0

···0 1

M. Drmota was supported in part by the Austrian Science Foundation FWF Grant No. S9604. W. Szpankowski was supported in part by
NSF Science and Technology Center on Science of Information Grant
CCF-0939370, NSF Grants DMS-0800568, CCF-0830140, AFOSR
Grant FA8655-11-1-3076, NSA Grant H98230-11-1-0141.

.

Hence, by expanding E[ΩX (w) log ΩX (w)],
E[ΩX (w) log ΩX (w)]
|w|=n−1

=

R EFERENCES
log

≥2

P (w)
|w|=n−1

(pI[ir (w)=

−1] +qI[jr (w)= −1] ),

[1] J. Bourdon, and B. Vall´ e, “Generalized pattern matching statistics,”
e
Mathematics and Computer Science II, Trends. Math., 249–26, 2002.
[2] M. Dalai, “A new bound for the capacity of the deletion channel with
high deletion probabilities,” arXiv:1004.0400.
[3] S. Diggavi and M. Grossglauser, “Information transmission over ﬁnite
buffer channels,” IEEE Trans. Info. Th., 52, 1226-1237, 2006.
[4] R.L. Dobrushin, “Shannon’s theorem for channles with synchronization
errors.” Prob. Info. Trans., 18-36, 1967.
[5] A. Iyengar, P. Siegel, J. Wolf, “Modeling and information rates for
synchronization error channels,” arXiv:1106.0070.
[6] P. Flajolet, “Singularity analysis and asymptotics of Bernoulli sums,”
Theoretical Computer Science, 215, 371–381, 1999.
[7] P. Flajolet, W. Szpankowski, and B. Vall´ e, “Hidden word statistics,” J.
e
ACM, 53(1), 147–183, 2006.
[8] P. Jacquet and W. Szpankowski, “Entropy computations via analytic
depoissonization,” IEEE Trans. Info. Th., 45, 1072-1081, 1999.
[9] A. Kalai, M. Mitzenmacher, and M. Sudan, “Tight asymptotic bounds
for the deletion channel with small deletion probabilities,” ISIT, Austin,
2010.
[10] Y. Kanoria and A. Montanari, “On the deletion channel with small
deletion probability,” ISIT, Austin, 2010; see arXiv:1104.5546 for an
extension.
[11] M. Mitzenmacher, “A survey of results for deletion channels and related
synchronization channels,” Probab. Surveys, 1-33, 2009.
[12] M. Mitzenmacher and E. Drinea, “A simple lower bound for the capacity
of the deletion channel,” IEEE Trans. Info. Th., 4657-4660, 2006.
[13] W. Szpankowski, “Average case analysis of algorithms on sequences,”
Wiley, New York, 2001.
[14] R. Venkataramanan, S. Tatikonda, and K. Ramchandran, “Achievable
rates for channels with deletions and insertions,” ISIT, St. Petersburg,
Russia, 2011.

r≥1

where ir (w) denotes the length of the r-th 0-run in w and
jr (w) the length of the r-th 1-run in w. Now let Z be a
new random variable deﬁned on words w of length n − 1 as
Z = Z(w) = r≥1 (pI[ir (w)= −1] + qI[jr (w)= −1] ). Then we
just have to compute the expected value
(pP[ir = − 1] + qP[jr = − 1]).

E[Z] =
r≥1

Recall that the expected value E I[ir = −1] = P[ir = − 1]
has to be computed according the probability distribution of
word W (of length n − 1).
Next, note that the probability distribution of the lengthk 0-run is given by pk q/(1 − q) = pk−1 q and that the
number of runs in a string of length n is approximately pqn.
Consequently
E[Z] ∼ npq pp

−2

q + qq

−2

p

and ﬁnally
E[ΩX (w) log ΩX (w)] ∼ n
|w|=n−1

log

p q 2 + q p2

≥2

= n q 2 f (p) + p2 f (q) .

5


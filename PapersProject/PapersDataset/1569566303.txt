Creator:         XeTeX output 2012.05.18:1804
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 09:10:30 2012
ModDate:        Tue Jun 19 12:54:35 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      351659 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569566303

Finding the Capacity of a
Quantized Binary-Input DMC
Brian M. Kurkoski

Hideki Yagi

School of Information Science
Japan Advanced Institute of Science and Technology
Nomi, Japan 923-1292
kurkoski@jaist.ac.jp

Dept. of Information and Communication Engineering
University of Electro-Communications
Tokyo, Japan 182-8585
h.yagi@uec.ac.jp

This paper makes the restriction that J = 2. Since J = 2,
p1 + p2 = 1, and in the sequel the input distribution is p = p1 .
The mutual information between X and Y is:
Pm|j
I(X; Y) =
pj Pm|j log
.
(2)
j pj Pm|j
j

Abstract—Consider a binary-input, M -output discrete memoryless channel (DMC) where the outputs are quantized to K
levels, with K < M . The subject of this paper is the maximization
of mutual information between the input and quantizer output,
over both the input distribution and channel quantizer. This
can be regarded as ﬁnding the capacity of a quantized DMC.
An algorithm is given, which either ﬁnds the optimal input
distribution and corresponding quantizer, or declares a failure.

k

The sum m , etc. means the sum over the whole alphabet
M
m=1 , etc. Mutual information I(X; Y) is convex (lower
convex) in Pm|j , for ﬁxed pj . Similarly, it is concave (upper
convex) in pj for ﬁxed Pm|j [1, Theorem 2.7.4].
It is well-known that the channel capacity is:

I. I NTRODUCTION
Consider a binary-input, M -output discrete memoryless
channel (DMC) where the outputs are quantized to K levels,
with K < M . The subject of this paper is the maximization
of mutual information between the input and quantizer output,
over both the input distribution and channel quantizer. This
can be regarded as ﬁnding the capacity of a quantized DMC.
An algorithm is given, which either ﬁnds the optimal input
distribution and corresponding quantizer, or declares a failure.
Concretely, let the DMC input be X, let the DMC output
be Y, and let the quantized output be Z. The alphabet sizes
of X, Y and Z are J, M and K, respectively. Here, K < M
is of interest, since K ≥ M implies no reduction in mutual
information due to quantization.
Let C denote the quantizer function, that is:
C : {1, 2, . . . , M } → {1, 2, . . . , K}.

max I(X; Y)
p

and clearly for any ﬁxed quantizer C the capacity of the quantized channel is maxp I(X; Z). Furthermore, the celebrated
Arimoto-Blahut algorithm [2] [3] ﬁnds the capacity-achieving
input distribution:
p∗

=

(1)

(4)

(5)

which will be denoted as a function Q UANT: C ∗ =
Q UANT(P, p).
The objective in this paper is to ﬁnd the jointly optimal input
distribution p∗ and channel quantizer C ∗ which maximizes the
mutual information:

= Pr(Y = m|X = j),

= Pr(Z = k|X = j) =

p

C

and denote the input-to-quantizer output transition probability
matrix as T , with elements:
Tk|j

arg max I(X; Z).

C ∗ = arg max I(X; Z)

Pr(X = j),

denote the channel probability transition probability matrix as
P , with elements:
Pm|j

=

Since P and C uniquely determine T , the capacity-achieving
input distribution of the quantized channel will be denoted as
a function C APACITY: p∗ = C APACITY(P, C).
On the other hand, for any ﬁxed p, it is possible [4]
[5] to ﬁnd the quantizer C ∗ which maximizes the mutual
information,

Let C −1 (k) denote the subset of {1, . . . , M } that maps to
channel quantizer k. Denote the channel input distribution as:
pj

(3)

max I(X; Z).
p,C

(6)

This expression is regarded as the capacity of a quantized
DMC. The algorithm described in this paper either ﬁnds a
jointly optimal p∗ and C ∗ , or it declares a failure.
The rest of this paper is outlined as follows. Section II
describes previous work on the capacity of quantized channels
and summarizes the contribution of this paper. Section III

Pm|j .
m∈C −1 (k)

B.K. was supported in part by the Ministry of Education, Science, Sports
and Culture; Grant-in-Aid for Scientiﬁc Research (C) number 23560439.
H.Y. was supported in part by the Ministry of Education, Science, Sports
and Culture; Grant-in-Aid for Young Scientists (B) number 22760270.

1

describes two key concepts: partial mutual information, a
partial sum of mutual information; and a certiﬁcate, the range
of input distributions over which a particular quantizer is
known to be optimal. Section IV describes the main algorithm
which either gives the quantizer and channel input distribution
which maximizes mutual information, or declares a failure.
Section V gives some numerical results that illustrate the
algorithm. Section VI is the conclusion.

restricted to binary-input channels. So joint optimization of
the input distribution and the quantizer is a convex-concave
optimization problem, and provably optimal methods remain
elusive.
The contribution of this paper is an algorithm which ﬁnds
the jointly optimal input distribution and channel quantizer,
for a given binary-input DMC, or declares a failure. The basic
approach is to augment the quantization algorithm for a ﬁxed
input distribution [4], by adding a “certiﬁcate” property. The
certiﬁcate is a range of input distributions over which the
channel quantizer (or a partial quantization of the channel)
is known to be optimal. The algorithm can be seen fom
a dynamic programming perspective. Dynamic programming
decompositions are an effective way to show the optimality
of algorithms. Distinct from previous work, this approach can
ﬁnd the capacity of arbitrary channels.

II. P REVIOUS W ORK AND C ONTRIBUTION
The importance of designing channel quantizers has long
been recognized as a topic of interest in information theory
with practical applications. In the 1960s, Wozencraft and
Kennedy suggested using the cut-off rate as a criteria for quantizer optimization [6], and design algorithms for both binaryinput channels and non-binary inputs channels were described
around that time [7] [8]. But the ﬁrst known reference to using
mutual information to design channel quantizers came in 2002
[9]. An important application is the design of analog-to-digital
converters for communication receivers, and codes for such
systems.
Singh et al. considered the capacity of quantized channels,
but of continuous output channels, partticularly the AWGN
channel [10]. For a ﬁxed quantizer, optimal input distributions
can be found using a cutting-plane algorithm. Since certain
two-bit quantizers can be characterized by one parameter
(for symmetrical channels), joint optimization of the input
distribution and quantizer can be performed in a brute-force
manner. But for three-bit quantization, it was necessary to
resort to an optimization approach that involves alternating
between ﬁnding the capacity-achieving input and the optimal
quantizer, but this was not proved globally optimal.
By considering a DMC rather than a continuous output
channel, further progress can be made on this problem. For
a ﬁxed input distribution, there exists a polynomial-time algorithm which gives the quantizer which maximizes mutual
information [4] [5], for a binary input channel. When the
channel outputs satisfy:
log

P1|1
P2|1
PM |1
< log
< · · · < log
,
P1|2
P2|2
PM |2

III. PARTIAL M UTUAL I NFORMATION AND I TS
C ERTIFICATE
This section considers a partially quantized channel, by
developing the concepts of partial mutual information and
a certiﬁcate for a partially quantized channel. After these
preliminary concepts are established, the algorithm to compute
the DMC capacity is given in the following section.
A. Partial Mutual Information
The objective function in (6) is the mutual information
between X and Z:
I(X; Z)

=

pj Tk|j log
j

j

k

Tk|j
.
pj Tk|j

(8)

This can be written as:
pj
j

j

m∈C −1 (k)

k

m∈C −1 (k)

Pm|j log

pj

Pm|j

m∈C −1 (k)

Pm|j

.

A partial sum of mutual information is called partial mutual
information in this paper. Partial mutual information is a function of the input distribution p. The partial mutual information
ιk for output k and quantizer C is:

(7)

then for the optimal quantizer, each quantizer output consists
of a convex subset of channel outputs. This quantization
problem is an example of impurity partitions from machine
learning, where convex subsets are known to be optimal [11].
While restricted to binary-input DMCs, this approach ﬁnds the
optimal quantizer for otherwise arbitrary channels.
It is also worth noting that the optimal quantizer is known
to be deterministic. That is, for a continuous-output channel
there is no advantage to using dithered quantization [12]. And
for a DMC, probabilistic quantizers are suboptimal [4].
Thus, various optimization problems have been considered.
The channel capacity is a straightforward convex minimization problem (Arimoto-Blahut). Finding the optimal channel
quantizer is a concave minimization problem which is NP-hard
in general, but has polynomial complexity when attention is

ιk (p; C) =

pj
j

m∈C −1 (k)

Pm|j log
j

m∈C −1 (k)

pj

Pm|j

m∈C −1 (k)

Pm|j

For quantizer outputs 1 to k, quantized using C, the partial
mutual information is:
k

ι(p; C, k)

=

ιk (p; C).

(9)

k =1

So the total mutual information for some input distribution
p is the sum of all the partial mutual information terms
evaluated at p :
I(X; Z)

=

ι(p , C, K) =

ιk (p , C).
k

2

.

ι1 (p) and ι2 (p). The derivative of partial mutual information
is:
k

ι (p; C, k)

=
k =1

− (Tk |1 − Tk |2 )(

1
+ log2 f (p))
ln 2

+Tk |1 log2 Tk |1 − Tk |2 log2 Tk |2 ,

(15)

where f (p) = pTk |1 + (1 − p)Tk |2 .
1) Input: two partially-quantized channels with partial mutual information ι1 (p) and ι2 (p) with ι1 (p∗ ) > ι2 (p∗ )
1
2
2) Initialize: 1 = p∗ and r1 = p∗ .
2
2
3) For i = 1, 2, . . ., ﬁnd i+1 , the solution in p to:
ι2 ( i ) p −

ι2 (ri ) p − ri + ι2 (ri ) = ι1 (p)

for which ri+1 ≥ ri . Repeat until a sufﬁciently accurate
solution r is obtained.
5) Output: L = [ , r].
The operation of the algorithm is illustrated in Fig. 1. The
key point is that for any r, the line tangent to ι2 (p) at r is
greater than or equal to ι2 (p):

B. Certiﬁcate on Maximum Partial Mutual Information
Consider distinct quantizers C1 , C2 , . . ., Ci , . . . and some
ﬁxed k. The partial mutual information is: ι(p, Ci , k). The
input which maximizes mutual information can be found:
=

arg max ι(p, Ci , k).
p

(10)

=

p

ι(p, C1 , k) > ι(p, Ci , k) .

{p|ι2 (p)(p − r) + ι2 (r) ≤ ι1 (p)},

(11)

P2 ∩ P3 ∩ · · ·

D. Newton-Raphson Method
The Newton-Raphson method is an iterative technique for
ﬁnding a root of f (p) which has a derivative f (p). Beginning
with an initial value p1 , compute:
f (pi )
pi+1 = pi −
,
(18)
f (pi )
iteratively until a sufﬁciently accurate value is obtained.
Here, f (p) is the difference between the partial mutual
information function ι(p) and a line. Since ι(p) is strictly
convex, the equality f (p) = 0 has at most two solutions.
However, which of the two solution found by the NewtonRaphson method depends upon the initial value. For Step 3,
use an initial value of 0. For Step 4, use an initial value of 1.
To deal with this, the above iteration step is modiﬁed as:
f (pi )
q = pi −
,
(19)
f (pi )
and

 0 if q < 0
q if 0 ≤ q ≤ 1 .
pi+1 =
(20)

1 if q > 1

(12)

(13)

Since L is a line segment, it is sufﬁcient to represent L by the
two values and r:
L = [ , r] =

{p| ≤ p ≤ r}.

(17)

the inequality ι2 (p) ≤ ι1 (p) holds.

A certiﬁcate for C1 , denoted L, is a domain of ι(p; C1 , k)
for which C1 achieves the maximum partial mutual information over all other quantizations:
L ⊆

(16)

for 0 ≤ p ≤ 1, and equality at r = p. In the region where this
line is less than ι1 (p):

Let Pi , i = 2, 3, . . . be the domain of p for which C1 has
higher partial mutual information than Ci :
Pi

≥ ι2 (p),

ι2 (p)(p − r) + ι2 (r)

Without loss of generality, assume that the channels are
ordered such that:
ι(p∗ , C1 , k) ≥ ι(p∗ , C2 , k) ≥ · · ·
1
2

+ ι2 ( i ) = ι1 (p)

for which i+1 ≤ i . Repeat until a sufﬁciently accurate
solution is obtained.
4) For i = 1, 2, . . ., ﬁnd ri+1 , the solution in p to

Fig. 1: Finding the certiﬁcate for ι1 (p), the region where ι1 (p) ≥ ι2 (p).

p∗
i

i

(14)

C. Finding the Certiﬁcate
The following subsection gives an explicit method for
ﬁnding P2 , but can of course be applied for any Pi . Then
the certiﬁcate L is found as the intersection P2 ∩ P3 ∩ · · · .
The algorithm input is two partially quantized channels with
partial mutual information ι(p; C1 , k) and ι(p; C2 , k). Since
C1 , C2 and k are ﬁxed, from here, write ι(p; Ci , k) as ιi (p)
since C and k are ﬁxed. The corresponding derivatives are

Because f (x) is convex, this modiﬁcation does not change the
convergence.

3

IV. A LGORITHM TO C OMPUTE THE C APACITY OF A
Q UANTIZED DMC
This section describes an algorithm which computes the
capacity of the quantized DMC. A dynamic programming
approach is used. In this way, it is possible to show the
optimality of the algorithm. Note that the algorithm may fail,
but if it does produce a solution, it is an optimal solution.
A. Consideration of Optimality
In dynamic programming, a problem exhibits optimal substructure if the optimal solution contains optimal solutions
to subproblems [13]. The subproblem is as follows. For m
channel outputs quantized to k quantizer outputs (with m ≤ M
and k ≤ K and k ≤ m), ﬁnd Cm,k with certiﬁcate Lm,k (that
is, Cm,k is known to be optimal over input distributions in the
set Lm,k ). Assume that the optimal quantization:
Ck−1,k−1 , Ck,k−1 , . . . , Cn−1,k−1

Fig. 2: The relationship between subproblems for case of M = 5 and K = 3.

(21)

c) Find K, the certiﬁcate for n∗
d) The locally optimal quantizer is:

is known, and each has a corresponding certiﬁcate Lk−1,k−1 ,
Lk,k−1 , . . . , Ln−1,k−1 .
The solution to the subproblem, forming the iterative step
of the algorithm, is as follows. For some ﬁxed m and k,
and for some n < m, consider a candidate quantizer for
(n)
channel outputs 1 to m, denoted Cm,k . This can be formed
by combining the known-optimal quantizer Cn,k−1 , with the
quantization of channel outputs n+1 to m to the single output
k. The candidate quantizer is given by:
(n)

Cm,k (m ) =

Cn,k−1 (m )
k

(n∗)

Cm,k = Cm,k
and the certiﬁcate is:
Lm,k

=

p∗

if 1 ≤ m ≤ n
. (22)
if n < m ≤ m

Ln∗,k−1 ∩ K.

Ln∗,k−1 ∩ K.

e) If Lm,k = ∅ then declare a failure. Stop.
4) Outputs. The globally optimal quantizer C ∗ is CM,K .
The capacity-achieving input distribution is:
=

C APACITY(P, CM,K ).

The algorithm has polynomial complexity. In step 3, it
can be seen there are three “for each” loops which which
contribute M 3 operations. For each of these, it is necessary
to ﬁnd p∗ , which is also polynomial complexity. Note that
ﬁnding the certiﬁcate has complexity linear in M .
The relationship between the subproblems is illustrated in a
trellis-type diagram in Fig. 2, for M = 5 and K = 3. For any
node Cm,k , lines indicate those Cn,k−1 , n = k − 1, . . . , m − 1
which are used in step 3.

For each n, compute the input distribution which achieves the
maximum partial mutual information. Then select n∗ for the
(n∗)
quantizer Cm,k which has maximum mutual information (here
n∗ corresponds to 1 of the previous section) with certiﬁcate
(n∗)
K. Then, the optimal quantizer is Cm,k with certiﬁcate:
Lm,k

=

(23)

Note that if Lm,k = ∅, then a valid certiﬁcate cannot be found
with this method, and the algorithm declares a failure.

C. An Alternating Algorithm
An alternating algorithm is presented, which is based upon
the principles similar to Singh et al [10]. By alternating
between the DMC Quantization algorithm [4] (for a ﬁxed
p) and the Arimoto-Blahut algorithm (for a ﬁxed C), this
approach is straightforward:
1) Initialize with i = 1 and p1 = 0.5.
2) Ci = Q UANT(P, pi )
3) pi+1 = C APACITY(P, Ci )
4) If Ci = Ci−1 and i > 1 then stop. Output quantizer Ci
and distribution pi+1 .
5) i ← i + 1. Goto step 2.
This algorithm is considerably simpler, but it is not guaranteed to ﬁnd optimum p∗ and C ∗ . In particular, the capacity
maximization may ﬁnd an input distribution p which is locally
optimal for all possible quantizers, but is distinct from the
global optimal p∗ .

B. Algorithm
The algorithm to compute the capacity of the quantized
DMC is as follows.
1) Inputs:
• Binary-input discrete memoryless channel Pm|j . If
necessary, modify labels to satisfy (7).
• The number of quantizer outputs K.
2) Initialize C0,0 = ∅ and L0,0 = [0, 1]
3) For each k ∈ {1, . . . , K}, and for each m ∈ {k, . . . , k +
M − K}
a) for each n ∈ {k − 1, . . . , m − 1}:
(n)
• Find Cm,k according to (22).
(n)
∗
• Compute pn = C APACITY (P, Cm,k ).
(n)

b) Select n∗ = arg maxn ι(p∗ ; Cm,k , k)
n

4

0.55

M
4
5
6
7
8
10
12
16
32
64

Unquantized channel

Quantized Channel Capacity

0.54

M=32
M=8

0.53
M=5

0.52

0.51

M=4 DMC outputs

0.5

2
-

3
-

4

5

6

7

K
8

10

12

14

16

20

F

-

-

F

F

F
F

F
F

F
F

F
F

F
F

TABLE I: Test channels which failed are marked “F”, and “-” indicates an optimal solution
was provided.

0.49

0.48
2

3

4

5
6
K Quantizer Outputs

7

a quantizer with K outputs, the algorithm may ﬁnd the
input distribution and quantizer which maximizes the mutual
information. If it does not ﬁnd these, then it declares a failure.
Jointly maximizing mutual information in both the input
distribution and the quantizer is concave-convex optimization
problem; this class of problem is NP-hard in general. However,
by exploiting the properties of efﬁcient quantizers (described
in II), polynomial complexity is possible. The optimality of the
solution produced by the algorithm can be shown by dynamic
programming techniques.

8

Fig. 3: For a test channel, the capacity of the quantized channel for various DMC outputs
M and various quantizer outputs K.

V. N UMERICAL R ESULTS
To illustrate, the following test channel is used. A BPSK
channel with data-dependent noise is used, where Gaussian
noise with variance 4 is added to −1 and Gaussian noise with
variance 0.1 is added to +1. A DMC is formed by uniformly
quantizing this between −1 and +1 to M levels.

R EFERENCES
[1] T. M. Cover and J. A. Thomas, Elements of Information Theory. Wiley,
1991.
[2] S. Arimoto, “An algorithm for computing the capacity of arbitrary discrete memoryless channels,” IEEE Transactions on Information Theory,
vol. 18, pp. 14–20, January 1972.
[3] R. E. Blahut, “Computation of channel capacity and rate-distortion
functions,” IEEE Transactions on Information Theory, vol. 18, pp. 460–
473, July 1972.
[4] B. M. Kurkoski and H. Yagi, “Quantization of binary-input discrete memoryless channels, with applications to LDPC decoding.”
Submitted to IEEE Transactions on Information Theory. Available
http://arxiv.org/abs/1107.5637.
[5] B. Kurkoski and H. Yagi, “Concatenation of a discrete memoryless
channel and a quantizer,” in Proceedings of the IEEE Information Theory
Workshop, (Cairo, Egypt), pp. 160–164, January 2010.
[6] J. M. Wozencraft and R. S. Kennedy, “Modulation and demodulation for
probabilistic coding,” IEEE Transactions on Information Theory, vol. 12,
pp. 291–297, July 1966.
[7] J. L. Massey, “Coding and modulation in digital communications,” in
Proceedings of International Zurich Seminar on Digital Communications, (Zurich, Switzerland), pp. E2(1)–E2(4), 1974.
[8] L. Lee, “On optimal soft-decision demodulation,” IEEE Transactions on
Information Theory, vol. 22, pp. 437 – 444, July 1976.
[9] X. Ma, X. Zhang, H. Yu, and A. Kavcic, “Optimal quantization for softdecision decoding revisited,” in International Symposium on Information
Theory and its Applications, (Xian, China), October 2002.
[10] J. Singh, O. Dabeer, and U. Madhow, “On the limits of communication
with low-precision analog-to-digital conversion at the receiver,” IEEE
Transactions on Communications, vol. 57, pp. 3629–3639, December
2009.
[11] D. Burshtein, V. D. Pietra, D. Kanevsky, and A. Nadas, “Minimum
impurity partitions,” The Annals of Statistics, vol. 20, no. 3, pp. 1637–
1646, 1992.
[12] J. Singh, O. Dabeer, and U. Madhow, “Communication limits with lowprecision analog-to-digital conversion at the receiver,” in International
Conference on Communications, Circuits and Systems, IEEE, 2007.
[13] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction
to Algorithms, Second Edition. The MIT Press, 2001.

A. Quantized Channel Capacity for the Test Channel
The quantized channel capacity of the test channel is shown
in Fig. 3 for various values of M and K, with K < M .
The unquantized channel capacity is also shown. Generally,
a DMC with a larger number of outputs M has a greater
channel capacity, for ﬁxed K. Note an exception for M = 5
and M = 8, where the later has greater channel capacity. This
may be attributed to relatively coarse channel quantization,
where the boundaries for the M = 5 test channel are more
suitable for quantization to K = 2.
For this particular test channel, the alternating algorithm
of Subsec. IV-C produced the same quantizer and input
distribution.
B. Algorithm Failure
The algorithm is able to certify the output. That is, if an
output is produced it is known to be optimal. Otherwise, the
algorithm declares a failure. Table I lists various combinations
of M and K for the test channel. Cases where the algorithm
failed are marked “F” (and success is marked “-”). The algorithm is more likely to fail when attempting to resolve small
differences between competing quantizers. In these cases, the
new certiﬁcate is relatively short, and has no overlap with the
prior certiﬁcate, which may also be short. When there is no
intersection, the capacity-achieving input distribution cannot
be found.
VI. C ONCLUSION
This paper has presented an algorithm which computes
the capacity of quantized discrete memoryless channels. For

5


Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 14:44:16 2012
ModDate:        Tue Jun 19 12:56:39 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      476804 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565635

Channel Simulation via Interactive Communications
Mohammad Hossein Yassaee, Amin Gohari, Mohammad Reza Aref
Information Systems and Security Lab (ISSL),
Sharif University of Technology, Tehran, Iran,
E-mail: yassaee@ee.sharif.edu, {aminzadeh,aref}@sharif.edu.
Channel Simulator: q(y1 y2 |x1 x2 )
with r communication rounds

Abstract—In this paper, we study the problem of channel
simulation via interactive communication, known as the coordination capacity, in a two-terminal network. We assume that
two terminals observe i.i.d. copies of two random variables and
would like to generate i.i.d. copies of two other random variables
jointly distributed with the observed random variables. The
terminals are provided with two-way communication links, and
shared common randomness, all at limited rates. Two special
cases of this problem are the interactive function computation
studied by Ma and Ishwar, and the tradeoff curve between
one-way communication and shared randomness studied by
Cuff. The latter work had inspired Gohari and Anantharam to
study the general problem of channel simulation via interactive
communication stated above. However only inner and outer
bounds for the special case of no shared randomness were
obtained in their work. In this paper we settle this problem
by providing an exact computable characterization of the multiround problem. To show this we employ the technique of “output
statistics of random binning” that has been recently developed
by the authors.

C1
n
X1

n
X2

C2
C3

Enc1 /Dec1

Y1n

Enc2 /Dec2

Cr

Y2n

nR0 bits
Fig. 1. Channel simulator model: collective forward and backward rates
satisfy nR12 ≥ i:odd H(Ci ) and nR21 ≥ i:even H(Ci ) respectively.

and would like to generate i.i.d. copies of Y1 and Y2 . Instead
of a one-way communication, now the terminals are provided
with a two-way communication at rates R12 and R21 (see
Fig. 1). They can use up these two resources in r rounds of
interactive communications as they wish (i.e. we only impose
the constraint that i odd H(Ci ) is less than or equal to nR12
where H(Ci ) is the entropy of the message sent from terminal
1 to terminal 2 at round i; a similar statement holds for R21 ).
Inner and outer bounds on R12 and R21 were derived in
the special case of no shared common randomness [3]. In
this paper we completely solve this problem under both the
strong and empirical coordination models. Strong coordination
demands a total variation converging to zero. On the other
hand, empirical coordination only demands closeness of the
empirical distribution of the generated random variables and
the i.i.d. ones [4] (See Section II for a detailed description
of these two models). Our result relates to the literature of
coordinating distributed controllers to carry out some joint
action (see e.g. [4]) since the generated random variables
can be thought of as coordinated actions. Also, our result
relates to the problem of ﬁnding the communication cost
of simulating non-local correlations in quantum information
theory. See [10].
This paper is organized as follows: in the next subsection
we describe the main proof technique at an intuitive level. In
Section II we deﬁne the problem and in Section III we state
the main results followed by proofs in Sections IV and V.
Notation: In this paper, we use XS to denote (Xj : j ∈
S). We use pU to denote the uniform distribution over the
A
n
set A and p(xn ) to denote the i.i.d. pmf i=1 p(xi ), unless
otherwise stated. The total variation between two pmf’s p and
q on the same alphabet X , is denoted by p(x) − q(x) 1 .

I. I NTRODUCTION
The minimum amount of interaction needed to create dependent random variables is an operational way to quantify
the correlation among random variables. Wyner considered the
problem of remote reconstruction of two dependent random
variables by two terminals which are provided with shared
randomness at a limited rate [1]. He used this approach
to measure the intrinsic common randomness between two
random variables. An alternative characterization of Wyner’s
common information as an extreme point of a channel simulation problem was provided in [2]. In this setup, a terminal
who observes i.i.d. copies of X sends a message at rate R1 to
a remote random number generator (decoder) that produces
i.i.d. copies of another random variable Y that is jointly
distributed with X. The total variation distance between the
achieved joint distribution and the i.i.d. distribution induced
by passing X through a DMC channel p(y|x) should be
negligible. In other words, the generated distribution and
the i.i.d. distribution should be statistically indistinguishable.
Shared common randomness exists between the two parties at
a limited rate R0 . Cuff found the tradeoff between R0 and R1
showing that when R0 = 0 the minimum admissible rate for
R1 is the Wyner’s common information; and when R0 = ∞,
the minimum admissible rate for R1 is the mutual information
between X and Y (this special case was already shown in [?]).
This setup was generalized in [3] by assuming that two
terminals have access to i.i.d. copies of X1 and X2 respectively
This work was supported by Iran-NSF under grant No. 88114.46.

1

When a pmf itself is random, we use capital letter, e.g. PX .
In the intuitive discussion of the proofs we use PX ≈ QX
when E PX − QX 1 is small.

randomness in generating a random codebook is generally conceived of a common randomness shared among the terminals
in a problem. However, we are changing the order by ﬁrst
generating i.i.d. distributions and then treating B as a random
binning on this product i.i.d. space.

A. Description of the proof technique
Our proof is based on the technique of “output statistics of
random binning” (OSRB) that has been recently developed
in [9]. To explain the technique we begin by describing
the resolvability lemma used by Cuff [2, Lemma 6.1], and
originally proved by Wyner. We report this lemma in a slightly
different form that suits our purpose. Although we do not
use this lemma in this work, since it is very central to the
achievability proof of [2], we illustrate how this lemma can
be proved using the OSRB approach.
To discuss the resolvability lemma of [2, Lemma 6.1],
let us ﬁx some p(x, y). Roughly speaking the lemma
states that one can ﬁnd 2nR sequences in X n , namely
xn (1), xn (2), · · · , xn (2nR ), such that if we choose one of
these sequences at random and pass it through the DMC
channel p(y|x) we get an output sequence that is almost i.i.d.
according to p(y), as long as R > I(X; Y ). We can restate
this lemma by letting M to be a random variable whose
alphabet is M = [1 : 2nI(X;Y ) ], and assuming that X n (M )
is transmitted over the DMC channel q(y|x). To prove this
lemma in the traditional way one would construct a random
codebook parametrized by a random variable B. Every choice
of B = b corresponds to a particular codebook (particular set
of sequences in the X n space). The probability distribution
imposed on the Y n space depends on the value of B, which
is itself random. Therefore we use the capital letter PY n to
denote the random p.m.f. induced on Y n , by the random
codebook. To show the above lemma one would need to show
that the expected value of the total variation distance between
the probability measure y n → P (y n ) and the i.i.d. distribution
is small. Therefore there exists B = b where the total variation
distance is small. Indeed this is the way that Cuff proves this
lemma in [6, Lemma 19].
To illustrate the proof of this lemma using the OSRB
approach, one would need to start from n i.i.d. copies of X n
and Y n from the given p(x, y). Random variables B and M
˜
are identiﬁed as random binnings of X n at rates 2nR and 2nR
respectively. Note the conceptual change is in starting from the
i.i.d. distribution and then deﬁning B as a function of X n . It is
˜
proved that if R < H(X|Y ), B is almost independent of Y n .
Therefore, for almost any choice of B = b, the distribution
of Y n conditioned on B = b is almost i.i.d. On the other
˜
hand, if R + R > H(X), X n will be a function of (M, B)
with high probability by the Slepian-Wolf. We are interpreting
B and M as two messages coming from two encoders both
observing X n . These imply that one can ﬁnd B = b such that
the conditional law y n → p(y n |B = b) is close to the i.i.d.
distribution, and at the same time X n is almost a function
of M conditioned on B = b. All the approximations in this
intuitive argument can be made accurate.
The crucial departure from the traditional argument was
our treatment of random variable B. As discussed in [9], the

II. P ROBLEM S TATEMENT
Two terminals observe i.i.d. copies of sources X1 , X2
(taking values in ﬁnite sets X1 and X2 and having a joint
pmf q(x1 , x2 )) respectively. A random variable ω which is
n
n n
independent of X[1:2] = X1 X2 and is uniformly distributed
over [1 : 2nR0 ] represents the common randomness provided to
the terminals. Given an arbitrary r ∈ N, an (n, R0 , R12 , R21 )
channel simulation code for simulating a channel with r
interactive rounds of communications, consists of
• a set of r randomized encodings speciﬁed with the
conditional pmf’s penc1 (ci |c[1:i−1] xn ω) for odd numbers
˜
1
i ∈ [1 : r] and penc2 (ci |c[1:i−1] xn ω) for even numbers
˜
2
i ∈ [1 : r], where Ci denotes the communication of the
i-th round,
n
• two randomized decoders p 1 (y1 |c[1:r] x1 ω) and
˜dec n
dec2 n
n
p (y2 |c[1:r] x2 ω),
˜
such that
1
1
H(Ci ) ≤ R12 ,
H(Ci ) ≤ R21 .
n
n i:even
i:odd

Deﬁnition 1: Given a channel with transition probability
q(y[1:2] |x[1:2] ), a rate tuple (R0 , R12 , R21 ) is said to be achievable if there exists a sequence of (n, R0 , R12 , R21 ) channel
simulation codes, such that the total variation between the
probability p(y[1:2] , xn ) induced by the code and the i.i.d.
˜ n
[1:2]
repetitions of the desired pmf q(y[1:2] |x[1:2] )q(x[1:2] ) vanishes
as n goes to inﬁnity, that is
n

lim

n→∞

p(y[1:2] xn ) −
˜ n
[1:2]

q(y[1:2],i x[1:2],i )
i=1

= 0.

(1)

1

Deﬁnition 2: The simulation rate region is the closure of
all the achievable rate tuples (R0 , R12 , R21 ).
Remark 1: In the special case r = 1, Y1 = X2 = ∅, our
problem reduces to the one considered by Cuff in [2].
Remark 2: Observe that if Y1 = f1 (X[1:2] ) and Y2 =
f2 (X[1:2] ) are deterministic functions, the total variation constraint of eq. (1) reduces to
n
n
lim p Y1n = f1 (X[1:2] ), Y2n = f2 (X[1:2] ) = 1.
˜

n→∞

Thus our problem reduces to the problem of interactive
function computation considered in [5].
Deﬁnition 3 (Empirical coordination [4]): Assume that instead of simulating the channel q(y[1:2] |x[1:2] ), the demand is
to ﬁnd encoders and decoders such that the output sequences
n
n
Y[1:2] are jointly typical with the inputs X[1:2] , with high
probability. In this case, condition (1) should be replaced by
the following condition:
lim p
˜

n→∞

2

˜ n n
pX[1:2] Y[1:2] − qX[1:2] Y[1:2]

>
1

= 0,

(2)

˜ n n
where pX[1:2] Y[1:2] is the empirical distribution of the pair
n
n
(X[1:2] , Y[1:2] ) induced by the chosen code.
Remark 3: It can be shown that if a sequence of codes
satisﬁes the channel simulation condition (1), then it also
satisﬁes the empirical coordination constraint (2). On the other
hand it was shown in [4, Theorem 2] that the empirical rate
region does not depend on the amount of common randomness, that is, if (R0 , R12 , R21 ) is achievable for empirical
coordination, then (0, R12 , R21 ) is also achievable. These two
facts imply that the achievability of a pair of (R12 , R21 ) for
empirical coordination can be proved indirectly through the
achievability proof for channel simulation in the presence of
an unlimited common randomness. In [4], it was conjectured
that this relation is two-sided, i.e. the rate regions for empirical
coordination and channel simulation with unlimited common
randomness are equal.

Theorem 2 (Empirical coordination): The empirical coordination rate region is the set of all non-negative rate pairs
(R12 , R21 ), for which there exists p(f1 , · · · , fr , x[1:2] , y[1:2] ) ∈
T (r) (deﬁned in Theorem 1) such that

III. M AIN R ESULTS

Let (X[1:T ] , Y ) be a DMCS distributed according to a joint
pmf pX[1:T ] ,Y on ﬁnite sets. A distributed random binning
consists of a set of random mappings Bi : Xin → [1 : 2nRi ],
i ∈ [1 : T ], in which Bi maps each sequence of Xin uniformly
and independently to [1 : 2nRi ]. We denote the random varin
able Bt (Xt ) by Bt . A random distributed binning induces the
T
n
following random pmf on the set X[1:T ] ×Y n × t=1 [1 : 2nRt ],

R12 ≥ I(X1 ; F[1:r] |X2 ),
R21 ≥ I(X2 ; F[1:r] |X1 ).

The achievability comes from setting R0 = ∞ in Theorem 1.
See [10] for the converse.
Remark 5: Interactive empirical coordination is related to
the problem of interactive lossy source coding solved by Kaspi
in [8]. The above theorem in conjunction with [6, Theorem 9]
provides an alternative proof for that result.
IV. P ROOF OF THE ACHIEVABILITY
A. Review of the output statistics of random binning

Theorem 1: The simulation rate region is the set S(r) of all
non-negative rate tuples (R0 , R12 , R21 ), for which there exists
p(f1 , · · · , fr , x[1:2] , y[1:2] ) ∈ T (r) such that
R12 ≥ I(X1 ; F[1:r] |X2 ),
R21 ≥ I(X2 ; F[1:r] |X1 ),
R0 + R12 ≥ I(X1 ; F[1:r] |X2 ) + I(F1 ; Y[1:2] |X[1:2] ),

T

R0 + R12 + R21 ≥ I(X1 ; F[1:r] |X2 ) + I(X2 ; F[1:r] |X1 )
+ I(F[1:r] ; Y[1:2] |X[1:2] ),

P (xn ] , y n , b[1:T ] ) = p(xn ] , y n )
[1:T
[1:T

(3)

1{Bt (xn ) = bt }.
t
t=1

Theorem 3 ([9]): If for each S ⊆ [1 : T ], the following
constraint holds

where T (r) is the set of p(f1 , · · · , fr , x[1:2] , y[1:2] ) satisfying
X[1:2] , Y[1:2] ∼ q(x[1:2] )q(y[1:2] |x[1:2] ),

Rt < H(XS |Y ),

Fi −F[1:i−1] X1 − X2 , if i is odd,

(6)

t∈S

Fi −F[1:i−1] X2 − X1 , if i is even,

then as n goes to inﬁnity, we have

Y1 − F[1:r] X1 − X2 Y2 ,
Y2 − F[1:r] X2 − X1 Y1 .

(5)

T

(4)

n

n

pU nRt ] (bt )
[1:2

E P (y , b[1:T ] ) − p(y )

Use of Caratheodory Theorem makes the region computable.
Remark 4: Note that the above region is not symmetric
since we have a ﬁnite r rounds of communication and the
ﬁrst party starts the communication. The region will become
symmetric for inﬁnite rounds of communication (i.e. r → ∞).
See [10] for a discussion on the form of the region where we
highlight the role of the ﬁrst round of communication.
Corollary 1 (Interactive function computation [5]):
Assume that the desired channel is deterministic, that is,
Y1 = f1 (X[1:2] ) and Y2 = f2 (X[1:2] ). Setting R0 = 0 in
Theorem 1 gives the following full characterization of the
rate region of reliable interactive computation,

t=1

→ 0.

(7)

1

B. Outline of the achievability proof
In this section we use a combination of the Slepian-Wolf
theorem and Theorem 3 to illustrate the proof of Theorem 1;
see [10] for a rigorous proof.
The proof is divided into three parts. In the ﬁrst part
we introduce two protocols each of which induces a pmf
on a certain set of r.v.’s. The ﬁrst protocol has the desired
n
n
i.i.d. property on (X[1:2] , Y[1:2] ) but leads to no concrete
coding algorithm. However the second protocol is suitable for
construction of a code, with one exception: the second protocol
is assisted with an extra common randomness that does not
really exist in the model. In the second part we ﬁnd constraints
on R0 , R12 , R21 implying that these two induced distributions
are almost identical. In the third part of the proof, we eliminate
the extra common randomness given to the second protocol
without disturbing the pmf induced on the desired random
n
n
variables (X[1:2] , Y[1:2] ) signiﬁcantly. This makes the second
protocol useful for code construction.

R(r) = {∃F[1:r] :R12 ≥ I(X1 ; F[1:r] |X2 )
R21 ≥ I(X2 ; F[1:r] |X1 )
Fi − F[1:i−1] X1 − X2 , if i is odd,
Fi − F[1:i−1] X2 − X1 , if i is even,
H(Y2 |F[1:r] X2 ) = H(Y1 |F[1:r] X1 ) = 0}.

3

n
estimate of f[1:r] . The Slepian-Wolf decoding for the
rounds i ∈ [2 : r] is reliable, if

Part (1) of the proof: Take an arbitrary p(f[1:r] x[1:2] y[1:2] ) ∈
T (r). We deﬁne two protocols each of which induces a joint
distribution on random variables that are deﬁned during the
protocol.
n
n
n
Protocol A. Let (F[1:r] X[1:2] Y[1:2] ) be i.i.d. and distributed
according to p(f[1:r] x[1:2] y[1:2] ). Since p(f[1:r] x[1:2] y[1:2] ) ∈
T (r), it factors as

˜
∀i ∈ [2 : r] : Ri + Ri ≥ H(Fi |X(i+1)2 F[1:i−1] ). (10)

After the communications, terminal k = 1, 2 having
n
n
n n
(f[1:r] , xn ), generates yk according to p(yk |f[1:r] , xn ).
k
k
To simplify the discussion we assume that the Slepian-Wolf
r
decoders succeed with probability exactly one (and not close
n
n n
n n
p(xn )
p(fin |f[1:i−1] xn 2 ) p(y1 |f[1:r] xn )p(y2 |f[1:r] xn ). to one discussed in the rigorous proof in [10]).
1
2
(i)
[1:2]
ˆ
The random pmf induced by the protocol, denoted by P ,
i=1
Random Binning: Consider the following random binning: factors as
r
n
• To each sequence f1 , assign uniformly and independently
n
p(xn )pU (ω)pU (b[1:r] )
P (fin , ki |bi , ωi , f[1:i−1] xn 2 )
˜
nR1
nR1
[1:2]
(i)
three bin indices b1 ∈ [1 : 2
], k1 ∈ [1 : 2
] and
i=1
nR0
ω ∈ [1 : 2
],
n n
n n
(11)
× p(y1 |f[1:r] xn )p(y2 |f[1:r] xn )
n
n
2
1
• For 2 ≤ i ≤ r, to each sequence (f1 , · · · , fi ), assign
uniformly and independently two bin indices bi ∈ [1 :
Part (2) of the proof: Sufﬁcient conditions that make the
˜
2nRi ] and ki ∈ [1 : 2nRi ].
induced pmfs approximately the same: To ﬁnd the constraints
ˆ
The random 1 pmf induced by the random binning, denoted that imply that the pmf P is close to the pmf P in total
ˆ
variation distance, we start with P and make it close to P
by P , can be expressed as follows:
in a few steps. The ﬁrst step is to note that (bi , ωi ) can be
r
n
interpreted as a bin index of f[1:i] . Then substituting T = 1,
n
n
p(xn )
p(fin |f[1:i−1] xn 2 )P (bi , ki , ωi |f[1:i] )
[1:2]
(i)
X1 = F[1:i] and Y = X(i)2 F[1:i−1] in Theorem 3 yields that
i=1
if
n n
n
n n
n
n
× p(y1 |f[1:r] x1 )p(y2 |f[1:r] x2 ) = p(x[1:2] )
˜
R0 + R1 < H(F1 |X1 ),
r
(12)
n
n
n
n
n
˜
×
P (bi , ωi |f[1:i−1] x(i)2 )P (fi , ki |bi , ωi , f[1:i−1] x(i)2 )
Ri < H(Fi |X(i)2 F[1:i−1] ), for i = 2, · · · , r,
•

i=1
n n
n n
× p(y1 |f[1:r] xn )p(y2 |f[1:r] xn ),
1
2

n
n
then we have P (bi ωi xn 2 f[1:i−1] ) ≈ pU (ωi bi )p(xn 2 f[1:i−1] ).
(i)
(i)
In particular, we have

(8)

where (i)2 := i mod 2 and ω1 = ω, and ωi is a constant
variable for i ≥ 2.
Protocol B. In this protocol we assume that the terminals
have access to the shared randomness B[1:r] where B[1:r]
are mutually independent r.v.’s and uniformly distributed on
˜
r
nRt
]. R.v. ω is also used for the common randomt=1 [1 : 2
ness. Random variable Ki is used for the communication at
round i. Then, the protocol proceeds as follows,
n
• In the ﬁrst round, knowing (b1 , ω, x1 ), terminal 1 genn
n
erates a sequence f1 according to P (f1 |b1 , ω, xn ) of
1
n
protocol A, and sends the bin index k1 (f1 ) to the
terminal 2. At the end of the ﬁrst round, terminal 2 having
(b1 , ω, k1 , xn ), uses the Slepian-Wolf decoder to obtain
2
n
an estimate of f1 . The decoding is reliable if,
˜
R1 + R0 + R1 ≥ H(F1 |X2 ).
•

n
P (bi ωi |xn 2 f[1:i−1] ) ≈ pU (ωi , bi ) = pU (ωi )pU (bi ).
(i)

Substituting this in (8) gives us the desired equation
n
n
P (xn , f1:r , b[1:r] , k[1:r] , ω, y[1:2] ) ≈ p(xn )pU (ω)pU (b[1:r] )
[1:2]
[1:2]
r
n
n n
n n
P (fin , ki |bi , ωi , f[1:i−1] xn 2 ) p(y1 |f[1:r] xn )p(y2 |f[1:r] xn )
1
2
(i)
i=1
n
n
ˆ
= P (xn , f1:r , b[1:r] , k[1:r] , ω, y[1:2] ).
[1:2]

Using a simple lemma about total variation distance that
is provided in [6, Lemma 16], we can deduce the same
approximation as above over the marginals
n
n
ˆ
P (b[1:r] , xn , y[1:2] ) ≈ P (b[1:r] , xn , y[1:2] ).
[1:2]
[1:2]
n
n
In particular, the marginal pmf of (X[1:2] , Y[1:2] ) of the RHS of
n
n
this expression is equal to p(x[1:2] , y[1:2] ) which is the desired
pmf.
Part (3) of the proof: In the protocol we assumed that the
terminals have access to a shared randomness B[1:r] which is
not present in the model. To get rid of the shared randomness
B[1:r] , we would like to condition on a particular instance of
n
ˆ
B[1:r] = b[1:r] . In this case, the induced pmf P (xn , y[1:2] )
[1:2]
n
ˆ
changes to the conditional pmf P (xn , y[1:2] |b[1:r] ). But if
[1:2]
n
n
B[1:r] is independent of (X[1:2] , Y[1:2] ), then the conditional
n
ˆ
pmf P (xn , y[1:2] |b[1:r] ) is also close to the desired distri[1:2]
bution. To obtain the independence, we again use Theorem 3

(9)

n
Assume that the estimation of f1 at the end of the
ﬁrst round is correct. In the second round, knowing
n
n
(b2 , xn , f1 ), terminal 2 generates a sequence f2 accord2
n
ing to P (f2 |b2 , xn ) of protocol A and sends the bin
2
n
n
index k2 (f1 , f2 ) to the terminal 1. At the end of the
n
second round, terminal 1 having (b2 , k2 , xn , f1 ), uses
1
n
the Slepian-Wolf decoder to estimate f2 . This procedure
is repeated interactively for i ∈ [3 : r]. Thus, at the
end of the round r, both terminals obtain an accurate

1 The

pmf is random due to the random binning assignment in the protocol.

4

n
where (14) follows from the Markov chain Ci −ωC[1:i−1] X2 −
n
X1 for even number i. It can be shown that the above
term can be bounded from below by nI(F[1:r] ; X1 |X2 ). The
inequality R21 ≥ I(F[1:r] ; X2 |X1 ) can be proved similarly.
Next consider,

where we substitute T = r, Xi = F[1:i] and Y = X[1:2] Y[1:2]
to get the following sufﬁcient condition:
i

˜
Rt < H(F[1:i] |X[1:2] Y[1:2] ).

∀i ∈ [1 : r],

(13)

t=1
n
n(R12 + R0 ) ≥ H(ωC1 |X2 ) +

We have found all the necessary constraints on the size
of the bins for the protocol to work. Finally, eliminating
˜
˜
(R1 , · · · , Rr ) from the inequalities (9)-(13) gives rise to
the constraints given in the statement of the problem. This
completes the proof of the achievability of Theorem 1.

i:odd, i>1
n
n
n
≥ I(ωC1 ; Y[1:2] X1 |X2 ) +
i:odd, i>1

V. P ROOF OF THE CONVERSE

n
n
I(ωC1 ; Y[1:2] |X[1:2] )
n

q=1

q=1

< <

−
(b)

q=1

< <

n
Q−1
= nI(ωC1 X1,Q+1 X2 ; Y[1:2],Q |X[1:2],Q |Q) − 2ng( )
n
Q−1
= nI(QωC1 X1,Q+1 X2 ; Y[1:2],Q |X[1:2],Q )
− nI(Q; Y[1:2],Q |X[1:2],Q ) − 2ng( )
≥ nI(F1 ; Y[1:2] |X[1:2] ) − 3ng( ),

|X

1
,
4

where [∼ q] = [1 : n]\{q}.

B. Proof of converse

< .

Next we have
nR12 ≥

n
n
I(Ci ; X1 |C[1:i−1] X2 ω)

H(Ci ) ≥
i:odd
r

i:odd

n
n
I(Ci ; X1 |C[1:i−1] X2 ω)

=

|

[1] A. Wyner, “The Common Information of Two Dependent Random
Variables,” IEEE Tr. IT, 21 (2), pp. , 1975.
[2] P. Cuff, “Communication requirements for generating correlated random
variables,” in Proc. IEEE Int. Symp. IT, 2008, pp.1393-1397.
[3] A. Gohari and V. Anantharam, “Generating dependent random variables
over networks,” in Proc. IEEE IT. Workshop (ITW), 2011, pp.698-672.
[4] P. Cuff, H. Permuter and T. M. Cover, “Coordination capacity,” IEEE
Tr. IT, 56(9): 4181–4206, 2010.
[5] N. Ma and P. Ishwar, “Some results on distributed source coding for
interactive function computation,” IEEE Tr. IT, 57(9):6180–6195 , 2011.
[6] P. Cuff. “Communication in networks for coordinating behavior,” Ph.D
dissertation, Stanford Univ., CA. Jul. 2009.
[7] I. Csiszar and J. Korner, “Information theory: coding theorems for
discrete memoryless systems,” Akademiai Kiado, 1997.
[8] A. H. Kaspi, “Two-way source coding with a ﬁdelity criterion,” IEEE
Tr. IT, 31(6):735–740, 1985.
[9] M. H. Yassaee, M. R. Aref and A. Gohari, “Achievability proof
via output statistics of random binning,” to appear in ISIT 2012,
arXiv:1203.0730.
[10] M. H. Yassaee, A. Gohari and M. R. Aref, “Channel simulation via
interactive communications”, arXiv:1203.3217.

Take a random variable Q uniform on [1 : n] and
independent of all other random variables. Deﬁne Fi =
Q−1
n
ωCi X1,Q+1 X2 Q for 1 ≤ i ≤ r and Xi = XiQ , Yi = YiQ
for i = 1, 2. In the ﬁrst step of the proof, we show the Markov
chain conditions given in the deﬁnition of T (r) are satisﬁed
by this choice of auxiliary r.v.’s (see [10] for a rigorous proof).
Also, we observe that
1

||Y

(16)

where g( ) := 4 log( [1:2]2 [1:2] ), (a) is a result of Lemma
1, (b) follows from the Lemma 2 and the last inequality is a
result of [6, Lemma 21]. The proof for other inequalities is
similar and is omitted here. The last step is to let converge
to zero. See [10] for details.
R EFERENCES

1

p(x[1:2] , y[1:2] ) − q(x[1:2] , y[1:2] )
˜

n
q−1
I(ωC1 X1,q+1 X2 ; Y[1:2],q |X[1:2],q ) − 2ng( )
q=1

p(xq )ˆ(yq |xq )
p

then I(X[∼q] ; Yq |Xq ) ≤ 4 log

I(X[1:2],∼q ; Y[1:2],q |X[1:2],q ) − ng( )
q=1
n

≥

1

|Y|
2 ,

I(ωC1 X[1:2],∼q ; Y[1:2],q |X[1:2],q )
q=1
n

1
,
4

n

q=1

n
I(ωC1 ; Y[1,2],q |X[1:2] ) − ng( )

=

n

p(xq ) −

q=1

q=1
n

q−1
for some pq (w|z), we have
ˆ
|Z) ≤
q=1 I(Wq ; W
|W|
4n log 2 .
Lemma 2: Take an arbitrary i.i.d. sequence X n distributed
according to p(x) and a conditional pmf p(y n |xn ) which is
not necessarily i.i.d. If there exists a conditional pmf p(y|x)
ˆ
such that
n

q−1
n
I(Y[1:2] ; Y[1:2],q |X[1:2] )

n

≥

The following lemmas which are consequences of a generalized version of Lemma 2.7 of [7], will be useful throughout
the proof of the converse.
Lemma 1: For any discrete random variables W n and Z
whose joint pmf satisﬁes

p(y n |xn )

n
q−1
n
I(ωC1 Y[1:2] ; Y[1,2],q |X1:2 ) −

=
(a)

pq (wq |z)

(15)

where (15) follows from the same lines as used in the
bounding of R12 . Now, consider

A. Mutual information bounds

p(wn , z) − p(z)

n
n
I(Ci ; X1 |C[1:i−1] X2 ω)

n
n
n
n
≥I(ωC1 ; Y[1:2] |X[1:2] ) + I(C[1:r] ω; X1 |X2 )
n
n
≥ I(ωC1 ; Y[1:2] |X[1:2] ) + nI(F[1:r] ; X1 |X2 ),

Let (R0 , R12 , R21 ) be an achievable rate tuple for r rounds
of communications. Then, for any < 1 , there exists a sim2
ulation code of length n such that the total variation between
the induced pmf p(y[1:2] , xn ) and the n i.i.d. repetitions of
˜ n
[1:2]
the desired pmf q(x, y) is less than .

n

n
H(Ci |C[1:i−1] X2 ω)

(14)

i=1

5


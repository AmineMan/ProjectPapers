Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sat Feb  4 14:04:55 2012
ModDate:        Tue Jun 19 12:54:08 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      457058 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569567015

On The Sum-Capacity of Gaussian MAC With
Peak Constraint
Babak Mamandipoor, Kamyar Moshksar, and Amir K. Khandani
Department of Electrical and Computer Engineering
University of Waterloo, Waterloo, ON, Canada N2L 3G1
Emails: {bmamandi, kmoshksa, khandani}@uwaterloo.ca
Z ∼ N(0, 1)

Abstract—This paper addresses a two-user Gaussian Multiple
Access Channel (MAC) under peak constraints at the transmitters. It is shown that generating the code-books of both users
according to discrete distributions achieves the largest sum-rate
in the network. In other words, sum-capacity achieving input
distributions for this channel are discrete with a ﬁnite number
of mass points. We also demonstrate uniqueness of the input
distributions which achieve rates at any of the corner points of
the capacity region of the channel.

X1 : |X1 | ≤ A

Y = X1 + X2 + Z

X2 : |X2 | ≤ A

Fig. 1.

I. I NTRODUCTION

A Two-User Gaussian MAC under Peak Constraint

B. Contribution

A. Summary of Prior Art

In this paper, we consider a two-user Gaussian MAC under
peak constraints at both transmitters1 as shown in Fig. 1.
Denoting the signal transmitted by user i by Xi for i = 1, 2,
the signal received at the receiver is

In his seminal thesis, J. G. Smith studies a point to point
Additive White Gaussian Noise (AWGN) channel under peak
constraint, as well as average power, constraints [1]. The main
observation in [1], [2] is that the unique capacity achieving
distribution is discrete with a ﬁnite number of mass points.
We remark that characterizing the exact number and location
of mass point must be done numerically. In [3], the authors
extend the results in [1], [2] to a quadrature AWGN channel.
[4] provides an insightful methodology in dealing with the
problem considered in [1], [2]. Among other related works,
[5] investigates the entropy of a sum of independent random
variables with common support on a ﬁnite interval. It is shown
that the entropy is maximized if one of these random variables
is uniform and the rest are discrete with mass points at the
endpoints of the support interval. Subsequently, [5] draws
conclusions about the sum-rate of a Gaussian MAC under peak
amplitude constraints at the transmitters in the high Signal-toNoise Ratio (SNR) regime where the additive Gaussian noise
at the common receiver is neglected.

Y = X1 + X2 + Z,

(1)

where Z ∼ N (0, 1) is the additive noise at the common
receiver. Note that X1 , X2 and Z are independent random
variables. It is required that the transmitted signal by any user
be in the interval [−A, A]. We are looking for the optimal
∗
∗
choice X1 and X2 of X1 and X2 such that the sum-rate is
maximized, i.e.,
∗
∗
X1 , X2 = arg

sup

I(X1 , X2 ; Y ).

(2)

X1 ,X2 :|Xi |≤A,i=1,2

Note that (2) can be alternatively stated as
∗
∗
X1 , X2 = arg

sup

h(Y ),

(3)

X1 ,X2 :|Xi |≤A,i=1,2

where h(·) is the differential entropy function. The main
contribution of the paper is that selecting X1 and X2 to be
discrete random variables is an answer for the problem in (2).
In order to prove our result, we extend the approach taken in
[1], [2] using some key Theorems in mathematical analysis
together with the results in [4]. The main idea of the proof is
based on the following steps:

Discrete input distributions appear in many different scenarios. For example, [6] explores a fast Rayleigh fading point to
point channel where the channel state information is unknown
to both transmitter and receiver. Surprisingly, it is shown that
under only a constraint on average power at the transmitter
the capacity-achieving input distribution is discrete. In another
scenario, the authors in [7], [8] demonstrate optimality of
discrete input distributions in channels with quantized output.

1 Our results can be easily extended to a Gaussian MAC with any ﬁnite
number of users.

1

∗
1- We ﬁx X1 = X1 . Note that we do not know the distribution
∗
∗
of X1 at this point. As such, we represent X1 by a distribution
that has both continuous and discrete parts. Then we look into
the new problem

X2 = arg

sup
X2 :|X2 |≤A

∗
h(X1 + X2 + Z),

II. P ROOF OF T HEOREM 1
The answer to our optimization problem in Theorem 1 is
in fact the capacity of a point-to-point scalar additive noise
channel Y = X + W + Z where X and Y denote input and
output of the channel, respectively. Let PY (·; FX ), ΦY (·; FX )
and hY (FX ) denote the output pdf, the output characteristic
function and the output differential entropy, respectively, when
X is generated according to FX . Let us deﬁne the mixed noise
as

(4)

where we show that X2 is a discrete random variable. By (4),
∗
∗
∗
h(X1 + X2 + Z) ≤ h(X1 + X2 + Z).

(5)

M

2- We look into the problem
X1 = arg

sup

h(X1 + X2 + Z),

∞

where following the same lines as in (4), we show X1 is a
discrete random variable. By (6),

h(x; FX )

−∞

PM (y − x) log(PY (y; FX ))dy.

(10)

+A

hY (FX ) =

h(x; FX ) dFX (x).

(11)

−A

Following [2], we invoke the Karush-Kuhn-Tucker (KKT)
Theorem to obtain the necessary and sufﬁcient conditions for
the optimum input pdf. The requirements of the KKT Theorem
are convexity and compactness of F and strict convexity,
continuity and weak differentiability of hY (FX ).
We ﬁrst develop upper and lower bounds for PM (·) and
PY (·). Propositions 1 and 2 are devoted to establish strict
convexity and continuity of hY (FX ), respectively. Proposition
3 veriﬁes the differentiability of hY (FX ). Lemma 5 uses the
Morera Theorem [12] to prove that the continuation of PM (·)
to the complex plane is analytic. This in turn provides us
with necessary tools to demonstrate analyticity of h(·; FX ) in
Proposition 4. Analyticity of h(·; FX ) is used in Proposition
5 where we conclude the claim in Theorem 1 based on the
Identity Theorem [12] in Complex Analysis. Throughout the
paper, we occasionally refer the reader to the technical report
[13] where we have presented details that we can not provide
here due to space limitations.
It is straightforward to see that [13]

Based on steps 1 and 2 above, the rest of the paper is
devoted to prove the following Theorem:
Theorem 1- Let W be a ﬁxed random variable with
support [−A, A] and Z ∼ N (0, 1). Then a unique and
discrete random variable X with a ﬁnite number of mass
points in [−A, A] is the answer to the optimization problem
supX:|X|≤A I(X; X + W + Z).
Notation: The set of real and complex numbers are √
denoted
by R and C, respectively. The imaginary number −1 is
shown by j. For any random variable U , PU (·), FU (·) and
ΦU (·) denote the Probability Density Function (pdf), Cumulative Distribution Function (cdf) and characteristic function
of U , respectively. A normal random variable with mean m
and variance σ 2 is denoted by N (m, σ 2 ). The imaginary part
of a complex number z is denoted by Im(z). Rδ denotes a
portion of the complex plane deﬁned by
{z : |Im(z)| ≤ δ}.

−

It is easy to see that

(7)

Finally, by (5) and (7), we have h(X1 + X2 + Z) ≥
∗
∗
∗
∗
h(X1 +X2 +Z). However, h(X1 +X2 +Z) ≤ h(X1 +X2 +Z)
∗
∗
by (2). Hence, h(X1 + X2 + Z) = h(X1 + X2 + Z), i.e.,
the highest sum-rate is achieved by discrete input distributions.

Rδ

(9)

to represent the total noise of the channel whose entropy is
denoted by h(PM ). Let us deﬁne the marginal entropy function
h(x; FX ) by

(6)

X1 :|X1 |≤A

∗
h(X1 + X2 + Z) ≥ h(X1 + X2 + Z).

W + Z,

q(m) ≤ PM (m) ≤ Q(m), ∀m ∈ R,
where

(8)

2

q(m) =

F denotes the convex space of cdfs having all points of
increase in the ﬁnite interval [−A, A]. F is endowed a
topology based on the so-called Levy metric [11]. The Levy
distance between F1 , F2 ∈ F is denoted by d(F1 , F2 ). For
c
a sequence of cdfs {Fn }n≥1 , Fn → F denotes complete
−
convergence (convergence in the Levy metric) to a speciﬁc
distribution F . f (x+ ) and f (x− ) denote the limits of the
function f (·) when approaching to the real point x from right
and left, respectively. A function Ψ : F → R is called weakly
differentiable at F0 ∈ F if limθ→0+ Ψ(F0 +θ(F −F0 ))−Ψ(F0 )
θ
exists. Finally, Ψ : F → R is called weakly differentiable
if Ψ(·) is weakly differentiable at every point in F.

k1 e−k2 (m−A)
2
k1 e−k2 (m+A)


2
k3 e−k4 (m+A)

Q(m) = k3
 −k (m−A)2

k3 e 4

(12)

m≤0
,
m>0

(13)

m < −A
m ∈ [−A, A] .
m>A

(14)

and k1 , k2 , k3 and k4 are positive constants. Moreover [13],
γ(y) ≤ PY (y, FX ) ≤ Γ(y),
where

2

γ(y) =

2

k1 e−k2 (y−2A)
2
k1 e−k2 (y+2A)

y≤0
y>0

(15)

(16)

and


2
k3 e−k4 (y+2A)

Γ(y) = k3
 −k (y−2A)2

k3 e 4

y < −2A
y ∈ [−2A, 2A] .
y > 2A

notation, let denote the input X corresponding to distributions
F1 and F2 by X1 and X2 , respectively. Based on Lemma
2, if d(F1 , F2 ) = 0, then PY (y; F1 ) = PY (y; F2 ). To
show the other direction, let PY (y; F1 ) = PY (y; F2 ). Then
ΦY (f ; F1 ) = ΦY (f ; F2 ). We can rewrite this equation as

(17)

Lemma 1: PM (·) is a continuous function.
Proof: Let {xn }n≥1 be a sequence of real numbers
converging to x. We have

ΦM (f ) (ΦX1 (f ) − ΦX2 (f )) = 0.

(22)

Based on lemma 3, ΦM (·) cannot be uniformly zero in any
interval. Let us denote the isolated zeros (if any) of ΦM (·) by
PZ (xn − λ) dFW (λ)
lim PM (xn ) = lim
f1 , f2 , f3 , · · · . This implies that ΦX1 (f ) = ΦX2 (f ) for any
n→∞
n→∞ −A
f ∈ R except possibly for f ∈ {f1 , f2 , f3 , · · · }. According to
+∞
λ
+
−
rect
= lim
PZ (xn − λ) dFW (λ), lemma 4, ΦXi (f ) = ΦXi (f ) = ΦXi (f ) for i = 1, 2 and
n→∞ −∞
2A
any f ∈ {f1 , f2 , f3 , · · · }. Therefore, ΦX1 (f ) = ΦX2 (f ) for
(18) all f ∈ R. This in turn yields d(F1 , F2 ) = 0.
Remark 1: Let f : R → R be a positive-valued and
where function rect(·) is deﬁned by
bounded function, i.e., 0 ≤ f (y) ≤ c < ∞ for any y ∈ R
1 −1 ≤ u ≤ 1
and some positive constant c. It is easy to see that [13]
2
2
rect(u)
(19)
0 otherwise.
| log(f (y))| ≤ − log(f (y)) + 2| log c|.
(23)
A

λ
λ
Since Z ∼ N (0, 1), |rect( 2A )PZ (xn − λ)| ≤ √1 rect 2A
2π
∞
λ
1
where −∞ √2π rect 2A dFW (λ) < ∞. On the other hand,
λ
λ
limn→∞ rect( 2A )PZ (xn − λ) = rect( 2A )PZ (x − λ). Hence,
using Dominated Convergence Theorem [11], one can take
the limit inside the integral. The result is immediate, i.e.,
limn→∞ PM (xn ) = PM (x).
Lemma 2: PY (y; FX ) is continuous in both arguments.
Proof: The proof for continuity in y is straightforward
and quite similar to the lines of proof in Lemma 1. In order to
prove continuity in FX , let us ﬁx a sequence {Fn (x)}n≥1 in
c
F such that Fn → F for some F ∈ F. We know that PM (·)
−
is bounded and continuous2 . Note that

We use this fact in several instances throughout the paper.
Proposition 2: hY (FX ) is a continuous function in terms
of FX .
c
Proof: For any sequence {Fn }n≥1 in F with Fn → F ,
−
we need to show that limn→∞ hY (Fn ) = hY (F ). Based on
Lemma 2, limn→∞ PY (y; Fn ) = PY (y; F ). Then
lim −PY (y; Fn ) log(PY (y; Fn )) = −PY (y; F ) log(PY (y; F )).
(24)
On the other hand,

n→∞

≤ Γ(y) − log γ(y) + 2| log k3 | ,
(25)
where we have used (15) and Remark 1. It is easy to verify
that [13]
−PY (y; Fn ) log PY (y; Fn )

∞

PY (y; F ) =
−∞

PM (y − x) dF (x).

(20)

∞

As such,
∞

A

lim PY (y; Fn )

n→∞

=
(a)

lim

n→∞

−A

−A

PM (y − x) dF (x) = PY (y; F ),
(21)

where (a) follows by the Helly-Bray Theorem [11]. This
completes the proof.
Lemma 3: The characteristic function ΦM (·) can not be
uniformly zero in any interval, i.e., zeros of ΦM (·) are isolated.
Proof: See [13].
Lemma 4: Every characteristic function is uniformly continuous on the whole real line.
Proof: See [10].
Proposition 1: hY (FX ) is a strictly convex-cap function.
Proof: We know that hY (FX ) is a convex-cap function
of FX . To prove strict convexity, we need to show that
d(F1 , F2 ) = 0 if and only if PY (y; F1 ) = PY (y; F2 ) for
any two distributions F1 , F2 ∈ F for X. For simplicity of
2 See

(26)

By (24), (25) and (26), we can use Dominated Convergence
Theorem to conclude limn→∞ hY (Fn ) = hY (F ).
Proposition 3: hY (FX ) is weakly differentiable.
Proof: The proof follows similar lines of reasoning presented in [2] for an additive white Gaussian noise channel.
The details are offered in [13].
Lemma 5: Continuation of PM (·) to the complex plane is
analytic on any domain Rδ for an arbitrary δ > 0.
Proof: Note that PM (·) : C → C is deﬁned by

PM (y − x) dFn (x)

A

=

Γ(y) − log γ(y) + 2| log k3 | dy < ∞.

A

PM (z)
−A

PZ (z − λ) dFW (λ).

(27)

We ﬁrst show that continuation of PM (·) to the complex plane
is continuous3 in Rδ . Let {zn }n≥1 be a sequence of complex
numbers in Rδ converging to z ∈ Rδ . Letting zn = ηn + jζn
and noting that |ζn | ≤ δ,
2
2
1 δ2
1 −1
(28)
|PZ (zn −λ)| = √ e 2 ((ηn−λ) −ζn) ≤ √ e 2 .
2π
2π
3 Note that continuity of P (·) is the essential assumption in Morera
M
Theorem to prove analyticity of PM (·).

Lemma 1.

3

δ2

A

Let us write k ∗ =

As −A √1 e 2 dFW (λ) < ∞ and limn→∞ PZ (zn − λ) =
2π
PZ (z − λ) exists, using the Dominated Convergence theorem,
we have limn→∞ PM (zn ) = PM (z). This implies that PM (·)
is continuous for all z ∈ Rδ . In order to verify analyticity
of PM (·), using Morera Theorem, we need to show that
P (z)dz = 0 for any closed contour ω in Rδ . This contour
ω M
integral is described as

|PM (y − zn )| ≤

PM (z)dz =
−A

ω

PZ (z − λ)dFW (λ)dz.

(29)

A

PM (z)dz =
−A

ω

PZ (z − λ)dzdFW (λ) = 0,

(30)

where we have used the fact that ω PZ (z − λ)dz = 0 due to
analyticity of PZ (·). Invoking Morera Theorem, we conclude
that PM (·) is analytic in Rδ . To complete the proof, we need
to show that changing the order of integration in (29) is indeed
valid. This can be done by Fubini Theorem [11] as we have

−A

≤

−A

A

2π k ∗
−A

PZ (y − ηn − λ) dFW (λ). (36)

max PZ (y − ηn − ν).

ν∈[−A,A]

κ∗ (y, ε)
ω

PZ (z − λ) dz dFW (λ)

where (a) is by (28) and lω is the length of ω which is ﬁnite
as ω is a closed curve. This completes the proof.
Proposition 4: Marginal entropy function h(· ; FX ) is analytic on the domain Rδ for all FX ∈ F and any δ > 0.
Proof: We start by proving that h(· ; FX ) is continuous.
Let {zn }n≥1 be a sequence of complex numbers in Rδ
converging to z ∈ Rδ . We show that limn→∞ h(zn ; FX ) =
h(z; FX ). We have

(a)

≤ κ∗ (y, ε) − log γ(y) + 2| log k3 | ,

∞

A
ω

(33)

(a)

1
PZ (y − zn − λ) = √ e
2π
Since |ζn | ≤ δ,
|PZ (y − zn − λ)|

≤

− 1 (y−ηn −λ)2 +jζn (y−ηn −λ)
2

δ2
2

PZ (y − ηn − λ).

ω

−∞

PM (y − z) log(PY (y; FX )) dy dz

∞

(b)

log(PY (y; FX ))
−∞

ω

PM (y − z) dz dy = 0, (42)

where (a) is due to Fubini Theorem4 and (b) holds by Lemma
5 where it was shown that PM (·) is analytic. This completes
the Proof of Proposition 4.

. (34)

Lemma 6: Entropy of the mixed noise h(PM ) is ﬁnite.

2
1 δ2 1
√ e 2 e− 2 (y−ηn −λ)
2π

= e

h(z; FX ) dz = −

= −

Writing zn = ηn + jζn ,
2
ζn
2

(41)

where (a) follows by Remark 1. Let us deﬁne g(y)
κ∗ (y, ε) − log γ(y) + 2| log k3 | for y ∈ R. It is a straight∞
forward task to verify that −∞ g(y)dy < ∞. We refer the
reader to [13] for the details. This complete the claim about
continuity of h(· ; FX ). We are ready to apply Morera Theorem
to show that h(· ; FX ) is analytic in Rδ . Let ω be a closed
contour in Rδ . Then

Note that limn→∞ PM (y − zn ) = PM (y − z) for any y ∈ R
due to continuity of PM (·) as shown in Lemma 5. If we
can show that there exists a function g : R → [0, ∞) such
that |PM (y − zn ) log(PY (y; FX ))| ≤ g(y) for any y ∈ R
∞
and −∞ g(y)dy < ∞, then we can invoke Dominated
Convergence Theorem to conclude continuity of h(·; FX ). We
have

−A

y <η−A−ε
y ∈ [η − A − ε, η + A + ε] .
y > η + A + ε.
(40)

|PM (y − zn ) log PY (y; FX ) | ≤ κ∗ (y, ε)| log PY (y; FX ) |

PM (y − zn ) log(PY (y; FX ))dy. (32)

|PZ (y − zn − λ)| dFW (λ).

(39)

Therefore, |PM (y − zn )| ≤ κ∗ (y, ε) for n ≥ N (ε).
We can write

∞

|PM (y − zn )| ≤

sup κn (y).

It is easily seen that [13]

2
1
k ∗ e− 2 (y−η+A+ε)

κ∗ (y, ε) = k ∗
 ∗ − 1 (y−η−A−ε)2

k e 2

(a)

−∞

(38)

n≥N (ε)

1 δ2
PZ (z − λ) dz dFW (λ) ≤ √ e 2 lω < ∞, (31)
2π
ω

h(zn ; FX ) = −

y − ηn < −A
y − ηn ∈ [−A, A] .
y − ηn > A

(37)

By (36) and (37), |PM (y − zn )| ≤ κn (y) for any n ≥ 1. As
zn approaches z, ηn approaches η where z = η + jζ. Let us
ﬁx ε > 0. There exists a natural number N (ε) such that for
n ≥ N (ε), we have |η − ηn | < ε. Let us deﬁne κ∗ (y, ε) by

A

A

. By (33) and (35),

It is easy to see that

2
1
k ∗ e− 2 (y−ηn +A)

κn (y) = k ∗
 ∗ − 1 (y−η −A)2

n
k e 2

If one can change the order of integration in (29), then

ω

√

Let us deﬁne κn (y) as
√
κn (y)
2πk ∗

A
ω

2

δ
√1 e 2
2π

4 Justiﬁcation for applying Fubini Theorem is similar to that brought in the
proof of Proposition 4 and is omitted here. Please see [13] for details.

(35)

4

Proof: We can calculate an upper bound for h(PM ) as
h(PM ) ≤ −

where (a) holds by (12) and (15). We can make this upper
bound arbitrarily small by choosing x large enough. On the
other hand, for x > A + l,

PM (x) log PM (x) dx

+∞

(a)

≤

PM (x) log q(x) + 2| log k3 | dx

(b)

≤

Ω−

PM (y − x)ρ(y) dy ≤
≤

Q(x) log q(x) dx + 2| log k3 | < ∞,

(a)

sup I(X; Y ) = max I(X; Y ),
FX ∈F

FX ∈F

{y : ρ(y) ≥ 0}, Ω−

{y : ρ(y) < 0}.

PM (y − x)ρ(y) dy +

Ω−

where (a) follows from (12). By (47) and (48), (46) can not
hold for x sufﬁciently large. This is a contradiction. Hence,
the number of mass points of F ∗ must be ﬁnite.
Based on Proposition 5, one can conclude that a unique
and discrete random variable X with a ﬁnite number of mass
points is the answer to the optimization problem addressed in
Theorem 1. It can also be deduced that the input pdfs which
achieve rates at any of the corner points of the capacity region
of the channel are unique. For example, letting R1 = I(X1 ; Y )
and R2 = I(X2 ; Y |X1 ), then based on [1], R2 is achieved by
a unique input pdf and according to Proposition 5, R1 has the
same uniqueness property.

(43)

R EFERENCES
[1] J. G. Smith, “The Information Capacity of Amplitude and VarianceConstrained Scalar Gaussian Channels,”Inform. Contr., vol. 18, pp. 203219, 1971.
[2] J. G. Smith,“On the information capacity of peak and average power
constrained Gaussian channels,” Ph.D. dissertation., Dep. Elec. Eng.,
Univ. of California, Berkeley, CA, Dec. 1969.
[3] S. Shamai (Shitz) and I. Bar-David, “The capacity of average and
peak-power limited quadrature Gaussian channel,” IEEE Trans. Inform.
Theory, vol. 41, pp. 1060-1071, July 1995.
[4] A. Tchamkerten, “On the Discreteness of Capacity-Achieving Distributions,” IEEE Tran. on Info. Theory, vol. 50, NO. 11, November 2004.
[5] E. Ordentlich, “Maximizing the entropy of a sum of independent random
variables”, IEEE Tran. on Info. Theory, vol. 52, no. 5, pp. 2176-2181,
May 2008.
[6] I. C. A. Faycal, M. D. Trott, and S. Shamai, “The capacity of discretetime memoryless rayleigh-fading channels,” Trans. Inform. Theory, vol.
47, pp. 1290-1301, May 2001.
[7] J. Singh,O. Dabeer and U. Madhow, ”On the limits of communication
with low-precision analog-to-digital conversion at the receiver,” IEEE
Transactions on Communications, vol.57, no.12, pp.3629-3639, December 2009. March 29, 1929.
[8] Y. Wu, L.M. Davis and R. Calderbank, ”On the capacity of the discretetime channel with uniform output quantization,” Information Theory,
2009. ISIT 2009. IEEE International Symposium on , pp.2194-2198,
June 28 2009-July 3 2009.
[9] A. Das, “Capacity-achieving distributions for non-Gaussian additive
noise channels,” in proc. IEEE Int. Symp. Information Theory, Sorrento,
Italy, June 2000, p.432.
[10] E. Lukacs, Characteristics Functions. Grifﬁn, London 1970.
[11] R. M. Dudley, Real analysis and probability, Cambridge University
Press, 2002.
[12] J. B. Conway, “Functions of One Complex Variable I”, Graduate Texts
in Mathematics, 1973.
[13] B. Mamandipoor, K. Moshksar and A. K. Khandani, “On the
sum-capacity of Gaussian MAC with peak constraint”, Technical Report, University of Waterloo, Feb. 2012. Please visit:
https://ece.uwaterloo.ca/∼bmamandi

(44)

(45)

PM (y − x)ρ(y) dy = 0. (46)

By (15), we get ρ(y) ≤ log(Γ(y)) + L ≤ log(k3 ) + L for any
y ∈ R. Hence, (46) requires k3 > 2−L . Choose a constant l
such that l > 2A + log(k3 )+L . Using (15), one has Ω+ ⊆
k4 log(e)
[−l, l]. Therefore,
l
Ω+

PM (y − x)ρ(y) dy ≤

−l

PM (y − x)ρ(y) dy
l

(a)

≤

log(k3 ) + L
−l

q(A) log(Γ(x − A)) + L dy

= 2A q(A) log(Γ(x − A)) + L < 0, (48)

Then
Ω+

x+A
x−A

where equality holds if x belongs to the points of increase of
F ∗ and C is deﬁned in (43). Suppose that F ∗ has an inﬁnite
number of mass points in [−A, A]. Then based on BolzanoWeierstrass Theorem [1], the set of these points admits a limit
point. According to Proposition 4 and Identity Theorem of
Complex Analysis, h(x; F ∗ ) = C + h(PM ) for all x ∈ R,
We will show that the this argument leads to a contradiction.
To do so, we adopt the same idea as in [4]. Let us deﬁne
L C + h(PM ) and ρ(y) log(PY (y; F ∗ )) + L. Based on
∞
the deﬁnition of the constant L, −∞ PM (y − x)ρ(y) dy =
∗
−h(x; F ) + L = 0 for all x ∈ R. Deﬁne
Ω+

PM (y − x) log(Γ(y)) + L dy

l

<

and the capacity-achieving input distribution is unique and
discrete with a ﬁnite number of mass points.
Proof: Based on Lemmas 6 and 7 and Proposition 2, (43)
is immediate. According to Propositions 1, 2 and 3 and Lemma
7 and invoking the KKT Theorem, there is a unique input
distribution, F ∗ , that achieves supFX ∈F I(X; Y ). Following
standard arguments in Smith [1], the necessary and sufﬁcient
condition for F ∗ to be the optimum input cdf is given by
h(x; F ∗ ) ≤ C + h(PM ) ∀x ∈ [−A, A],

PM (y − x)ρ(y) dy

+∞

(c)

where (a) holds by a similar argument made in Remark 1, (b)
hold by (12) and (c) is an easy observation made in [13].
The proof of the following Lemma is provided in [1].
Lemma 7: F is convex and compact in the Levy metric.
Proposition 5: supFX ∈F I(X; Y ) is achieved for some
FX ∈ F, i.e.,
C

l

Q(y − x) dy, (47)

5


Creator:         TeX output 2012.05.16:1034
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May 16 10:36:24 2012
ModDate:        Tue Jun 19 12:55:54 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      431642 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569561379

An Information-Spectrum Approach to the Capacity
Region of General Interference Channel
âˆ— Department

â€  Department

Lei Linâˆ— , Xiao Maâ€  , Xiujie Huangâ€  and Baoming Baiâ€¡

of Applied Mathematics, Sun Yat-sen University, Guangzhou 510275, Guangdong, China
of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou 510006, GD, China
â€¡ State Lab. of ISN, Xidian University, Xiâ€™an 710071, Shaanxi, China
Email: linlei2@mail2.sysu.edu.cn, maxiao@mail.sysu.edu.cn
i

Abstractâ€”This paper is concerned with general interference
channels characterized by a sequence of transition (conditional)
probabilities. We present a general formula for the capacity
region of the interference channel with two pairs of users.
The formula shows that the capacity region is the union of
a family of rectangles, where each rectangle is determined by
a pair of spectral inf-mutual information rates. Although the
presented formula is usually difï¬cult to compute, it provides us
useful insights into the interference channels. For example, the
formula suggests us that the simplest inner bounds (obtained by
treating the interference as noise) could be improved by taking
into account the structure of the interference processes. This is
veriï¬ed numerically by computing the mutual information rates
for Gaussian interference channels with embedded convolutional
codes.

x1 (i)

Encoder1

y1

Decoder1

Ë†
i

Decoder2

Ë†
j

Wn

j
Encoder2

y2

x 2 (j)

Fig. 1.

General interference channel W.

to compute the accessible capacity [9], the primary link is
allowed to have a non-neglected error probability. This makes
the model unattractive when the capacity region is considered.
For this reason, we will remove the ï¬xed-code constraints
on the primary users in this paper.1 In other words, we will
compute a pair of transmission rates at which both links can
be asymptotically error-free.
To justify the computational results, we consider a more
general interference channel which is characterized by a
sequence of transition probabilities W = {W n }âˆ . Similar
n=1
to [12], we utilize the information spectrum approach [13][14].
The capacity region can be described in terms of the spectral
inf-mutual information rates.
The rest of the paper is structured as follows. Sec. II
introduces the main deï¬nitions, including general IC and
spectral inf-mutual information rate. In Sec. III, a formula is
proved for the capacity region of the general IC. In Sec. IV
we present numerical results for GIFC with binary-phase shiftkeying (BPSK) modulation. Sec. V concludes this paper.

I. I NTRODUCTION
The interference channel (IC) is a communication model
with multiple pairs of senders and receivers, in which each
sender has an independent message intended only for the
corresponding receiver. This model was ï¬rst mentioned by
Shannon [1] in 1961 and further studied by Ahlswede [2]
in 1974. A basic problem for the IC is to determine the
capacity region, which is currently one of long-standing open
problems in information theory. Only in some special cases,
the capacity regions are known, such as strong interference
channels and deterministic interference channels [3â€“6]. On
the other hand, some inner and outer bounds of the capacity
region are obtained, for example, see [4, 7, 8]. In recent years,
Etkin, Tse and Wang [8] introduced the idea of approximation
to show that Han and Kobayashi region (HK region) [4] is
within one bit of the capacity region for Gaussian interference
channel (GIFC).
In [9], the authors proposed a new computational model
for the two-user GIFC, in which one pair of users (called
primary users) are constrained to use a ï¬xed encoder and
the other pair of users (called secondary users) are allowed
to optimize their codes. The maximum rate at which the
secondary users can communicate reliably without degrading
the performance of the primary users is called the accessible
capacity of the secondary users. Since the structure of the
interference from the primary link has been taken into account
in the computation, the accessible capacity is usually higher
than the maximum rate when treating the interference as
noise, as is consistent with the spirit of [10][11]. However,

II. BASIC D EFINITIONS A ND P ROBLEM S TATEMENT
A. General IC
Let X1 , X2 be two ï¬nite input alphabets and Y1 , Y2 be two
ï¬nite output alphabets. A general interference channel W (see
Fig. 1) is characterized by a sequence W = {W n (Â·, Â·|Â·, Â·)}âˆ ,
n=1
n
n
n
n
where W n : X1 Ã— X2 â†’ Y1 Ã— Y2 is a probability transition
matrix. That is, for all n,
âˆ‘

W n (y1 , y2 |x1 , x2 )

â‰¥ 0

W (y1 , y2 |x1 , x2 )

=

n

1.

n
n
y1 âˆˆY1 ,y2 âˆˆY2

1 The authors are grateful to Prof. Tse who inspired us to continue the
research in this direction when our previous work was presented in ISITâ€™2011.

1

n
n
The marginal distributions W1 , W2 of the W n are given by
âˆ‘
n
W1 (y1 |x1 , x2 ) =
W n (y1 , y2 |x1 , x2 ),
(1)

B. Preliminaries of Information-Spectrum Approach
The following notions can be found in [14].
Deï¬nition 4 (liminf in probability): For a sequence of random variables {Zn }âˆ ,
n=1

n
y2 âˆˆY2

n
W2 (y2 |x1 , x2 )

âˆ‘

=

W n (y1 , y2 |x1 , x2 ).

(2)

n
y1 âˆˆY1

p- lim inf Zn â‰¡ sup{Î²| lim Pr{Zn < Î²} = 0}.
nâ†’âˆ

(1)
(2) (1) (2)
(n, Mn , Mn , Îµn , Îµn )

Deï¬nition 1: An
code for the
interference channel W consists of the following essentials,
a) message sets:
(1)
M(1) = {1, 2, . . . , Mn },
n
(2)
M(2) = {1, 2, . . . , Mn },
n

Deï¬nition 5: If two random variables sequences X1 =
n
n
{X1 }âˆ and X2 = {X2 }âˆ satisfy that
n=1
n=1
n
n
n
n
PX1 X2 (x1 , x2 ) = PX1 (x1 )PX2 (x2 )

for sender 1
for sender 2

(1)

for encoder 1
for encoder 2

For sender 1 to transmit message i, encoder 1 outputs
the codeword x1 (i). Similarly, for sender 2 to transmit
message i, encoder 2 outputs the codeword x2 (j).
c) collections of decoding sets:

n
PY n |X n (Y1n |X1 )
1
log 1 1
(4)
n)
nâ†’âˆ n
PY1n (Y1
n
PY n |X n (Y2n |X2 )
1
, (5)
I(X2 ; Y2 ) â‰¡ p- lim inf log 2 2
nâ†’âˆ n
PY2n (Y2n )

I(X1 ; Y1 ) â‰¡ p- lim inf

n
{B1i âŠ† Y1 }i=1,...,M (1) ,
for decoder 1
n
n
{B2j âŠ† Y2 }j=1,...,M (2) , for decoder 2
n
âˆ©
âˆ©
where B1i B1iâ€² = âˆ… for i Ì¸= iâ€² and B2j B2j â€² = âˆ… for
j Ì¸= j â€² . After receiving y1 , decoder 1 outputs Ë† whenever
i
y1 âˆˆ B1Ë†. Similarly, after receiving y2 , decoder 2 outputs
i
Ë† whenever y2 âˆˆ B Ë† .
j
2j
d) probabilities of decoding errors:
(1)

Îµn =
(2)

Îµn =

(1)
Mn

(2)
Mn

âˆ‘ âˆ‘

1
(1)
(2)
Mn Mn

i=1 j=1
(1)
(2)
Mn Mn
âˆ‘ âˆ‘

1
(1)
(2)
Mn Mn
i=1 j=1

where

n
PX1 (x1 )W n (y1 , y2 |x1 , x2 ).(7)

x1 ,y1

c
n
W1 (B1i |x1 (i), x2 (j))

III. T HE C APACITY R EGION OF G ENERAL IC
In this section, we derive a formula for the capacity region
C(W) of the general IC.
Theorem 1: The capacity region C(W) of the interference
channel W is given by
âˆª
C(W) =
RW (X1 , X2 ),
(8)

c
n
W2 (B2j |x1 (i), x2 (j)),

(X1 ,X2 )âˆˆSI

where RW (X1 , X2 ) is deï¬ned as the collection of all
(R1 , R2 ) satisfying that
0 â‰¤ R1

j

â‰¤ I(X1 ; Y1 )

(9)

0 â‰¤ R2

Ë† = arg max Pr{y2 |x2 (j)}
j

â‰¤ I(X2 ; Y2 ).

(10)

To prove Theorem 1, we need the following lemmas.
Lemma 1: Let

as the transmitted messages.
Deï¬nition 2: A rate pair (R1 , R2 ) is achievable if there
(1)
(2) (1) (2)
exists a sequence of (n, Mn , Mn , Îµn , Îµn ) codes such that

n
n
(X1 = {X1 }âˆ , X2 = {X2 }âˆ )
n=1
n=1

lim Îµ(2) = 0,
n

be any channel input such that (X1 , X2 ) âˆˆ SI . The
corresponding output via an interference channel W =
{W n } is denoted by (Y1 = {Y1n }âˆ , Y2 = {Y2n }âˆ ).
n=1
n=1
(1)
(2)
Then, for any ï¬xed Mn and Mn , there exists an
(1)
(2) (1) (2)
(n, Mn , Mn , Îµn , Îµn ) code satisfying that

nâ†’âˆ

(2)
log Mn

â‰¥ R1 and lim inf
â‰¥ R2 .
nâ†’âˆ
n
n
Deï¬nition 3: The set of all achievable rates is called the
capacity region of the interference channel W, which is
denoted by C(W).
lim inf

âˆ‘

n
PY2n |X2 (y2 |x2 ) =

and

and

n
PX2 (x2 )W n (y1 , y2 |x1 , x2 ),(6)

x2 ,y2

i

=0

âˆ‘

n
PY1n |X1 (y1 |x1 ) =

where â€œcâ€ denotes the complement of a set. Here we have
(2)
(1)
assumed that each message of i âˆˆ Mn and j âˆˆ Mn
is produced with equal probability.
Remark: The optimal decoding to minimize the probability
of errors is deï¬ning the decoding sets B1i and B2j according
to the the maximum likelihood decoding [15]. That is, the two
receivers choose, respectively,
Ë† = arg max Pr{y1 |x1 (i)}
i

lim Îµ(1)
n
nâ†’âˆ
(1)
log Mn

(3)

n
n
for all x1 âˆˆ X1 , x2 âˆˆ X2 and n, they are called independent
and denoted by X1 âŠ¥X2 .
Similar to [13], we have
â–³
Deï¬nition 6: Set SI = {(X1 , X2 )|X1 âŠ¥X2 }. Given an
(X1 , X2 ) âˆˆ SI , for the interference channel W, we deï¬ne
the spectral inf-mutual information rate by

b) sets of codewords:
n
{x1 (1), x1 (2), . . . , x1 (Mn )} âŠ‚ X1 ,
(2)
n
{x2 (1), x2 (2), . . . , x2 (Mn )} âŠ‚ X2 ,

nâ†’âˆ

nâ†’âˆ

c
c
Îµ(1) + Îµ(2) â‰¤ Pr{Tn (1)} + Pr{Tn (2)} + 2eâˆ’nÎ³ ,
n
n

2

(11)

(1)

1
Tn (1) = {(x1 , y1 )| n log
1
Tn (2) = {(x2 , y2 )| n log

PY n |X n (y1 |x1 )
1
1
PY n (y1 )
1
PY n |X n (y2 |x2 )
2
2
PY n (y2 )
2

(1)

>

1
n

>

(2)
1
n log Mn

log Mn + Î³}

(1)

1
Îµn â‰¥ Pr{ n log

+ Î³}.

n
n
PY n |X n (Y1 |X1 )
1
1
n
PY n (Y1 )
1
n
n
PY n |X n (Y2 |X2 )
2
2
n
PY n (Y2 )

â‰¤
â‰¤

âˆ‘

âˆ’nÎ³
(1)

(14)

In order to prove this inequality, we set
n
n
Ai = {y1 âˆˆ Y1 |PX1 |Y1n (x1 (i)|y1 ) â‰¤ eâˆ’nÎ³ }.

It can be seen that
Pr{Ln } =

(1)
Mn
âˆ‘

i=1

=
â‰¤
=

(1)
Mn
âˆ‘

i=1
(1)
Mn
âˆ‘
i=1
(1)
Mn
âˆ‘

=

n
PX1 Y1n (x1 (i), Ai

âˆ‘

(1)
Mn
âˆ‘

âˆ’nÎ³

â‰¤e

(1)
Mn
âˆ‘

âˆ©

B1i ) +
B1i ) +

(1)
Mn
âˆ‘

i=1
(1)
Mn
âˆ‘
i=1

n
PX1 Y1n (x1 (i), Ai

âˆ©

c
B1i )

c
n
PX1 Y1n (x1 (i), B1i )

B1i

n
PX1 Y1n (x1 (i), y1 ) + Îµn

(1)

âˆ©

B1i

n
PX1 |Y1n (x1 (i)|y1 )PY1n (y1 ) + Îµn

âˆ‘

(1)

i=1 y1 âˆˆB1i
(1)
Mn
âˆª

= eâˆ’nÎ³ PY1n (

â‰¤ eâˆ’nÎ³ ,

âˆ©

(1)

âˆ©

âˆ‘

i=1 y1 âˆˆAi
a

n
PX1 Y1n (x1 (i), Ai )

n
PX1 Y1n (x1 (i), Ai

i=1 y1 âˆˆAi

âˆ’nÎ³
n
n
PX1 (x1 )PY1n |X1 (y1 |x1 ) e (1)
Mn

Mn

(2)

1
n

Pr{Ln } â‰¤ Îµ(1) + eâˆ’nÎ³ .
n

jÌ¸=1

âˆ’ 1) e

â‰¤

2

the ï¬rst inequality of (13) can be expressed as

(2)

iÌ¸=1 (x1 ,y1 )âˆˆTn (1)
âˆ‘ eâˆ’nÎ³
(1)
(1) = (Mn
Mn
iÌ¸=1

(1)

n
Ln = {(x1 , y1 )|PX1 |Y1n (x1 |y1 ) â‰¤ eâˆ’nÎ³ },

iÌ¸=1 (x1 ,y1 )âˆˆTn (1)

âˆ‘

log Mn âˆ’ Î³} âˆ’ eâˆ’nÎ³

By setting

It can be seen that
âˆ‘
âˆª
Pr{E1i }
Pr{ E1i } â‰¤
iÌ¸=
iÌ¸=1
âˆ‘1
=
Pr{(x1 (i), y1 ) âˆˆ Tn (1)}
iÌ¸=1
âˆ‘
a âˆ‘
n
=
PX1 (x1 )PY1n (y1 )
b

1
n

n
n
Pr{PX1 |Y1n (X1 |Y1n ) â‰¤ eâˆ’nÎ³ }.

Îµn + Îµn = Îµn + Îµn
âˆª
âˆª
c
c
E2j }
â‰¤ Pr{E11 } + Pr{ E1i } + Pr{E21 } + Pr{
iÌ¸=1

â‰¤

n
and noticing PX1 (x1 ) = 1 , we can rewrite the ï¬rst term
(1)
Mn
on the right-hand side of the ï¬rst inequality of (13) as

For receiver 1, an error occurs if (x1 (1), y1 ) âˆˆ Tn (1) or
/
(x1 (i), y1 ) âˆˆ Tn (1) for some i Ì¸= 1. Similarly, for receiver 2,
an error occurs if (x2 (1), y2 ) âˆˆ Tn (2) or (x2 (j), y2 ) âˆˆ Tn (2)
/
for some j Ì¸= 1. So the ensemble average of the error
probabilities of decoder 1 and decoder 2 can be upper-bounded
as follows:
(1)

(2)

n
n
PY1n |X1 (y1 |x1 )
PX1 |Y1n (x1 |y1 )
=
n (y1 )
n
PY1
PX1 (x1 )

E1i = {(x1 (i), y1 ) âˆˆ Tn (1)}, E2j = {(x2 (j), y2 ) âˆˆ Tn (2)},

(2)

(1)

log Mn âˆ’ Î³} âˆ’ eâˆ’nÎ³ ,
(13)
n
n
for every Î³ > 0, where X1 (resp., X2 ) places probability
(1)
(2)
mass 1/Mn (resp., 1/Mn ) on each codeword for encoder
1 (resp., encoder 2) and (3), (6), (7) hold.
Proof of Lemma 2: The proof is similar to that of [13,
Lemma 4]. By using the relation
(2)

1
Îµn â‰¥ Pr{ n log

Proof of Lemma 1: The proof is similar to that of [13,
Lemma 3].
(1)
Codebook generation. Generate Mn independent code(1)
n
words x1 (1), ..., x1 (Mn ) âˆˆ X1 subject to the probability
(2)
n
distribution PX1 . Similarly, generate Mn independent code(2)
n
words x2 (1), ..., x2 (Mn ) âˆˆ X2 subject to the probability
n
distribution PX2 .
Encoding. To send message i, sender 1 sends the codeword
x1 (i). Similarly, to send message j, sender 2 sends x2 (j).
Decoding. The receiver 1 chooses the i such that
(x1 (i), y1 ) âˆˆ Tn (1) if such i exists and is unique. Similarly,
the receiver 2 chooses the j such that (x2 (j), y2 ) âˆˆ Tn (2) if
such j exists and is unique. Otherwise, an error is declared.
Analysis of the error probability. By the symmetry of the
random code construction, we can assume that (1, 1) was sent.
Deï¬ne

(1)

(2)

Lemma 2: For all n, any (n, Mn , Mn , Îµn , Îµn ) code
satisï¬es that

where

PY1n (y1 ) + Îµn

B1i ) + Îµn â‰¤ eâˆ’nÎ³ + Îµn ,
(1)

(1)

i=1

(15)
where B1i is the decoding region corresponding to codeword
x1 (i) and â€œaâ€ follows from the deï¬nition of Ai . Therefore,
the ï¬rst inequality of (13) is proved. Similarly, we can obtain
the second inequality of (13).
Now we prove Theorem 1.
Proof of Theorem 1:
1) To prove that an arbitrary (R1 , R2 ) satisfying (9) and
(10) is achievable, we deï¬ne

where â€œaâ€ follows from the independence of x1 (i) (i Ì¸= 1)
and y1 and â€œbâ€ follows from the deï¬nition of Tn (1). Similarly,
we obtain
âˆª
E2j } â‰¤ eâˆ’nÎ³ .
(12)
Pr{
jÌ¸=1

Combining all inequalities above, we can see that there
(1)
(2) (1) (2)
must exist at least one (n, Mn , Mn , Îµn , Îµn ) code satisfying (11).

(1)
(2)
Mn = en(R1 âˆ’2Î³) and Mn = en(R2 âˆ’2Î³)

3

n1

i
Encoder1

y1

x1 (i)

Decoder1

Decoder2

into the interference channels, as illustrated by the following
example.
The considered example has the model as shown in Fig. 2,
where the channel inputs x1 (i) and x2 (j) are BPSK sequences
with power constraints P1 and P2 , respectively; the additive
noise n1 and n2 are sequences of independent and identically
distributed (i.i.d.) Gaussian random variables of variance one;
the channel outputs y1 and y2 are
âˆš
y1 = x1 (i) + ax2 (j) + n1 ,
(20)
âˆš
y2 = x2 (j) + ax1 (i) + n2 .
(21)

Ë†
i

Ë†
j

a
a

j
Encoder2

y2

x 2 (j)
n2

Fig. 2.

Symmetric Gaussian interference channel.

For any two arbitrary input processes x1 and x2 , the deï¬ned
pair of rates given in Theorem 1 are infeasible to compute.
Therefore, we assume that x1 and x2 are the outputs from
two (possibly different) generalized trellis encoders driven by
independent uniformly distributed (i.u.d.) input sequences, as
proposed in [9]. In this case, both x1 and x2 are block-wise
stationary, and (hence)

for an arbitrarily small constant Î³ > 0. Then, Lemma 1
(1)
(2) (1) (2)
guarantees the existence of an (n, Mn , Mn , Îµn , Îµn ) code
satisfying
(1)

n
n
PY n |X n (Y1 |X1 )
1
1
n
PY n (Y1 )
1
n
n
PY n |X n (Y2 |X2 )
2
2
â‰¤ R2 âˆ’ Î³}
n)
PY n (Y

(2)

1
Îµn + Îµn â‰¤ Pr{ n log

â‰¤ R1 âˆ’ Î³}

1
+Pr{ n log

+ 2eâˆ’nÎ³

1
â‰¤ Pr{ n log
1
+Pr{ n

2
2
n
n
PY n |X n (Y1 |X1 )
1
1
n
PY n (Y1 )
1
n
n
PY n |X n (Y2 |X2 )
2
2
n)
PY n (Y2
2

â‰¤ I(X1 ; Y1 ) âˆ’ Î³}

I(X1 ; Y1 ) =
âˆ’nÎ³

â‰¤ I(X2 ; Y2 ) âˆ’ Î³} + 2e

.
(16)
From the deï¬nition of the spectral inf-mutual information rate,
we have
lim Îµ(1) = 0 and lim Îµ(2) = 0.
n
n
log

nâ†’âˆ

I(X2 ; Y2 ) =

2) Suppose that a rate pair (R1 , R2 ) is achievable. Then, for
(2) (1) (2)
(1)
any constant Î³ > 0, there exists an (n, Mn , Mn , Îµn , Îµn )
code satisfying
(1)

(2)

lim Îµ(1) = 0 and
n

nâ†’âˆ

â‰¥ R2 âˆ’ Î³

(17)

G(D) = [1 + D + D2 1 + D2 ].

(1)

nâ†’âˆ

(2)

1
Îµn â‰¥ Pr{ n log

n
n
PY n |X n (Y1 |X1 )
1
1
n)
PY n (Y1
1
n
n
PY n |X n (Y2 |X2 )
2
2
n)
PY n (Y2

â‰¤ R1 âˆ’ 2Î³} âˆ’ eâˆ’nÎ³
â‰¤ R2 âˆ’ 2Î³} âˆ’ eâˆ’nÎ³

2

.
(18)

Taking the limits as n â†’ âˆ on both sides, we have
1
lim Pr{ n log

nâ†’âˆ

1
lim Pr{ n log

nâ†’âˆ

n
n
PY n |X n (Y1 |X1 )
1
1
n
PY n (Y1 )
1
n
n
PY n |X n (Y2 |X2 )
2
2
n
PY n (Y2 )

â‰¤ R1 âˆ’ 2Î³} = 0
â‰¤ R2 âˆ’ 2Î³} = 0

,

(23)

(24)

Fig. 3 shows the trellis representation of the signal model when
sender 1 uses CcBPSK and sender 2 uses UnBPSK. Fig. 4
shows the numerical results. The point â€œAâ€ can be achieved by
a coding scheme, in which sender 1 uses a binary linear (coset)
code concatenated with the convolutional code and sender 2
uses a binary linear code, and the point â€œBâ€ can be achieved
similarly; while the points on the line â€œABâ€ can be achieved
by time-sharing scheme. The point â€œCâ€ represents the limits
when the two users use binary linear codes but regard the
interference as an i.u.d. additive (BPSK) noise. It can be seen
that knowing the structure of the interference can be used to
improve potentially the bandwidth-efï¬ciency.

lim Îµ(2) = 0.
n

From Lemma 2, we get
1
Îµn â‰¥ Pr{ n log

(22)

Since x1 , x2 and y1 , y2 can be viewed as the Markov chains
and the hidden Markov chains, respectively, the right-hand
sides of (22) and (23) can be estimated by performing the
BCJR algorithm 2 [16][17].
We consider two schemes, UnBPSK and CcBPSK, where
UnBPSK means that x1 (resp. x2 ) is an i.u.d. BPSK sequence
and CcBPSK means that x1 (resp. x2 ) is an output from the
convolutional encoder with the generator matrix

nâ†’âˆ

log Mn
log Mn
â‰¥ R1 âˆ’ Î³ and
n
n
for all sufï¬ciently large n and

1
n
I(X1 ; Y1n ),
n
1
n
lim I(X2 ; Y2n ).
nâ†’âˆ n
lim

nâ†’âˆ

(19)

2

V. C ONCLUSIONS

implying by deï¬nition that R1 âˆ’ 2Î³ â‰¤ I(X1 ; Y1 ) and R2 âˆ’
2Î³ â‰¤ I(X2 ; Y2 ). This completes the proof since Î³ is arbitrary.

In this paper, we have proved that the capacity region of
the two-user interference channel is the union of a family
of rectangles, each of which is deï¬ned by a pair of spectral
inf-mutual information rates associated with two independent
input processes. For special cases, the deï¬ned pair of rates
can be computed, which provide us useful insights into the
interference channels.

IV. N UMERICAL R ESULTS
We have obtained a formula of the capacity region for the
general IC, which shows that any pair of independent input
processes deï¬ne a pair of achievable rates. Although it is not
computable in general, the formula provides us useful insights

2 Only

4

forward recursion is required.

s-(b) u1(b) x1(b) x2(b)
0
0 P P P P
2
2
1
1
0
0 P P P P
1
1
2
2
0
0  P  P  P2  P2
1
1
0
0  P  P  P2  P2
1
1
0
1 P P P P
2
2
1
1
0
1 P P P P
2
2
1
1
0
1  P  P  P2  P2
1
1
0
1  P  P  P2  P2
1
1
1
0 P P P P
2
2
1
1
1
0 P P P P
2
2
1
1
1
0  P  P  P2  P2
1
1
1
0  P  P  P2  P2
1
1
1
1 P P P P
1
1
2
2
1
1 P P P P
1
1
2
2
1
1  P  P  P2  P2
1
1
1
1  P  P  P2  P2
1
1

s+(b) s-(b) u1(b) x1(b)
0
2
0  P  P
1
1
0
2
0  P  P
1
1
0
2
0  P  P
1
1
0
2
0  P  P
1
1
2
2
1 P P
1
1
2
2
1 P P
1
1
2
2
1 P P
1
1
2
2
1 P P
1
1
0
3
0 P P
1
1
1
1
0
3
0 P P
0
3
0 P P
1
1
0
3
0 P P
1
1
2
3
1  P  P
1
1
2
3
1  P  P
1
1
2
3
1  P  P
1
1
2
3
1  P  P
1
1

x2(b)
 P P
2
2
P P
2
2
 P  P
2
2
 P  P
2
2

 P P
2
2
P P
2
2
 P  P
2
2
 P  P
2
2

 P P
2
2
P P
2
2
 P  P
2
2
 P  P
2
2

 P P
2
2
P P
2
2
 P  P
2
2
 P  P
2
2

s+(b)
1
1
1
1
3
3
3
3
1
1
1
1
3
3
3
3

[4]

[5]

[6]

[7]

Fig. 3. The trellis section of (CcBPSK, UnBPSK) with 32 branches. For
each branch b, sâˆ’ (b) and s+ (b) are the starting state and the ending state,
respectively; while the associated labeling x1 (b) and x2 (b) are the transmitted
signals at sender 1 and sender 2, respectively.

[8]

1

[9]

A

0.9

0.8

0.7

I(X2;Y2)

0.6

[10]

B

0.5

0.4

[11]

0.3

0.2

(UnBPSK, CovBPSK)
(CovBPSK, UnBPSK)

0.1

(UnBPSK, CovBPSK)
0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

[12]

I(X1;Y1)

Fig. 4.
The evaluated achievable rate pairs of a speciï¬c GIFC, where
P1 = P2 = 7.0 dB and a = 0.5. â€œ(UnBPSK, UnBPSK)â€ states that both
senders adopt UnBPSK; â€œ(CovBPSK, UnBPSK)â€ states that sender 1 adopts
CovBPSK and sender 2 adopts UnBPSK; â€œ(UnBPSK, CoBPSK)â€ states that
sender 1 adopts UnBPSK and sender 2 adopts CovBPSK.

[13]

[14]
ACKNOWLEDGMENT
This work was supported by the 973 Program (No.2012CB316100) and the NSF under Grants
61172082 and 60972046 of China.
R EFERENCES

[15]

[1] C. E. Shannon, â€œTwo-way communication channels,â€
in Forth Berkeley Symp. on Math. Statist. and Prob.,
J. Neyman, Ed., vol. 1. Statist. Lab. of the University of
California, Berkely: University of California Press, Jun.
20 - Jul. 30 1961, pp. 611â€“644.
[2] R. Ahlswede, â€œThe capacity region of a channel with two
senders and two receivers,â€ The Annals of Probability,
vol. 2, no. 5, pp. 805â€“814, Oct. 1974.
[3] A. B. Carleial, â€œA case where interference does not

[16]

[17]

5

reduce capacity,â€ IEEE Trans. Inform. Theory, vol. 21,
no. 5, pp. 569â€“570, 1975.
T. S. Han and K. Kobayashi, â€œA new achievable rate
region for the interference channel,â€ IEEE Trans. Inform.
Theory, vol. IT-27, no. 1, pp. 49â€“60, Jan. 1981.
A. A. El Gamal and M. H. M. Costa, â€œThe capacity
region of a class of determinsitic interference channels,â€
IEEE Trans. Inform. Theory, vol. IT-28, no. 2, pp. 343â€“
346, Mar. 1982.
M. H. M. Costa and A. A. El Gamal, â€œThe capacity
region of the discrete memoryless interference channel
with strong interference.â€ IEEE Trans. Inform. Theory,
vol. IT-33, no. 5, pp. 710â€“711, Sep. 1987.
G. Kramer, â€œOuter bounds on the capacity of Gaussian interference channels,â€ IEEE Trans. Inform. Theory,
vol. 50, no. 3, pp. 581â€“586, Mar. 2004.
R. H. Etkin, D. N. C. Tse, and H. Wang, â€œGaussian
interference channel capacity to within one bit,â€ IEEE
Trans. Inform. Theory, vol. 54, no. 12, pp. 5534â€“5562,
Dec. 2008.
X. Ma, X. Huang, L. Lin, and B. Bai, â€œAccessible
capacity of secondary users,â€ Dec. 2010, submitted
to IEEE Trans. Inform. Theory, available on the web
http://arxiv.org/abs/1012.5197, also presented in ISIT, St.
Petersburg, Russia, Aug.2011, PP.814-818.
F. Baccelli, A. A. El Gamal, and D. Tse, â€œInterference
networks with point-to-point codes,â€ IEEE Trans. Inform.
theory, vol. 57, no. 5, pp. 2582â€“2596, May 2011.
K. Moshksar, A. Ghasemi, and A. K. Khandani, â€œAn alternative to decoding interference or treating interference
as gaussian noise,â€ in IEEE International Symposium on
Information Theory,, St. Petersburg, Russia, Aug. 2011,
pp. 1176â€“1180.
T. S. Han, â€œAn information-spetrum approach to capacity
theorems for the general multiple-access channel,â€ IEEE
Trans. Inform. Theory, vol. 44, no. 7, pp. 2773â€“2795,
Nov. 1998.
T. S. Han and S. VerdÂ´ , â€œApproximation theory of output
u
statistics,â€ IEEE Trans. Inform. Theory, vol. 39, no. 3, pp.
752â€“772, May 1993.
T. S. Han, Information-spectrum methods in information
theory. New York: Springer-Verlag Berlin Heidelberg,
2003.
R. H. Etkin, N. Merhav, and E. Ordentlich, â€œError exponents of optimum decoding for the interference channel,â€
IEEE Trans. Inform. Theory, vol. 56, no. 1, pp. 40â€“56,
Jan. 2010.
D. M. Arnold, H.-A. Loeliger, P. O. Vontobel, A. KavË‡ iÂ´ ,
cc
and W. Zeng, â€œSimulation-based computation of information rates for channels with memory,â€ IEEE Trans.
Inform. Theory, vol. 52, no. 8, pp. 3498â€“3508, Aug.
2006.
L. R. Bahl, J. Cocke, F. Jelinek, and J. Raviv, â€œOptimal
decoding of linear codes for minimizing symbol error
rate,â€ IEEE Trans. Inform. Theory, vol. IT-20, no. 2, pp.
284â€“287, Mar. 1974.


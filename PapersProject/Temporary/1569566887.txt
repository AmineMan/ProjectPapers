Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 19:25:34 2012
ModDate:        Tue Jun 19 12:56:24 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      974707 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566887

Locally Repairable Codes
Dimitris S. Papailiopoulos and Alexandros G. Dimakis
University of Southern California,
Los Angeles, CA 90089, Email:{papailio, dimakis}@usc.edu

variety of asymptotic and explicit repair bandwidth optimal
code constructions were introduced in [1], [3], [7]–[12].
It seems that for cloud storage applications, the main repair
performance bottleneck is the disk I/O overhead. The disk I/O
is proportional to the number of nodes r involved in the repair
process of a failed node. This number r deﬁnes our metric of
interest: repair locality.
Repair locality was identiﬁed as a good metric for repair
cost independently by Gopalan et al. [14], Oggier et al. [6],
and Papailiopoulos et al. [15]. Codes that have good locality
properties where studied in [4], [13]–[15], [18]. In [14], a
trade-off between locality and code distance, i.e., reliability,
was deﬁned for scalar linear codes. However, up to now
there do not exist explicit and high rate codes optimized for
locality and there is no universal approach (for both linear and
nonlinear codes) that characterizes the information theoretic
limits between repair locality r, code distance d, and storage
per node α. In this paper we address this open problem.
Our Contribution: Let a ﬁle of size M that is cut in
k pieces which are encoded in n > k elements of size
α = (1 + ) M . We establish an information theoretic tradeoff
k
between the repair locality r, the code distance d, and the
amount of storage spent per node α, for storage codes of length
n. We derive our bounds using a characterization of the code
distance d in terms of entropy. A new information ﬂow graph is
fundamental to our derivations. Using random linear network
coding (RLNC) arguments on this ﬂow graph [16], we show
that linear vector codes sufﬁce to achieve the trade-off. We
call these optimal codes locally repairable codes.
Then, we focus on the operational point where any k coded
elements can recover the ﬁle, i.e., d = n − k + 1. We construct
the ﬁrst explicit family of LRCs that have locality r at the cost
of an excess storage overhead of = 1 . This cost can be made
r
asymptotically (in n, k) negligible when√ is any sub-linear
r
function of k such as r = log(k), or r = k. Our designs are
vector linear, work for any n, k, r and require ﬁnite ﬁeld of
order n. A general LRC construction for any feasible point of
the tradeoff is left as an interesting open problem.

Abstract—One main challenge in the design of distributed
storage codes is the Exact Repair Problem: if a node storing
encoded information fails, to maintain the same level of reliability,
we need to exactly regenerate what was lost in a new node. A
major open problem in this area has been the design of codes
that i) admit exact and low cost repair of nodes and ii) have
arbitrarily high data rates.
In this paper, we are interested in the metric of repair
locality, which corresponds to the the number of disk accesses
required during a node repair. Under this metric we characterize
an information theoretic trade-off that binds together locality,
code distance, and storage cost per node. We introduce Locally
repairable codes (LRCs) which are shown to achieve this tradeoff.
The achievability proof uses a “locality aware” ﬂow graph gadget
which leads to a randomized code construction. We then present
the ﬁrst explicit construction of LRCs that can achieve arbitrarily
high data-rates.

I. I NTRODUCTION
Distributed and cloud storage systems have reached such
a massive scale that recovery from failures is now part of
regular operation rather than a rare exception. These large
scale storage systems have to allow for high data availability
and be able to tolerate multiple physical node failures to
prevent data loss. These systems can achieve the targeted
data availability and reliability requirements by introducing
redundancy among the stored bits. Erasure coded storage
systems achieve high reliability without requiring the increased
storage cost that is associated with data replication [5]. Three
application contexts where erasure coding techniques are being
currently deployed or under investigation are Cloud storage
systems like Facebook’s Hadoop cluster, archival storage, and
peer-to-peer storage systems like Cleversafe and Wuala (see
e.g. [1], [3])
A central issue that arises in coded storage is the Repair
Problem: how to maintain the encoded representation when
failures (node erasures) occur. To maintain the same redundancy when a storage node leaves the system, a newcomer
node has to join the array, access some existing nodes, and
exactly reproduce the lost contents. During this repair process,
there are several metrics that can be optimized: the total
information read from existing disks, the total number of
bits communicated in the network [7]–[12] (called repair
bandwidth [2]), or the total number of disks required for each
repair [4], [6]. Currently, the most well-understood metric
is repair bandwidth that was characterized in [2]. A great

II. R EPAIR L OCALITY VS . C ODE D ISTANCE VS . S TORAGE
C APACITY
A. Storage Code Distance Through Entropy
In the following, we see how we can use entropy on the
coded elements of a storage code to make arguments on
the code distance. This way, we aim to establish a universal
information theoretic tradeoff of (linear or nonlinear) codes

This research was supported in part by NSF Career Grant CCF-1055099
and research gifts by Intel and Microsoft Research.

1

Yi , i ∈ [n], has entropy α = M , and locality r, then the
k
minimum code distance is bounded as
k
d≤n−k−
+ 2.
(6)
r

that binds together the metric of locality, the code distance,
and the storage capacity spent for each coded element or node.
We would like to note that in many points in this work, we
use the phrase “coded element” instead of “node”.
Let a ﬁle of size M be cut in k equally sized pieces
x = [X1 . . . Xk ],

Observe that according to the above bound, low-locality
r << k is penalizing minimum distance by a component
of k . This distance, or reliability, penalty cannot be avoided
r
for scalar codes. On the other hand, maximum reliability, i.e.,
d = n − k + 1, costs in locality. Indeed an (n, k)-MaximumDistance Separable (MDS) code has both the maximum possible distance n − k + 1 and the worst possible locality r = k.
However, for our purposes we would like locality to be low:
either a constant, or a sub-linear function of k.
In the following, we derive an information theoretic tradeoff
between locality r, distance d, and storage per node α. We see
how the third parameter α can be used to deﬁne operational
points of high distance and low locality. We will refer to codes
that achieve this tradeoff as (n, k, r, d, α)-LRCs.
We will eventually present explicit LRCs for any n, k, r
that have the “(n, k) erasure property”, i.e., that any set of k
coded elements has entropy at least M , which is equivalent to
requiring distance d = n − k + 1. This operational point will
require storage α = M + 1 M .
k
r k

(1)

where Xi s can be viewed as k source elements over some ﬁnite
ﬁeld F that are i.i.d. random variables each having entropy
H(Xi ) = M , for all i ∈ [k], where [N ] denotes the set of
k
integers {1, . . . , N }. Moreover, let an encoding (generator)
function G : F1×k → F1×n that takes as input the k elements
and outputs n coded elements
G(x) = y = [Y1 . . . Yn ],

(2)

where each encoded element (which can be also seen as a
random variable) has entropy
M
,
(3)
k
for all i ∈ [n]. The generator function G deﬁnes a code C. The
rate of the code is the ratio of the aggregate useful information
to the aggregate stored information, i.e., the entropy of the
source elements to the sum of the entropy of each encoded
element
H(X1 , . . . , Xk )
k
R=
≤ ,
(4)
n
n
H(Yi )
i=1
H(Yi ) = α ≥

B. An information theoretic (r, d, α) tradeoff
In the following we determine the information theoretic
minimum distance of a code, where each coded element has
entropy α and repair locality r. We do that by an algorithmic
proof in the same manner as [14] that bounds the distance for
any possible code C. We give a lower bound over all codes,
of the largest set S of coded elements whose entropy is less
than M . To simplify calculations, we denote the storage of
each node as α = (1 + ) M , where ≥ 0.
k
The only structural property of a code that we use in our
proof, is the fact that there exist (r + 1) sized repair groups.
For a code of length n, locality r, and for each of its coded
elements, say Yi , there exist at most other r coded elements
YR(i) that can reconstruct Yi , for i ∈ [n]. Then, the coded
elements indexed by Γ(i) = {i, R(i)} from an (r + 1)-group,
that has the property

with equality when α = M .
k
Deﬁnition 1 (Minimum Code Distance): The
minimum
distance d of the code C is equal to the minimum number
of erasures of elements in y after which the entropy of the
non-erased variables is strictly less than M , that is,
d = min |E|,

(5)
{Y1 ,...,Yn }

such that H ({Y1 , . . . , Yn }\E) < M and E ∈ 2
,
where 2{Y1 ,...,Yn } is the power set of the elements in
{Y1 , . . . , Yn }.
In other words, a code has minimum distance d, when there
is “enough” entropy after any d − 1 coded element erasures
to reconstruct the ﬁle. The above deﬁnition can be restated
in its “dual” form: the minimum distance d of the code C is
equal to n minus the maximum number of non-erased coded
elements in y that cannot reconstruct the ﬁle, that is, d =
n − maxH(S)<M |S|, where S ∈ 2{Y1 ,...,Yn } .
Remark 1: Observe that the above distance deﬁnition is
universal in the sense that it applies to linear or nonlinear codes
and is oblivious to any type of element subpacketization.
We continue by explicitly deﬁning repair locality.
Deﬁnition 2 (Repair Locality): A coded element Yi , i ∈
[n], has repair locality r, if it is a function of r other coded
variables Yi = fi (YR(i) ). The set R(i) indexes the smallest
set of r coded elements that can reconstruct Yi and fi is some
function on these r coded elements.
In [14] Gopalan et al. show that for length n scalar linear
codes, where G is a linear function on x, each coded element

H(YΓ(i) ) = H(Yi , YR(i) ) = H(YR(i) ) ≤ rα,

(7)

for all i ∈ [n], due to the functional dependencies induced
by the locality property. To determine the upper bound on
minimum distance of a C(n, r, d, α) code, we construct the
maximum set of nodes, or coded elements, S that have entropy
less than the M .
Theorem 1: For a code C(n, r, d, α), the minimum distance
is bounded as
M
M
d≤n−
−
+2
α
rα
k
k
=n−
−
+ 2.
(8)
1+
r(1 + )
Proof: The proof can be found in the Appendix of the
full-version of the manuscript [17].

2

∞

Remark 2: We would like to note that the main difference
of our proving technique compared to the one in [14], is
that it involves counting arguments on information ﬂows, or
entropies, instead of ranks of matrices.
Corollary 1: In terms of the code distance, non-overlapping
(r + 1)-groups are optimal.
Observe that if we set = 0 in the above bound, we get
the same bound as [14]. This means that the bound derived
in [14] applies to nonlinear codes as well.
In the following, for simplicity we will assume that (r+1)|n
and then prove that the above bound is achievable using information ﬂows and random linear network coding techniques.

rα

V = {Xi ; i ∈ [k]},
Yjin , Yjout ; j

k ﬁle elements

n−d+1

.
.
.

∞

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

n coded elements

III. ACHIEVABILITY OF THE B OUND : R ANDOM LRC S
In this section, we show that the bound of Theorem 1 is
achievable using a random linear network coding (RLNC)
scheme [16]. In our proof, we use a variant of the information
ﬂow graph of [2] and show that when the (n, k, r, d, α)
parameters of an LRC agree with the bound in Theorem 1,
then the min-cut of this ﬂow graph is large enough for a
speciﬁc multicast session to be feasible. The feasibility of this
multicasting problem is shown to be equivalent to the existence
of (n, k, r, d, α)-LRCs. More precisely we have the following
theorem which we prove in this section.
Theorem 2: Let (r + 1)|n. Then, there exist vector linear
codes (n, k, r, d, α)-LRCs over F2n , that have minimum distance d = n − M − M − 1.
a
ra
In the same manner as [2], the information ﬂow graph is a
directed network, where the k input elements correspond to k
sources, the n coded elements are represented as intermediate
nodes, and the sinks of the network are what we call Data
Collectors (DC), each of which requires to decode all k
source elements. The speciﬁcations of this network, such as the
number and degree of nodes, the edge-capacities, and the cutset bounds, are determined by the (n, k, r, d, α) parameters.
Here, in contrast to the work in [2], we need to account for
the locality properties of the code. By incorporating a subgraph
that accounts for the dependencies among the coded elements
of a repair group, we obtain our “locality aware” ﬂow graph.
In Fig. 1, we show the general ﬂow graph that we use in
our proof. The network that is deﬁned by the ﬂow-graph has
n
k sources and N = n−d+1 sinks (DCs). We refer to this
directed graph as G(n, k, r, d, α) with vertex set
Γin , Γout ; j
j
j

∞

α

￿

n
n−d+1

￿

DCs

Fig. 1.
We sketch the directed acyclic information ﬂow graph
G(n, k, r, d, α). The black vertices correpond to the k sources, each of which
n
has entropy M . The r+1 white vertices correspond to nodes that bottleneck
k
the in-ﬂow within a repair group. The blue vertices correspond to the n nodes,
or coded elements, and the yellow vertices are the DCs (sinks) of the network.

edge capacity between the in- and out- Yi vertices corresponds
to the entropy of a single coded block. The fundamental
difference with the ﬂow graph of [2] is the additional ﬂow
constraints invoked by repair locality assumptions: coded
elements (nodes) in an (r + 1)-group have joint ﬂow (entropy)
at most rα, instead of (r + 1)α. To enforce this constraint, we
bottleneck the in-ﬂow of each group by a node that restricts
it to be at most rα. In Fig. 2, we show the part of the ﬂow
graph that enforces the “bottleneck” induced by a repair group,
where we consider the ﬁrst group, without loss of generality.
n
For a group Γ(i), i ∈ r+1 , we add node Γin that receives
i
in-degree = r

∞

Γin
1

rα

Y1in

∞

Y2in

Γout
1

∞

Yrin

α
α

.
.
.
α

Y1out

α

Y2out

Yrout

Fig. 2. The ﬂow bottlenecks of an (r + 1)-group of elements. Observe
that when an element is lost, a new node can join the system and download
everything from the remaining nodes in its group.

∈ [n] ,

∈ [n] , {DCl ; ∀l ∈ [N ]} .

ﬂow from the sources and is connected with an edge of
capacity rα to a new node Γout . The latter connects to the r+1
i
elements of the i-th group. We should note that when we are
considering a speciﬁc group, it is implied that any block within
that group can be repaired from the remaining r elements.
When a block is lost, the functional dependence among the
elements of that group allows a newcomer block to compute
a function on the remaining r elements and reconstruct what
was lost.
Linear combinations of the ﬁle elements travel along the
edges of this graph towards the sinks, which we call Data

The directed edge set is implied by the following edge capacity
function
io”
“
n
h
8
n
> ∞,(v, u) ∈ {Xi ; i ∈ [k]}, Γin ; j ∈ r+1
j
>
>
“n
h
io ˘
>
¯”
>
n
<
∪ Γout ; j ∈ r+1 , Yjin ; j ∈ [n]
j
`˘
¯
´
ce (v, u) =
∪ Yjout `˘ [n] , {DCl ;˘ ∈ [N ]} , ¯´
j∈
>
>
¯ l out
>
> α, (v, u) ∈ Yjin j ∈ [n] , Y j ∈ [n] ,
>
j
:
0, otherwise.

The vertices {Xi ; i ∈ [k]} correspond to the k ﬁle elements
and Yjout ; j ∈ [n] correspond to the coded elements. The

3

Lemma 3: An LRC with node repair locality r and distance
n − k + 1 requires an additional storage overhead that is equal
1
to min = 1 − δk , where δk ∈ 0, r+1 k+1 .
r
r
Proof: The proof can be found in the Appendix of the
full-version of the manuscript [17].
Remark 3: We can always perform a line search within
r+1 1
1
1
to ﬁnd the minimum that satisﬁes the
r − r k+1 , r
erasure distance.
In the following we present LRCs with repair locality r for
= 1 . In the proof of Lemma 3, we show that when = 1 ,
r
r
then d = n − k + 1 is the maximum possible distance when
(r+1) k. When (r+1)|k the optimal distance is d = n−k+2.

Collectors (DCs). A DC needs to connect to as many coded
elements as such that it can reconstruct the ﬁle. This is
equivalent to requiring source-to-sink (s − t) cuts between the
ﬁle elements and the DCs that are at least equal to M , i.e., the
ﬁle size. An s−t cut in G(n, k, r, d, α) determines the amount
of ﬂow, or entropy, that can travel from the source elements
to the destinations. When d is consistent with the bound of
Theorem 1, then the minimum of all the cuts is at least as
much as the ﬁle size M .
Lemma 1: The minimum source-DC cut in G(n, k, r, d, α)
is larger than or equal to M , when d is consistent Theorem 1.
Proof: The proof can be found in the Appendix of the
full-version of the manuscript [17].
Then, a successful multicast session on G(n, k, r, d, α) can be
interpreted as, and is equivalent to, all DCs decoding the ﬁle,
i.e., all k source elements can be reconstructed at each sink
using the received linear combinations. Interestingly, the linear
combinations of the elements along the edges between the n
node couples (Yiin , Yiout ), are exactly the coding coefﬁcients
that need to be used by the n coded elements to achieve
distance d. Hence, we will use the following lemma to prove
the existence of codes.
Lemma 2 (RLNC): For a network with k sources and N
destinations where η links transmit linear combination of
η
inputs, the probability of success is at least 1 − N
q
We can now combine the above with the fact that there exists
a RLNC that succeeds when q > N , i.e., when q > n =
d
n
M
+ M −1 , to obtain Theorem 2. For simplicity we used
a

ra

the upper bound 2n for the binomial coefﬁcient

n
r

B. Code Construction
The codes that follow are optimal, i.e., LRC, for all n, k, r,
when (r + 1) k and (r + 1)|n.
Let a ﬁle x, of size M = rk, that is subpacketized in r
parts, x = x(1) . . . x(r) , with each x(i) , i ∈ [r], having
size k. We encode each of the r ﬁle parts independently, into
coded vectors y(i) of length n, where (r + 1)|n, using an
outer (n, k) MDS code y(1) = x(1) G, . . . , y(r) = x(r) G,
where G is the n × k MDS generator matrix. As MDS
pre-codes, we use (n, k)-RS codes that require Fq , with
q ≥ n. We generate a single parity sum vector from all
r
(i)
the coded vectors s =
i=1 y . This precoding process
yields a total of rn coded blocks in the y(i) vectors and
n parity blocks in s, i.e., an aggregate of (r + 1)n blocks
available to place in n nodes. In our code, each node expends
α = M + 1 M = r + 1 (coded blocks) in storage capacity.
k
r k
Below we state the circular placement of elements in nodes
of the ﬁrst (r + 1)-group

.

IV. E XPLICIT LRC C ONSTRUCTIONS
A. The d = n − k + 1 operational point
In this section, we study the operational point of d = n−k+
1. We calculate the minimum storage overhead that allows the
“any k property” and we construct explicit LRCs. Our codes
are based on existing MDS codes, like Reed-Solomon (RS)
codes, and the ﬁnite ﬁeld order that we require is q ≥ n.
We ﬁrst solve for the storage overhead that is required to
have distance n − k + 1. This overhead is the minimum one
k
k
that satisﬁes the equation d = n− 1+ − r(1+ ) +2, where
d = n − k + 1. Therefore, the minimum storage overhead
for erasure distance can be found through the following
optimization
min
k
subject to:
1+
≥ 0.

+

k
=k+1
r(1 + )

blocks of y(1)
blocks of y(2)
blocks of y(r)
blocks of s

k
1+

−

k
+ 2.
r(1 + )

node 2
(1)
y2
(2)
y3
.
.
.
(r)
yr+1
s1

...
...
...
.
.
.
...
...

node r
(1)
yr
(2)
yr+1
.
.
.
(r)
yr−2
sr−1

node r + 1
(1)
yr+1
(2)
y1
.
.
.
.
(r)
yr−1
sr

There are three key properties that are obeyed by our
placement:
i) each node contains r coded blocks coming from different
y(l) coded vectors and 1 additional parity element,
ii) The blocks in the r + 1 nodes of the i-th (r + 1)-group,
have indices that appear only in that speciﬁc repair group,
n
i ∈ r+1 , and
iii) the blocks of each “row” have indices that obey a
circular pattern, i.e., the ﬁrst row of elements has indices
{1, 2, . . . , r + 1} the second {2, 3, . . . , r + 1, 1}, and so on.
n
We follow the same placement for all r+1 groups. In Fig. 3,
we show an LRC of the above construction with n = 6 and
k = 4 that has locality 2.

(9)

Due to the ceiling function in the above constraint we do
not obtain a closed form expression of the minimizer min .
However, we identify a potential range of values that satisfy
the equation
n−k+1=n−

node 1
(1)
y1
(2)
y2
.
.
.
(r)
yr
sr+1

C. Repairing Lost Nodes
We will concentrate on the repair of lost nodes in the
ﬁrst repair group of r nodes. This is sufﬁcient since the

(10)

4

Placement

Precoding
(6, 4)
RS-Code

(6, 4)
RS-Code

x1
x2
x3
x4
x5
x6

y1
y2
y3
y4
y5
y6

x1 + y1
x2 + y2
x3 + y3
x4 + y4
x5 + y5
x6 + y6

x1
y2
x3 + y3

x2
y3
x1 + y1

r
That is, the rate of the code is a fraction r+1 of the coding
rate of an (n, k) MDS code, hence is always upper bounded
r
by r+1 . This is due to the extra storage overhead required to
store the parity blocks si , i ∈ [n].
Remark 4: Observe that if we set the repair locality to√ =
r
f (k) and f is a sub-linear function of k (i.e., log(k) or k),
then we obtain non-trivially low locality r << k, while the
excess storage cost = 1 is vanishing when n, k grow.
r
The distance of the presented code is (at least) d = n−k+1
due to the MDS precodes that are used in its design: any k
nodes in the system contain rk distinct ﬁle pieces, k from
each ﬁle piece. Hence, performing erasure decoding on these
r k-tuples of blocks we can generate the r pieces of the ﬁle.

x3
y1
x2 + y2

1st repair group

x5
y6
x4 + y4

x4
y5
x6 + y6

x6
y4
x5 + y5

2nd repair group

Fig. 3.
A (6, 4)-LRC with locality 2. Observe that we spent 3 blocks
per coded element, instead of M = 2. This allows us to have easy and
k
repairability with locality 2. The distance of this code is guaranteed through
the 2 (6, 4)-MDS precodes.

R EFERENCES
[1] The
Coding
for
Distributed
Storage
wiki
http://tinyurl.com/storagecoding
[2] A. G. Dimakis, P. G. Godfrey, Y. Wu, M. J. Wainwright, and K. Ramchandran, “Network coding for distributed storage systems,” in IEEE
Trans. on Inform. Theory, vol. 56, pp. 4539 – 4551, Sep. 2010.
[3] A. G. Dimakis, K. Ramchandran, Y. Wu, and C. Suh, “A survey on
network codes for distributed storage,” in IEEE Proceedings, vol. 99,
pp. 476 – 489, Mar. 2011.
[4] O. Khan, R. Burns, J. Plank, and C. Huang, “In search of I/O-optimal
recovery from disk failures,” in Hot Storage 2011, 3rd Workshop on Hot
Topics in Storage and File Systems, Portland, OR, Jun., 2011.
[5] H. Weatherspoon and J. D. Kubiatowicz, “Erasure coding vs. replication:
a quantitative comparison,” in Proc. IPTPS, 2002.
[6] F. Oggier and A. Datta, “Self-repairing homomorphic codes for distributed storage systems,” in Proc. IEEE Infocom 2011, Shanghai, China,
Apr. 2011.
[7] K.V. Rashmi, N. B. Shah, P. V. Kumar, and K. Ramchandran “Explicit
construction of optimal exact regenerating codes for distributed storage,”
In Allerton Conf. on Control, Comp., and Comm., Urbana-Champaign,
IL, September 2009.
[8] C. Suh and K. Ramchandran, “Exact regeneration codes for distributed
storage repair using interference alignment,” in Proc. 2010 IEEE Int.
Symp. on Inform. Theory (ISIT), Seoul, Korea, Jun. 2010.
[9] K. Rashmi, N. B. Shah, and P. V. Kumar, “Optimal exact-regenerating
codes for distributed storage at the MSR and MBR points via a productmatrix construction,” in IEEE Trans. on Inform. Theory, vol. 57, pp.
5227 – 5239, Jul. 2011.
[10] I. Tamo, Z. Wang, and J. Bruck “MDS Array Codes with Optimal
Rebuilding,” in Proc. IEEE Intern. Symp. on Inform. Theory (ISIT) 2001,
St. Petersburg, Russia, Aug. 2011.
[11] V. R. Cadambe, C. Huang, S. A. Jafar, and J. Li, “Optimal repair of
MDS codes in distributed storage via subspace interference alignment,”
arxiv pre-print 2011. Preprint available at http://arxiv.org/abs/1106.1250.
[12] D. S. Papailiopoulos, A. G. Dimakis, and V. R. Cadambe, “Repair
optimal erasure codes through Hadamard designs,” In Allerton Conf. on
Control, Comp., and Comm., Urbana-Champaign, IL, September 2011.
[13] C. Huang, M. Chen, and J. Li, “Pyramid codes: ﬂexible schemes to trade
space for access efﬁciency in reliable data storage systems”, in Proc.
IEEE International Symposium on Network Computing and Applications
(NCA 2007), Cambridge, MA, Jul. 2007.
[14] P. Gopalan, C. Huang, H. Simitci, and S. Yekhanin, “On the locality of
codeword elements,” Preprint available at http://arxiv.org/abs/1106.3625.
[15] D. S. Papailiopoulos, Jianqiang Luo, Alexandros G. Dimakis, C. Huang,
and J. Li, “Simple regenerating codes: network coding for cloud storage”, in IEEE International Conference on Computer Communications
(Infocom) 2012, Miniconference.
[16] T. Ho, R. Koetter, M. M´ dard, M. Effros, J. Shi, and D. Karger, ”A rane
dom linear network coding approach to multicast,” IEEE Transactions
on Information Theory, vol. 52 (10), pp. 4413–4430, Oct. 2006.
[17] D. S. Papailiopoulos and A. G. Dimakis, “Locally repairable codes,” full
version available at http://tinyurl.com/82cucvd
[18] J. Han and L. A. Lastras-Monta˜ o, “Reliable memories with subline
n
accesses” in Proc. 2010 IEEE Int. Symp. on Inform. Theory (ISIT), pp.
2531–2535, 2007.

block placement is identical across repair groups. The key
observation is that each node within a repair group stores r +1
blocks of distinct indices: the r+1 blocks of a particular index
are stored in r + 1 distinct nodes within the repair group.
Hence, when for example the ﬁrst node fails, then element
(1)
y1 is regenerated by downloading s1 from the second node
(2)
(r+1)
from the third, . . . , and y1 from the last
in the group, y1
node in the group. A simple XOR of the above blocks sufﬁces
to reconstruct the lost block. Hence, for every lost block of
a failed node, the r remaining blocks of the same index that
are stored in the r remaining nodes of the repair group have
to be XORed to regenerate what was lost.
Hence, a single block repair has locality r. Interestingly, the
same applies to the repair locality of an entire node. For each
lost block, r other coded blocks are used for regeneration, and
all originate from the r remaining nodes. In Fig. 4, we show
how repair is performed for the code construction presented
in Fig. 3.

x1
y2
x3 + y3
x2
y3
x1 + y1

x3
y1
x2 + y2

x1
y2
x3 + y3

Fig. 4. The repair of a node failure. The repair locality is 2 since 2 remaining
nodes are involved in reconstructing the lost information of the ﬁrst node, or
the ﬁrst coded element. Observe that we repair by only transferring blocks:
no block combinations are need to be performed at the sender nodes.

D. Rate and Code Distance
The effective coding rate of the codes presented in this
section is
r·k
size of useful information
=
.
(11)
R=
total storage spent
(r + 1) · n

5


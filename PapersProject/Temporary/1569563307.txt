Title:          polar-isit.dvi
Creator:        dvips(k) 5.99 Copyright 2010 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 16:38:05 2012
ModDate:        Tue Jun 19 12:55:46 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      390409 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569563307
Polar codes for q -ary channels, q = 2r
Woomyoung Park

Alexander Barg

that virtual channels for the transmitted symbols converge to
one of r+1 extremal conﬁgurations in which j out of r bits are
transmitted near-perfectly while the remaining r − j bits carry
almost no information. Moreover, the good bits are always
aligned to the right of the transmitted r-block, and no other
situations arise in the limit. Thus, the extremal conﬁgurations
for information rates that arise as a result of polarization are
easily characterized: they form an upper-triangular matrix as
described in Sect. II-B.
Another related work is the paper by Abbe and Telatar
[4]. In it, the authors observed multilevel polarization under
the (F2 )r addition. The main result of their paper provides a
characterization of extremal points of the region of attainable
rates when polar codes are used for each of the r users of
a multiple-access channel. This result applies to a singleuser channel, but does not lead to an easy description of the
extremal conﬁgurations.
Polarization of the kind established in this paper was also
observed in a concurrent independent work by Sahebi and
Pradhan [5].

Abstract—We study polarization for nonbinary channels with
input alphabet of size q = 2r , r = 2, 3, . . . . Using Arıkan’s
polarizing kernel H2 , we prove that virtual channels that arise
in the process of polarization converge to q-ary channels with
capacity 1, 2, . . . , r bits, and that the total transmission rate
approaches the symmetric capacity of the channel. This leads
to an explicit transmission scheme for q-ary channels. The error
probability of decoding using successive cancellation behaves as
exp(−N α ), where N is the code length and α is any constant
less than 0.5.

I. I NTRODUCTION
Polarization was ﬁrst described by Arıkan [1] who constructed binary codes that achieve capacity of symmetric
memoryless channels (and “symmetric capacity” of general
binary-input channels). The main idea of [1] is to combine
the bits of the source sequence using repeated application of
10
the “polarization kernel” H2 = 1 1 . The resulting linear
code of length N = 2n has the generator matrix which forms
⊗n
a submatrix of GN = BH2 , where B is a permutation
matrix. The choice of the rows of GN is governed by the
polarization of virtual channels for individual bits that arise
in the process of channel combining and splitting. Namely,
the data bits are written in the coordinates that correspond to
near-perfect channels while the other bits are ﬁxed to some
values known to both the transmitter and the decoder.
A study of polar codes for channels with nonbinary input
was undertaken by Saso˘ lu et al. [2] and Mori and Tanaka
¸ ¸ g
[3]. For prime q, it sufﬁces to take the kernel H2 , while
for nonprime alphabets, the kernel is time-varying and not
explicit. [2] remarks that the transmission scheme that uses
the kernel H2 with modulo-q addition for composite q does
not necessarily lead to the polarization of the channels to the
two extremes. The authors of [2] suggest several alternatives
to the kernel H2 that rely on randomized permutations or, in
the case of q = 2r , on multilevel schemes that implement
polar coding for each of the bits of the symbol independently,
combining them in the decoding procedure.
In this paper we study polarization for channels with input
alphabet of size q = 2r , r = 2, 3, . . . . Suppose that the
channel is given by a stochastic matrix W (y|x) where x ∈
X , y ∈ Y, X = {0, 1, . . . , q − 1}, and Y is a ﬁnite alphabet.
Assuming that the channel combining is performed using the
kernel H2 with addition modulo q, we establish results about
the polarization of channels for individual symbols. It turns out

II. P OLARIZATION FOR q- ARY CHANNELS
We consider combining of the q-ary data under the action
of the operator H2 , where q = 2r , r ≥ 2. Let W : X →
Y, |X | = q be a discrete memoryless channel (DMC). The
symmetric capacity of the channel W equals
I(W )
x∈X y∈Y

1
W (y|x) log
q

W (y|x)
1
x ∈X q W (y|x )

where the base of the logarithm is 2. Deﬁne the combined
channel W2 and the channels W − and W + by
W2 (y1 , y2 |u1 , u2 ) = W (y1 |u1 + u2 )W (y2 |u2 ),
W − (y1 , y2 |u1 ) =
u2 ∈X

W + (y1 , y2 , u1 |u2 ) =

1
W2 (y1 , y2 |u1 , u2 ),
q

1
W2 (y1 , y2 |u1 , u2 ),
q

(1)
(2)

where u1 , u2 , y1 , y2 are r-vectors and + is a modulo-q
sum. This transformation can be applied recursively to the
channels W − , W + resulting in four channels of the form
W b1 b2 , b1 , b2 ∈ {+, −}. After n steps we obtain N = 2n
(j)
channels WN , j = 1, . . . , N. For the case q = 2 it is shown
(j)
in [1] that as n increases, the channels WN become either
almost perfect or almost completely noisy (polarize). In formal
terms, for any ε > 0

The authors are with Department of ECE and Institute for Systems
Research, University of Maryland, College Park, 20742, USA. E-mails:
woomyoung.park@gmail.com, abarg@umd.edu. This research was partially
supported by NSF grants CCF0916919, CCF0830699, and DMS1117852.

|{b ∈ {+, −}n : I(W b ) ∈ (ε, 1 − ε)}|
= 0.
n→∞
2n
lim

1

(3)

In this paper we extend this result to the case q = 2r , r > 1.
As shown in [1], after n steps of the transformation (1)-(2)
(i)
the channels WN : X → Y N × X i−1 , 1 ≤ i ≤ N are given
by
(i) N
N
WN (y1 , ui−1 |ui ) = qN1−1 uN ∈X N −i W N (y1 |uN GN ),
1
1

Theorem 1: (a) Let n → ∞. The random variable In converges a.e. to a random variable I∞ with E(I∞ ) = I(W ).
(b) For all i = 1, 2, . . . , r
lim Zi,n = Zi,∞

n→∞

i+1

where the variables Zi,∞ take values 0 and 1. With probability
one the vector (Zi,∞ , i = 1, . . . , r) takes one of the following
values:
(Z1,∞ = 0, Z2,∞ = 0, . . . , Zr−1,∞ = 0, Zr,∞ = 0)
(Z1,∞ = 1, Z2,∞ = 0, . . . , Zr−1,∞ = 0, Zr,∞ = 0)
(Z1,∞ = 1, Z2,∞ = 1, . . . , Zr−1,∞ = 0, Zr,∞ = 0)
(5)
.
.
.
.
.
.
.
.
.

⊗n
where GN = BH2 and B is a permutation matrix. Here
we use the shorthand notation for sequences of symbols: for
N
(y1 , y2 , . . . , yN ), etc.
instance, y1

A. Notation
For any pair of input symbols x, x ∈ X , the Bhattacharyya
distance between them is
Z(W{x,x } ) =

(Z1,∞ = 1, Z2,∞ = 1, . . . , Zr−1,∞ = 1, Zr,∞ = 0)
(Z1,∞ = 1, Z2,∞ = 1, . . . , Zr−1,∞ = 1, Zr,∞ = 1).

W (y|x)W (y|x )
y∈Y

Let us restate part (b) of this theorem for ﬁnite n.
Proposition 1: Let ε, δ > 0 be ﬁxed. For k = 0, 1, . . . , r
deﬁne disjoint events

where W{x,x } is the channel obtained by restricting the
input alphabet of W to the subset {x, x } ⊂ X . For x =
(x1 x2 . . . xr ) ∈ X let wtr (x) = max{i : xi = 0} and let
dr (x, y) = wtr (x − y).
Deﬁne the quantity Zv (W ) for v ∈ X \ {0}:
Zv (W ) =

1
2r

Bk,n (ε) = ω : (Z1,n , Z2,n , . . . , Zr,n ) ∈ Rk
k
r
where Rk = Rk (ε)
and
i=1 D1 ×
i=k+1 D0
r
D0 = [0, ε), D1 = (1 − ε, 1]. Then P (∪k=0 Bk,n (ε)) ≥ 1 − δ
starting from some n = n(ε, δ).

Z(W{x,x+v} ).
x∈X

Introduce the ith average Bhattacharyya distance of the channel W by
1
Zv (W )
(4)
Zi (W ) = i−1
2

We need the following lemma.
Lemma 1: For a DMC with q -ary input, I(W ) and Z(W )
are related by

v∈Xi

I(W ) ≥ log

where i = 1, 2, · · · , r and Xi = {v ∈ X : wtr (v) = i}. Then
Z(W )

1
2r (2r −1)

x=x

Z(W{x,x } ) =

1
2r −1

r

i=1

a.e.,

r

2i−1 Zi (W )

1 − Zi (W )2 .

I(W ) ≤

Recall the setting of [1] for the evolution of the channel
parameters. On the set Ω = {+, −}∗ of semi-inﬁnite binary
sequences deﬁne a σ-algebra F on Ω generated by the cylinder
sets S(b1 , . . . , bn ) = {ω ∈ Ω : ω1 = b1 , . . . , ωn = bn } for all
sequences (b1 , . . . , bn ) ∈ {+, −}n and for all n ≥ 0. Consider
the probability space (Ω, F , P ), where P (S(b1 , . . . , bn )) =
2−n , n ≥ 0. Deﬁne a ﬁltration F0 ⊂ F1 ⊂ · · · ⊂ F where
F0 = {∅, Ω} and Fn , n ≥ 1 is generated by the cylinder sets
S(b1 , . . . , bn ), bi ∈ {+, −}.
Let Bi , i = 1, 2, · · · be i.i.d. {+, −}-valued random variables with Pr(B1 = +) = Pr(B1 = −) = 1/2. The random
channel emerging at time n will be denoted by W B , where
(i)
B = (B1 , B2 , · · · , Bn ). Thus, P (W B = WN ) = 2−n for all
n
B
i = 1, . . . , 2 . Let Wn = W , In = I(W B ), Z{x,x },n =
B
Z(W{x,x } ), Zv,n = Zv (W B ), and Zi,n = Zi (W B ). These
random variables are adapted to the above ﬁltration (meaning
that In etc. are measurable w.r.t. Fn for every n ≥ 1).

1+

2r
2i−1 Zi (W )

r
i=1

(6)
(7)

i=1

For r = 1 these inequalities are proved in [1]. For r > 1
Eq. (6) is a restatement of [2, Prop. 3]. The fact that (7) holds
for all r > 1 is proved in [6].
..
Inequalities (6)-(7) imply that if (Z1 , √ . , Zr ) ∈ Rk (ε) then
|I(W )−(r −k)| ≤ δ where δ = max(k ε, (2r−k −1)ε log e).
The following proposition is an immediate corollary of the
above results.
Proposition 2: (a) The random variable I∞ is supported on
the set {0, 1, . . . , r}.
(b) For every 0 ≤ k ≤ r and every δ > 0 there exists ε > 0
such that
lim P ({|In − (r − k)| ≤ δ}

n→∞

Bk,n (ε)) = 0.

(c) E(|{i : Zi,∞ = 0}|) = I(W ).
Proof: The ﬁrst statement is obvious from (6)-(7). To
prove the second statement we note that, with the appropriate
choice of ε

B. Channel polarization

{|In − (r − k)| ≤ δ} ⊃ Bk,n (ε)

In this section we state a sequence of results that shows
that q-ary polar codes based on the kernel H2 can be used to
transmit reliably over the channel W for all rates R < I(W ).
The proofs for the most part are given in [6].

for all n ≥ 0. At the same time, P ({|In − (r − k)| ≤ δ} ∩
◦
Bk ,n (ε)) = 0 for all k = k, and P (∪ Bk,n (ε)) → 1 for
any ε > 0. Together this implies (b). Finally, we have that

2

E(I∞ ) = I(W ). Then use (a) and (b) to claim that E(|{i :
Zi,∞ = 0}|) = r kP (I∞ = k) = I(W ).
k=0

j ∈ Ak,n refers to an r-bit symbol in which r − k rightmost
(j)
bits correspond to small values of Zi (WN ). To transmit data
over the channel, we write the data bits in these coordinates
and encode them using the linear transformation GN .
More speciﬁcally, let us order the coordinates j ∈ [N ]
(j)
r
by the increase of the quantity i=1 2i−1 Zi (WN ) and use
these numbers to locate the subsets Ak,n . We transmit data
by encoding messages uN = (u1 , . . . , uN ) in which if
1
j ∈ Ak,n , k = 0, . . . , r − 1 then the symbol uj is taken from
the subset of symbols of X with the ﬁrst k symbols ﬁxed
and known to both the encoder and the decoder ([1] calls
them frozen bits). In particular, the subset Ar,n is not used to
transmit data. A polar codeword is computed as xN = uN GN
1
1
and sent over the channel.
Decoding is performed using the “successive cancellation”
procedure of [1] with the obvious constraints on the symbol
values. Namely, for j = 1, . . . , N put

We can say a bit more about the nature of convergence
established in this proposition. Let us ﬁx k ∈ {0, 1, . . . , r}
and deﬁne the channel for the r − k rightmost bits of the
transmitted symbol as follows:
W [r−k] (y|u) =

1
2k

u ∈ {0, 1}r−k

W (y|x),
x∈X :xr =u
k+1

where x = (x1 , x2 , . . . , xr ).

˜
Lemma 2: Let V : X → Y be a DMC and let δ > 0. Suppose that (Z1,n (V ), Z2,n (V ), . . . , Zr,n (V )) ∈ Rk (ε), for some
0 ≤ k ≤ r. If ε is sufﬁciently small, then I(V [r−k] ) ≥ r−k−δ.
In particular, it sufﬁces to take ε ≤ (2k+δ − 1)/23r+k−1 .
It turns out that the channels for individual bits converge to
either perfect or fully noisy channels. If the channel for bit j
is perfect then the channels for all bits i, r ≥ i > j are perfect.
If the channel for bit i is noisy then the channels for all bits
j, 1 ≤ j < i are noisy. The total number of near-perfect bits
approaches I(W ). This is made formal in the next proposition.
Proposition 3: Let Ωk = {ω : (Z1,∞ , Z2,∞ , . . . , Zr,∞ ) =
1k 0r−k }, k = 0, 1, . . . , r. For every ω ∈ Ωk

uj =
ˆ

j ∈ Ar,n
j ∈ ∪k≤r−1 Ak,n

where if j ∈ Ak,n , k = 0, 1, . . . , r − 1, then the maximum
is computed over the symbols x ∈ X with the ﬁxed (known)
values of the ﬁrst k bits.
The error probability of this decoding is estimated in
Sect. II-E.

[r−k]
lim |In − I(Wn
)| = 0.

n→∞

Proof: For every ω ∈ Ωk we have that In (ω) → r − k.
Combining this with the previous lemma and Proposition 2(b),
[r−k]
) → r − k.
we conclude that for such ω also I(Wn

D. Proof of Theorem 1: an outline
Part (a) of Theorem 1 follows straightforwardly from [1],
[2]. Namely, as shown in [1, Prop. 4], I(W + ) + I(W − ) =
2I(W ). We note that the proof in [1] uses only the fact that
u1 , u2 are recoverable from x1 , x2 which is true in our case.
Hence the sequence In , n ≥ 1 forms a bounded martingale. By
Doob’s theorem it converges a.e. in L1 (Ω, F , P ) to a random
variable I∞ with E(I∞ ) = I(W ).
To prove part (b) we show that each of the Zi,n ’s converges
a.s. to a (0, 1) Bernoulli random variable Zi,∞ . This convergence occurs in a concerted way in that the limit r.v.’s obey
Zj,∞ ≥ Zi,∞ a.e. if j < i. This is shown by observing that
for any ﬁxed i = 1, . . . , r and for all v ∈ Xi , the Zv,n (W )
converge to identical copies of a Bernoulli random variable.

The concluding claim of this section describes the channel
polarization and establishes that the total number of bits sent
over almost noiseless channels approaches N I(W ).
(i)
Theorem 2: For any DMC W : X → Y the channels WN
polarize to one of the r + 1 extremal conﬁgurations. Namely,
(i)
let Vi = WN and
[k]

|{i ∈ [N ] : |I(Vi ) − k| < δ ∧ |I(Vi ) − k| < δ}|
,
N
where δ > 0, then limN →∞ πk,N = P (I∞ = k) for all k =
0, 1, . . . , r. Consequently
πk,N =

uj ,
(j) N
arg maxx WN (y1 , uj−1 |x),
ˆ1

r

1) Convergence of Zv,n , v ∈ X : In this section we outline
the convergence proof of the Bhattacharyya parameters Zv,n
to Bernoulli random variables. The following relations were
proved in [2]: for all v ∈ X \{0}

kπk → I(W ).
k=1

This theorem follows directly from Theorem 1 and Propositions 2 and 3. Some examples of convergence to the extremal
conﬁgurations described by this theorem are given in the end
of this paper.

Zv (W + ) = Zv (W )2
−

Zv (W ) ≤ 2Zv (W ) +

Zδ (W )Zv+δ (W ).

(8)
(9)

δ∈X \{0,−v}

C. Transmission with polar codes

They imply the following lemma.
Lemma 3: Let

Let us describe a scheme of transmitting over the channel
W with polar codes. Take ε > 0 and choose a sufﬁciently
large n. Assume that the length of the code is N = 2n .
Proposition 1 implies that set [N ], apart from a small subset, is
partitioned into r + 1 subsets Ak,n such that for j ∈ Ak,n the
(j)
(j)
(j)
vector (Z1 (WN ), Z2 (WN ), . . . , Zr (WN )) ∈ Rk (ε). Each

(j)
Zmax (W ) = max Zv (W ),
v∈Xj

j = 1, . . . , r.

Then
(r−j)
(r−j)
Zmax (W + ) = Zmax (W )2 ,

3

j = 0, . . . , r − 1.

(10)

(r)
(r)
Zmax (W − ) ≤ qZmax (W )
q (r)
q (r−1)
(r−1)
Zmax (W − ) ≤ Zmax (W ) + Zmax (W )
2
2
and generally

q (r)
q (r−1)
Z
(W ) + Zmax (W )+
2 max
4
q (r−j+1)
q (r−j)
· · · + j Zmax
(W ) + j Zmax (W ).
2
2

this convergence is uniform, and thus P (An ) ≥ α/2 for all
sufﬁciently large n. Therefore

(11)
(12)

2
P (|Yn+1 − Yn | ≥ δ 2 /2) ≥ P (Yn+1 = Yn , Yn ∈ (δ, 1/2 − δ))
α
≥ ,
4
the last step by (17). This however contradicts the almost sure
convergence of Yn .

(r−j)
Zmax (W − ) ≤

(13)

(c) This implies that P (Y∞ < 1/2) = P (Yn → 0) =
P (Un → 0). From (16)

In particular, take j = 0. Relations (10), (11) imply that
(r)

(r)
Zmax,n+1 = (Zmax,n )2 if Bn+1 = +
(r)

(r)
Zmax,n+1 ≤ qZmax,n if Bn+1 = −.

(14)

P (Un → 0) ≥

(15)

1
4

1
β

.

P ((Un → 0) or (Un ≥ (1/2)1/β for some n)) = 1

Lemma 4: Let Un , n ≥ 0 be a sequence of random variables
adapted to a ﬁltration Fn with the following properties:
(i) Un ∈ [0, 1]
2
(ii) P (Un+1 = Un |Fn ) ≥ 1/2
(iii) Un+1 ≤ qUn for some q ∈ Z+ .
Then there are events Ω0 , Ω1 such that P (Ω0 ∪ Ω1 ) = 1 and
Un (ω) → i for ω ∈ Ωi , i = 0, 1.
Proof: (a) First let us rescale the process Un so that in
the neighborhood of zero it has a drift to zero. Let β ∈ (0, 1)
be such that
q β − 1 < 1/4.

(18)

(19)

provided that U0 ≤ (1/2)1/β .
1

(d) Let δ > 0 be such that q( 1 ) β < 1 − δ (depending on
2
q this may require taking a sufﬁciently small β). Let L :=
1
1
[0, ( 4 ) β ] and R := [1 − δ, 1]. Observe that the process Un
1
cannot move from L to R without visiting C := (( 1 ) β , 1−δ).
2
Let σ1 be the ﬁrst time when Un ∈ C, let η1 be the ﬁrst time
after σ1 when Un ∈ L ∪ R, let σ2 be the ﬁrst time after η1
when Un ∈ C, etc., σ1 < η1 < σ2 < η2 < . . . . We shall prove
that every sample path of the process eventually stays outside
C, i.e., that for almost all ω there exists k = k(ω) < ∞ such
that σk (ω) = ∞.
Assume the contrary, i.e., limk→∞ P (σk < ∞) = α > 0
(since P (σk+1 < ∞) < P (σk < ∞), this limit exists.) We
have

β
Let Xn = Un . Take τ (ω) to be the ﬁrst time when Xn (ω) ≥
1/2. Let Yn = Xmin(n,τ ) . On the event Yn ≥ 1/2 we have
Yn = Yn+1 or
E(Yn+1 − Yn |Fn ) = 0

∞

while on the event Yn < 1/2 we have

P (∃k : σk = ∞) ≥

1 2
1
E(Yn+1 − Yn |Fn ) ≤ (Yn − Yn ) + (q β Yn − Yn )
2
2
1
≤ − Yn ≤ 0.
8
This implies that the sequence Yn , n ≥ 0 forms a supermartingale which is bounded between 0 and 1. By the convergence
theorem, Yn → Y∞ a.e. and in L1 (Ω, F , P ), where Y∞
is a random variable supported on [0, 1]. This implies that
EY0 ≥ EYn ↓ EY∞ . Further, if X0 ∈ [0, 1/4] then (since
EY0 = EX0 )

∞

≥α

P (σj = ∞; Uηj ∈ L; σj+1 = ∞)
j=1

P (Uηj ∈ L; σj+1 = ∞|σj = ∞).

(20)

j=1

Consider the process Un = Uσk +n on the event σk < ∞
(with the measure renormalized by P (σk < ∞)). This
process has the same properties (i)-(iii) as Un . Let J =
J
1
log2 ( β log1−δ 1/4) , then x2 ∈ L for any x ∈ C. Therefore,
P (UJ ∈ L) ≥ 2−J by property (ii). Now consider the process
UJ+n on the event UJ ∈ L. This process has properties (i)(iii), so we can use (18) to conclude that for

(16)

P (Uηk ∈ L; σk+1 = ∞|σk = ∞) ≥ 2−(J+1)

(b) Now we shall prove that P (Y∞ ∈ (δ, 1 −δ)) = 0 for any
2
2
δ > 0. From (ii) it follows that P (Xn+1 = Xn |Fn ) ≥ 1/2,
which implies that
2
P (Yn+1 = Yn |Fn ) ≥ 1/2 on Yn < 1/2

provided that U0 ≤

Moreover, if U0 ≤ (1/2)1/β then either Yn → 0 or Yn ≥ 1/2
for some n. This translates to

Convergence of this process is established in the following
lemma.

P (Y∞ ≥ 1/2) ≤ 2EY0 ≤ 1/2.

1
2

uniformly in k. But then the sum in (20) is equal to inﬁnity,
a contradiction.
(e) The proof is completed by showing that the probability
of Un staying in Rc = [0, 1]\R without converging to zero
is zero. We know that almost all trajectories stay outside C,
so suppose that the process starts in (0, (1/2)1/β ). Then the
probability that it enters L in a ﬁnite number of steps is
uniformly bounded from below (this is shown similarly to
(20)), so the probability that it does not go to L is zero. Next

(17)

for all n ≥ 0. Suppose that Y∞ takes values in (δ, 1/2 − δ)
with probability α > 0. Let An = {ω : Yn ∈ (δ, 1/2 − δ)}.
Since Yn → Y∞ a.e., the Egorov theorem implies that there
is a subset of probability arbitrarily close to P (An ) which

4

W (0|2) = W (1|1) = W (1|3) = 1. This channel has capacity
1 bit. Computing the channels W + and W − we ﬁnd that
they are equivalent to the original channel W . The conclusion
reached in [2] was that there are nonbinary channels that do
not polarize under the action of H2 .
We observe that the above channel corresponds to the extremal conﬁguration (10) in (5) (the other two conﬁgurations
arise with probability 0), and therefore is a stable point of the
channel combining operation. It is possible to reach capacity
by transmitting the least signiﬁcant bit of every symbol.

assume that the process starts in L, then by (19) it either goes
to zero or enters C with probability one. Together with part
(d) this implies that the process that starts in L converges to
zero or one with probability one.
˜
Lemma 5: Let V : X → Y be a channel. Let v, v ∈ X \{0}
be such that wtr (v) ≥ wtr (v ). For any δ > 0 there exists
δ > 0 such that Zv (V ) ≥ 1 − δ whenever Zv (V ) ≥ 1 − δ . In
particular, we can take δ = δ q −3 .
This lemma is proved by relating Z(Vx,x ) to the Eu˜
clidean distance between the vectors ( V (y|x), y ∈ Y) and
˜ and some geometric arguments [6].
( V (y|x ), y ∈ Y)

E. Rate of polarization and error probability of decoding
The main result in this part extends the argument of [1] to
nonbinary alphabets.

Lemma 6: For all j = 1, . . . , r
a.e.

(j)
(j)
Zmax,n −→ Zmax,∞ .

Theorem 3: Let 0 < α < 1/2. For any DMC W : X → Y
with I(W ) > 0 and any R < I(W ) there exists a sequence
of r-tuples of disjoint subsets A0,N , . . . , Ar−1,N of [N ] such
α
(i)
that k |Ak,N |(r − k) ≥ N R and Zv (WN ) < 2−N for all
r
i ∈ Ak,N , all v ∈ l=k+1 Xl , and all k = 0, 1, . . . , r − 1.
This theorem follows upon combining Lemma 4 and a result
of [7].

(j)

where Zmax,∞ is a Bernoulli random variable supported on
{0, 1}.
Convergence of the rv’s Zv,n is established in the following
lemma.
Lemma 7: Zv,n → Zv,∞ a.e., where Zv,∞ is a (0, 1)valued random variable whose distribution depends only on the
ordered weight wtr (v).

The next theorem gives an estimate of the block error rate.
Theorem 4: Let 0 < α < 1/2 and let 0 < R < I(W ),
where W : X → Y is a DMC. The best achievable error probability of block error under successive cancellation decoding at
α
block length N = 2n and rate R satisﬁes Pe = O(2−N ).

2) Proof of Part (b) of Theorem 1:
Lemma 8: For any i = 1, . . . , r, the random variable Zi,n
converges a.e. to a (0, 1)-valued random variable Zi,∞ . Moreover, Zi,∞ → 1 implies that Zi−1,∞ → 1 a.e.
The ﬁrst part follows because all the Zv , v ∈ Xi converge to
identical copies of the same random variable. The second part
holds true because Lemma 5 allows us to move from higherweight symbols v in Zv to lower-weight symbols [6].

F. Examples
A simple example is given by the ordered erasure channel,
deﬁned as Wr : Fr → (Fq ∪ {?})r , where
q

Now the proof of Theorem 1(b) can be completed as
follows. We obtain that Zi,∞ is a (0, 1) random variable a.e.
and for all i, and if Zi,∞ = 1 then Zj,∞ = 1 for all 1 ≤ j < i.
(j)
Consider the events Ψi = {ω : Zj,∞ = i}, i = 0, 1; j =
1, . . . , r. We have
(1)

(2)

(2)

(r)

Ψ1 ⊃ Ψ1 ⊃ · · · ⊃ Ψ1

Ψ0 ⊂ Ψ0 ⊂ · · · ⊂ Ψ0 .

ACKNOWLEDGMENT: The authors are grateful to Emmanuel Abbe, Eren Saso˘ lu, and Emre Telatar (EPFL), and
¸ ¸ g
Leonid Koralov, Armand Makowski, and Himanshu Tyagi
(UMD) for useful discussions of this work.

We need to prove that with probability one, the vector
(Zi,∞ , i = 1, . . . , r) takes one of the values (5). With
probability one Zr,∞ = 1 or 0. If it is equal to 1 then
necessarily Zr−1,∞ = · · · = Z1,∞ = 1. Otherwise Zr,∞ = 0.
In this case it is possible that Zr−1,∞ = 1 (in which case
Zr−2,∞ = · · · = Z1,∞ = 1) or Zr−1,∞ = 0. Of course
(r−1)
(r−1)
P (Ψ0
∪ Ψ1
) = 1, so in particular
(r)

(r−1)

P (Ψ0 \(Ψ0

(r−1)

∪ (Ψ1

ε0 , y = x,
εi , y = (?? . . .?xi+1 . . . xr ), 1 ≤ i ≤ r

and Wr (y|x) = 0 if y does not contain any erased bits and y =
x. Its capacity equals r− r iεi . We computed the behavior
i=1
of the virtual channels for n = 15 steps of the iteration and
observed polarization to r + 1 levels for several values of r.

(r)

(1)

Wr (y|x) =

R EFERENCES
[1] E. Arıkan, “Channel polarization: a method for constructing capacityachieving codes for symmetric binary-input memoryless channels,” IEEE
Trans. Inform. Theory, vol. 55, no. 7, pp. 3051–3073, 2009.
[2] E. Saso˘ lu, E. Telatar, and E. Arıkan, “Polarization for arbitrary discrete
¸ ¸ g
memoryless channels,” arXiv:0908.0302.
[3] R. Mori and T. Tanaka, “Channel polarization on q-ary discrete memoryless channels by arbitrary kernels,” in Proc. IEEE Internat. Sympos.
Inform. Theory, Austin, TX, pp. 894–898, 2010.
[4] E. Abbe and E. Telatar, “Polar codes for the m-user MAC,”
arXiv:1002.0777.
[5] A. G. Sahebi and S. S. Pradhan, “Multilevel polarization of polar codes
over arbitrary discrete memoryless channels,” Proc. Alleron Conf. 2011.
[6] W. Park and A. Barg, “Polar codes for q-ary channels, q = 2r ,”
arXiv:1107.4965v3, January 2012.
[7] E. Arıkan and E. Telatar, “On the rate of channel polarization,” in Proc.
IEEE Int. Sympos. Inform. Theory, Seoul, Korea, June 28–July 3, 2009,
pp. 1493–1495, 2009.

(r)

\Ψ1 ))) = 0.

If Zr−1,∞ = 0 then the possibilities are Zr−2,∞ = 1 or 0, up
to another event of probability 0, and so on. Thus, the union
of the disjoint events given by (5) holds with probability one.
These arguments complete the proof.
Remark : Saso˘ lu et al. [2] note that the symbols can
¸ ¸ g
polarize to states that carry partial information about the transmission. In particular, they give an example of a quaternaryinput channel W : {0, 1, 2, 3} → {0, 1} with W (0|0) =

5


Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sun Apr 22 11:54:32 2012
ModDate:        Tue Jun 19 12:55:17 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      359913 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569551905

Perfectly Secure Encryption of Individual Sequences
Neri Merhav
Department of Electrical Engineering
Technion – Israel Institute of Technology
Technion City, Haifa 32000, Israel
Email: merhav@ee.technion.ac.il

perfectly secure encryption in that sense, cannot be smaller (up
to asymptotically vanishing terms) than the Lempel–Ziv (LZ)
complexity of the plaintext source [19]. This lower bound is
obviously asymptotically achieved by one–time pad encryption
of the bit-stream obtained by LZ data compression of the
plaintext source.
In this work, we also consider encryption of individual
sequences, but our modeling approach and the deﬁnition of
perfect secrecy are different. Rather than assuming that the
encrypter and decrypter have unlimited resources, and that it
is the eavesdropper which has limited resources, modeled in
terms of FSMs, in our setting, the converse is true. We adopt a
model of a ﬁnite–state encrypter, which receives as inputs the
plaintext stream and the secret key bitstream, and it produces
a ciphertext. Based on this model, we deﬁne a notion of ﬁnite–
state encryptability (in analogy to the notions of ﬁnite–state
compressibility [19] and the ﬁnite–state predictability [2]),
as the minimum achievable rate at which key bits must be
consumed by any ﬁnite–state encrypter in order to guarantee
perfect security, while keeping the cryptogram decipherable
at the legitimate receiver. Our main result is that the ﬁnite–
state encryptability is equal to the ﬁnite–state compressibility,
similarly as in [14].
More precisely, denoting by c(xn ) the number of LZ phrases
associated with the plaintext xn = (x1 , . . . , xn ), we show that
the number of key bits required by any encrypter with s states,
normalized by n cannot be smaller than [c(xn ) log c(xn )]/n −
δs (n), where δs (n) = O(s log(log n)/ log n). On the other
hand, this bound is obviously essentially achievable by applying the LZ ‘78 algorithm [19], followed by one–time
pad encryption, since the compression ratio of the LZ ‘78
algorithm is also [c(xn ) log c(xn )]/n, up to vanishingly small
terms. It follows then that the ﬁnite–state encryptability of
every (inﬁnite) individual sequence is equal to its ﬁnite–state
compressibility.
While the idea of LZ data compression, followed by one–
time padding is rather straightforward, our main result, that
no ﬁnite–state encrypter can do better than that for any
given individual sequence, may not be quite obvious since
the operations of compression and encryption are basically
different – secret key encryption need not necessarily be based
on compression followed by one–time padding, deﬁnitely not
if both operations are formalized in the framework of ﬁnite–
state machines.
For ﬁnite sequences of length n, the difference between

Abstract—In analogy to the well–known notion of ﬁnite–state
compressibility of individual sequences, due to Lempel and Ziv,
we deﬁne a similar notion of “ﬁnite–state encryptability” of an
individual plaintext sequence, as the minimum asymptotic key
rate that must be consumed by ﬁnite–state encrypters so as
to guarantee perfect secrecy in a well–deﬁned sense. Our main
basic result is that the ﬁnite–state encryptability is equal to the
ﬁnite–state compressibility for every individual sequence. This
is in parallelism to Shannon’s classical probabilistic counterpart
result, asserting that the minimum required key rate is equal
to the entropy rate of the source. However, the redundancy,
deﬁned as the gap between the upper bound (direct part) and
the lower bound (converse part) in the encryption problem, turns
out to decay at a different rate (in fact, much slower) than the
analogous redundancy associated with the compression problem.
We also extend our main theorem, allowing: (i) availability of
side information (SI) at the encrypter/decrypter/eavesdropper,
(ii) lossy reconstruction at the decrypter.

I. I NTRODUCTION
The paradigm of individual sequences and ﬁnite–state machines (FSMs), as an alternative to the traditional probabilistic
modeling of sources and channels, has been studied and
explored quite extensively in several information–theoretic
problem areas, including data compression [9], [13], [15], [16],
[19], source/channel simulation [5], [10], classiﬁcation [18],
[20], prediction [2], [21], denoising [11], and even channel
coding [4], just to name a few representative references. On
the other hand, it is fairly safe to say that the entire literature
on information—theoretic security (see, e.g., [3], [6], [12] for
surveys as well as references therein), is based exclusively on
the probabilistic setting.
To the best of our knowledge, the only exception to this
rule is [14]. In that work, the plaintext source to be encrypted,
using a secret key, is an individual sequence, the encrypter
is a general encoder, and the eavesdropper employs an FSM
as a message discriminator. Speciﬁcally, it is postulated in
[14] that the eavesdropper has prior knowledge that can be
expressed in terms of the existence of some set of “acceptable
messages” that constitutes the a-priori level of uncertainty
that the eavesdropper has concerning the plaintext message.
Next, it is assumed that there exists an FSM that can test
whether a given candidate plaintext message is acceptable: Iff
the FSM produces the all–zero sequence in response to that
message, then this message is acceptable. Perfect security is
then deﬁned as a situation where the size of the acceptance
set is not reduced in the presence of the cryptogram. The
main result in [14] is that the asymptotic key rate needed for

1

the upper bound (of the direct part) and the lower bound
(of the converse part), which can be thought of as some
notion of redundancy, is again O(s log(log n)/ log n), which
decays much more slowly than the corresponding redundancy
in data compression [19, Theorems 1, 2], which is roughly
O((log s)/ log n).
Finally, we extend our main basic theorem in two directions. The ﬁrst is in allowing availability of side information
(SI) at all three parties (encrypter, legitimate decrypter and
eavesdropper) or at the decrypter and the eavesdropper only.
We assume that the SI sequence is an individual sequence
as well. We initially assume that it is the same SI that is
available to all three parties in the ﬁrst case or to both the
legitimate decrypter and the eavesdropper, in the second case.
Our main result is essentially unaltered, except that the LZ
∆
complexity, ρLZ (xn ) = [c(xn ) log c(xn )]/n, is replaced by
the conditional LZ complexity given the SI, to be deﬁned
later (see also [7], [17]). Our second extension is to the
case where lossy reconstruction is allowed at the legitimate
receiver (ﬁrst, without SI). Here the LZ complexity is replaced
by a notion of “LZ rate–distortion function,” rLZ (D; xn ),
which means the smallest LZ complexity among all sequences
that are within the allowed distortion relative to the input
plaintext sequence. While our framework allows randomized
reconstruction sequences (that may depend on the random
key), we ﬁnd that at least asymptotically, there is nothing to
gain from this degree of freedom, as optimum performance
can be achieved by a scheme that generates deterministic
reproductions. Finally, we make a few short comments on the
case where both SI and lossy reconstruction are allowed at the
same time.
It should be pointed out that throughout the entire paper,
most of our emphasis is on converse theorems (lower bounds).
The compatible direct parts (upper bounds) will always be
attainable by a straightforward application of the suitable data
compression scheme, followed by one–time padding.

notation standards. Information theoretic quantities, like entropies and mutual informations, will be denoted following
the usual conventions of the information theory literature, e.g.,
H(K m ), I(W ; X m |S m ), and so on.
A ﬁnite–state encrypter is deﬁned by a sixtuplet E =
(X , Y, Z, f, g, ∆), where X is a ﬁnite input alphabet of size
|X | = α, Y is a ﬁnite set of binary words, Z is a ﬁnite
set of states, f : Z × X × {0, 1}∗ → Y is the output
function, g : Z × X → Z is the next–state function,
∆ : Z × X → {0, 1, 2, . . .}, and {0, 1}∗ is the set of all binary
strings of ﬁnite length. The set Y is allowed to contain binary
strings of various lengths, including the null word λ (whose
length is zero). When two inﬁnite sequences, x = x1 , x2 , . . .,
xi ∈ X , and u = u1 , u2 , . . ., ui ∈ {0, 1}, i = 1, 2, . . . are fed
into E, it produces an inﬁnite output sequence y = y1 , y2 , . . .,
yi ∈ Y, while passing through an inﬁnite sequence of states
z = z1 , z2 , . . ., zi ∈ Z, according to the following recursive
equations, implemented for i = 1, 2, . . .
= ti−1 + ∆(zi , xi ),

∆

t0 = 0

ti
ki

=

yi

= f (zi , xi , ki )

(3)

= g(zi , xi )

(4)

zi+1

(uti−1 +1 , uti−1 +2 , . . . , uti )

(1)
(2)

where it is understood that if ∆(zi , xi ) = 0, then ki = λ, the
null word of length zero, namely, no key bits are used in the
i–th step. By the same token, if yi = λ, no output is produced
at this step, i.e., the system is idling and only the state evolves
in response to the input. An encrypter with s states E, is one
with |Z| = s. It is assumed that x is deterministic (i.e., an
individual sequence), whereas u is purely random, i.e., for
every positive integer n, PU n (un ) = 2−n .
By f (z1 , xn , k n ), we refer to the vector y n produced by E
in response to the inputs xn and k n when the initial state is
z1 . Similarly, the notation g(z1 , xn ) will mean the state zn+1
n
and ∆(z1 , xn ) will designate i=1 ∆(zi , xi ) under the same
circumstances. An encrypter E deﬁned as perfectly secure if
for every positive integer n and for every x ∈ X ∞ and y n ∈
Y n , the probability Pr{Y n = y n |x} is independent of x.
An encrypter is deﬁned as information lossless (IL) if for
every z1 ∈ Z, every sufﬁciently large1 n and all xn ∈ X n
and k n ∈ Kn , the quadruple (z1 , k n , f (z1 , xn , k n ), g(z1 , xn ))
uniquely determines xn . Given E and xn , the encryption key
rate of xn w.r.t. E is deﬁned as

II. N OTATION AND P ROBLEM F ORMULATION
Throughout this paper, scalar random variables (RV’s) will
be denoted by capital letters, their sample values will be
denoted by the respective lower case letters, and their alphabets will be denoted by the respective calligraphic letters.
A similar convention will apply to random vectors and their
sample values, which will be denoted with same symbols
superscripted by the dimension. Thus, for example, Am (m –
positive integer) will denote a random m-vector (A1 , ..., Am ),
and am = (a1 , ..., am ) is a speciﬁc vector value in Am , the m–
th Cartesian power of A. The notations aj and Aj , where i and
i
i
j are integers and i ≤ j, will designate segments (ai , . . . , aj )
and (Ai , . . . , Aj ), respectively, where for i = 1, the subscript
will be omitted (as above). For i > j, aj (or Aj ) will be
i
i
understood as the null string.
Sources and channels will be denoted generically by the
letter P or Q, subscripted by the name of the RV and its
conditioning, if applicable, exactly like in ordinary textbook

∆

σE (xn ) =

(k n )
1
=
n
n

n

(ki ),

(5)

i=1

where (ki ) = ∆(zi , xi ) is the length of the binary string ki
n
and (k n ) = i=1 (ki ) is the total length of k n .
The set of all perfectly secure, IL encrypters {E} with no
1 It should be pointed out that this deﬁnition of information losslessness
is more relaxed (and hence more general) than the deﬁnition in [19]. While
in [19], the requirement is imposed for every positive integer n, here it is
required only for all sufﬁciently large n.

2

more than s states will be denoted by E(s). Next deﬁne
σs (xn ) = min σE (xn ),

(6)

σs (x) = lim sup σs (xn ),

1). Moreover, it is possible to relax this requirement even
further by allowing a relatively small uncertainty in xn given
(z1 , k n , f (z1 , xn , k n ), g(z1 , xn )) (see Subsection IV-B), at the
possible cost of further slowing down the convergence of
δs (n).
Partial Proof. Let m divide n and consider the partition of
xn into n/m non–overlapping m–vectors x1 , x2 , . . . , xn/m ,
where xi = xim
(i−1)m+1 . Recall that for a given z(i−1)m+1
im
and xi , the length li of ki = k(i−1)m+1 is uniquely determined as li = ∆(zi , xim
(i−1)m+1 ). Let us now deﬁne a
joint empirical distribution of several variables. For every
am ∈ X m , z, z ∈ Z, and every positive integer l, let us
deﬁne PX m ZZ L (am , z, z , l) as the relative frequency of the
m
combination xim
(i−1)m+1 = a , z(i−1)m+1 = z, zim+1 = z
m
and ∆(z, a ) = l, as i = 1, 2, . . . , n/m. Now, deﬁne

(7)

E∈E(s)

n→∞

and ﬁnally, deﬁne the ﬁnite–state encryptability of x as
σ(x) = lim σs (x).
s→∞

(8)

Our purpose it to characterize these quantities and to point out
how they can be achieved in principle.
III. M AIN R ESULT
Incremental parsing [19] of a string xn is a sequential
procedure of parsing xn into distinct phrases, where each new
phrase is the shortest string that has not been encountered
before as a phrase of xn , with the possible exception of the
last phrase that might be incomplete. Let c(xn ) denote the
number of phrases in LZ incremental parsing of xn . The LZ
complexity of xn is deﬁned as

PK m X m Y m ZZ L (κm , am , bm , z, z , l)
=

2−l PX m ZZ L (am , z, z , l) · 1{bm = f (z, am , κm )}

Throughout this proof, all information measures are deﬁned
w.r.t. PK m X m Y m ZZ L . Consider the following chain of equalities for the given xn and an arbitrary encrypter E ∈ E(s):

n
n
∆ c(x ) log c(x )
ρLZ (xn ) =
.
(9)
n
The ﬁnite–state compressibility, ρ(x), of x = (x1 , x2 , . . .) is
deﬁned, in [19], as the best compression ratio achieved by IL
ﬁnite–state encoders, analogously to the above deﬁnition of
ﬁnite–state encryptability. From Theorems 1, 2 and 3 of [19],
∆
it follows that ρLZ (x) = lim supn→∞ ρLZ (xn ) is equal to
ρ(x).
The following theorem establishes a lower bound on σs (xn )
in terms of ρLZ (xn ) and hence a lower bound of σ(x) in terms
of ρ(x).
Theorem 1: For every xn , σs (xn ) ≥ ρLZ (xn ) − δs (n),
where δs (n) is independent of xn and behaves according
to δs (n) = O(s log(log n)/ log n). Consequently, σ(x) ≥
ρ(x).
Discussion. A few comments on Theorem 1 are in order.

n

σE (x )

=

=

1 m
(k n )
=
·
n
m n
1 m
·
m n

n/m
im
(k(i−1)m+1 )
i=1

n/m
im
H(K(i−1)m+1 )
i=1

H(K m |L)
=
.
(10)
m
Note that the length of the key for the i-th m–
block, li =
(ki ) = ∆(z(i−1)m+1 , xim
=
(i−1)m+1 )
im
t=(i−1)m+1 ∆(zt , xt ), is a variable that may take on no
more than (m + 1)αs−1 different values,2 and hence the
same is true concerning the random variable L, and so,
H(L) ≤ (αs − 1) log(m + 1). Thus,

1. It is readily observed that a compatible direct theorem
holds, simply by applying the LZ ‘78 algorithm followed
by one–time pad encryption of the compressed bits. The
1
resulting key–rate needed is then upper bounded by n [c(xn )+
n
1] log[2α(c(x ) + 1)], following [19, Theorem 2], which is,
within negligible terms, equal to ρLZ (xn ). Thus, σ(x) =
ρ(x).

σE (xn )

2. Consider the difference between the upper bound pertaining to the direct part (as mentioned in item no. 1
above) and the lower bound of the converse √
part. The behavior of this difference is O(αs log(log n)/ log n). This
behavior is different from the behavior of the corresponding
gap in compression (Theorems 1 and 2 in [19]), which is
O([log(2α)] log(8αs2 )/ log n). The guaranteed convergence
to optimality is therefore considerably slower in the encryption
problem.

Now, for all large

=
=
≥
≥

1
H(K m |L)
m
1
[H(K m ) − I(K m ; L)]
m
1
[H(K m ) − H(L)]
m
1
[H(K m ) − (αs − 1) log(m + 1)]. (11)
m
m,

H(K m ) ≥ H(K m |Y m )
≥ I(K m ; X m |Y m )
= H(X m |Y m ) − H(X m |Y m , K m )
= H(X m ) − H(X m |Y m , K m )
2 To see why this is true, observe that the sum that deﬁnes l depends on
i
im
xi = xim
and z i = z(i−1)m+1 only via the joint type class of pairs
(i−1)m+1
(x, z) ∈ X × Z, associated with (xi , z i ). Thus, the number of different
values that li may take cannot exceed the total number of such type classes,
which in turn is upper bounded by (m + 1)αs−1 .

3. We already mentioned that the deﬁnition of the IL property
here is somewhat more relaxed than in [19] (see footnote no.

3

= H(X m ) − H(X m |Y m , K m , Z, Z ) −
m

m

s = (s1 , s2 , . . .), si ∈ S, i = 1, 2, . . ., where S is a ﬁnite
alphabet. Assume ﬁrst that all three parties (encoder, decoder,
and eavesdropper) have access to s. In the formal model
deﬁnition, a few modiﬁcations are needed: (i) In eqs. (1),
(3), and (4), the functions ∆, f and g should be allowed to
depend on the additional argument si , (ii) The deﬁnition of
perfect security should allow conditioning on s, in addition
to the present conditioning on x. I.e., Pr{Y n = y n |x, s} is
independent of x for every positive integer n (but it is allowed
to depend on s). (iii) In the deﬁnition of an IL encrypter, the
quadruple (z, k n , f (z, xn , k n ), g(z, xn )) should be extended
to be the quintuple (z, k n , sn , f (z, xn , k n ), g(z, xn )).
In Theorem 1, the LZ complexity of xn , should be replaced
by the conditional LZ complexity of xn given sn , denoted
ρLZ (xn |sn ), which is an empirical measure of conditional
entropy (or conditional compressibility), that is deﬁned as
follows (see also [7], [17]): Given xn and sn , let us apply
the incremental parsing procedure of the LZ algorithm to the
sequence of pairs ((x1 , s1 ), (x2 , s2 ), . . . , (xn , sn )). According
to this procedure, all phrases are distinct with a possible
exception of the last phrase, which might be incomplete. Let
c(xn , sn ) denote the number of distinct phrases. For example,3
if

m

I(Z, Z ; X |Y , K )
= H(X m ) − 0 − I(Z, Z ; X m |Y m , K m )
≥ H(X m ) − H(Z, Z |Y m , K m )
≥ H(X m ) − 2 log s,

(12)

where the second equality is due to the perfect security
assumption and the third equality is due to the IL property,
assuming that m is sufﬁciently large. Thus, combining eqs.
(11) and (12), we obtain
log(m + 1)
H(X m ) 2 log s
−
− (αs − 1) ·
. (13)
m
m
m
Now, the main term, H(X m )/m, is nothing but the normalized
m–th order empirical entropy associated with xn . Next, in
order to get rid from the dependence of this main term on m,
we further lower bound it in terms of ρLZ (xn ) at the (small)
price of reducing the bound further by additional terms that
will be shown later to be negligible. In particular, in [8], we
prove the following inequality:
σE (xn ) ≥

c(xn ) log c(xn ) 2m(log α + 1)2
−
−
n
(1 − n ) log n
1
2mα2m log α
− .
(14)
n
m
→ 0 as n → ∞. Combining this with eq. (13), we

H(X m )
m

where
get

n

≥

c(xn ) log c(xn )
− δs (n, m)
σE (x ) ≥
n
n

x6
s6

=

0 | 1 | 0 0 | 0 1|
0 | 1 | 0 1 | 0 1|

then c(x6 , s6 ) = 4. Let c(sn ) denote the resulting number of
distinct phrases of sn , and let s(l) denote the lth distinct s–
phrase, l = 1, 2, ..., c(sn ). In the above example, c(s6 ) = 3.
Denote by cl (xn |sn ) the number of occurrences of s(l) in the
parsing of sn , or equivalently, the number of distinct x-phrases
c(sn )
n n
that jointly appear with s(l). Clearly,
l=1 cl (x |s ) =
c(xn , sn ). In the above example, s(1) = 0, s(2) = 1,
s(3) = 01, c1 (x6 |s6 ) = c2 (x6 |s6 ) = 1, and c3 (x6 |s6 ) = 2.
Now, the conditional LZ complexity of xn given sn is deﬁned
as

(15)

where
δs (n, m)

=
=

log(m + 1)
2 log s
+ (αs − 1) ·
+
m
m
2
2m
2m(log α + 1)
2mα log α
1
+
+ .
(1 − n ) log n
n
m

We now have the freedom to let m = mn grow slowly
enough as a function of n such that δs (n) = δs (n, mn ) will
vanish for every ﬁxed s. By letting mn be proportional to
(log n) log(log n), δs (n) becomes O(s log(log n)/ log n).
Note that the ﬁrst two terms of δs (n, m) come from considerations pertaining to encryption, whereas the other terms
appear also in compression. The second term turns out to
be the dominant one, which means that in the encryption
problem we end up with slower decay of the redundancy.
If we compare the difference between the upper bound and
the lower bound in compression (coding them and converse
in [19]), this difference is dominated by a term that is
O(([log(2α)] log(8αs2 )/ log n), whereas in encryption the difference is O(αs log(log n)/ log n), namely, a signiﬁcantly
slower decay rate.

1
ρLZ (x |s ) =
n
n

c(sn )

n

cl (xn |sn ) log cl (xn |sn ).

(16)

l=1

The direct is obtained by ﬁrst, compressing xn to about
n · ρLZ (xn |sn ) bits using the conditional parsing scheme
[17, Lemma 2, eq. (A.11)] and then applying one–time pad
encryption.
The same performance can be achieved even if the encrypter
does not have access to sn , by using a scheme in the spirit
of Slepian–Wolf coding: Randomly assign to each member
of X n a bin, selected independently at random across the
set {1, 2, . . . , 2nR }. The encrypter applies one–time pad to
the (nR)–bit binary representation of the bin index of xn .
The decrypter, ﬁrst decrypts the bin index using the key
and then seeks a sequence xn within the given bin, which
ˆ
satisﬁes ρLZ (ˆn |sn ) < R − . If there is one and only
x
one such sequence, then it becomes the decoded message,

IV. E XTENSIONS
A. Availability of Side Information
Consider the case where SI is available at the encrypter/decrypter/eavesdropper. Suppose that, in addition to
the source sequence x, there is an (individual) SI sequence

3 The

4

same example appears in [17].

otherwise an error is declared. This scheme works, just like
the ordinary SW coding scheme, because the number of
{ˆn } for which ρLZ (ˆn |sn ) < R − does not exceed
x
x
2n[R− +O(log(log n)/ log n)] [17, Lemma 2]. The weakness of
this is that prior knowledge of (a tight upper bound on)
ρLZ (xn |sn ) is required. If, for example, it is known that xn
is a noisy version of sn , generated, say, by a known additive
channel, then R should be essentially the entropy rate of the
noise.

The simultaneous extension of Theorem 1, allowing both
distortion D and SI sn leads, with the obvious modiﬁcations,
to min{ρLZ (ˆn |sn ) : d(xn , xn ) ≤ nD}, whose achievability
x
ˆ
is conceptually straightforward when all parties have access
to sn , including the encrypter. But what if the encrypter does
not have access to sn ? Due to space limitations, we will
not provide detailed results concerning this case, except for
a comment that for the case where the legitimate decrypter
is also based on an FSM, it can be handled using the same
line of thought as in [9] (and references therein), showing that
the best general block code of length m is at least as good as
any s–state encrypter–decrypter, up to a redundancy term that
tends to zero as m → ∞ for every ﬁxed s (see [8]).

B. Lossy Reconstruction
Suppose that we are content with a lossy reconstruction,
xn , at the legitimate receiver. In general, this reconstruction
ˆ
may be a random vector due to possible dependence on the
random key bits. It is required, however, that d(xn , xn ) ≤ nD
ˆ
with probability one, for some distortion measure d. Then,
in Theorem 1, ρLZ (xn ) should be replaced by the “LZ rate–
distortion function” of xn , which is deﬁned as
∆

rLZ (D; xn ) =

min

{ˆn : d(xn ,ˆn )≤nD}
x
x

ρLZ (ˆn ).
x

R EFERENCES
[1] T. M. Cover and J. A. Thomas, Elements of Information Theory, John
Wiley & Sons, Hoboken, New Jersey, U.S.A., 2006.
[2] M. Feder, N. Merhav, and M. Gutman, “Universal prediction of individual
sequences,” IEEE Trans. Inform. Theory, vol. 38, no. 4, pp. 1258–1270,
July 1992.
[3] Y. Liang, H. V. Poor and S. Shamai (Shitz), “Information theoretic
security,” Foundations and Trends in Communications and Information
Theory, vol. 5, no. 4–5 pp. 355–580. 2009.
[4] Y. Lomnitz and M. Feder, “Universal communication over modulo–
additive channels with an individual noise sequence,” arXiv:1012.2751v1
[cs.IT] 13 Dec 2010.
[5] A. Mart´n, N. Merhav, G. Seroussi, and M. J. Weinberger, “Twice–
ı
universal simulation of Markov sources and individual sequences,” IEEE
Trans. Inform. Theory, vol. 56, no. 9, pp. 4245–4255, September 2010.
[6] J. L. Massey, “An introduction to contemporary cryptology,” Proc. IEEE,
vol. 76, vol. 5, pp. 533–549, May 1988.
[7] N. Merhav, “Universal detection of messages via ﬁnite–state channels,”
IEEE Trans. Inform. Theory, vol. 46, no. 6, pp. 2242–2246, September
2000.
[8] N. Merhav, “Perfectly secure encryption of individual sequences,”
http://arxiv.org/PS cache/arxiv/pdf/1112/1112.2262v2.pdf
[9] N. Merhav and J. Ziv, “On the Wyner–Ziv problem for individual
sequences,” IEEE Trans. Inform. Theory, vol. 52, no. 3, pp. 867–873,
March 2006.
[10] G. Seroussi, “On universal types,” IEEE Trans. Inform. Theory, vol. 52,
no. 1, pp. 171–189, January 2006.
[11] T. Weissman, E. Ordentlich, G. Seroussi, S. Verd´ , and M. J. Weinberger,
u
“Universal denoising: known channel,” IEEE Trans. Inform. Theory, vol.
51, no. 1, pp. 5–28,January 2005.
[12] H. Yamamoto, “Information theory in cryptology,” IEICE Trans.,
vol. E74, no. 9, pp. 2456–2464, September 1991.
[13] J. Ziv, “Coding theorems for individual sequences,” IEEE Trans. Inform. Theory, vol. IT–24, no. 4, pp. 405–412, July 1978.
[14] J. Ziv, “Perfect secrecy for individual sequences,” unpublished
manuscript, 1978.
[15] J. Ziv, “Distortion–rate theory for individual sequences,” IEEE Trans. Inform. Theory, vol. IT–26, no. 2, pp. 137–143, March 1980.
[16] J. Ziv, “Fixed-rate encoding of individual sequences with side information”, IEEE Transactions on Information Theory, vol. IT–30, no. 2,
pp. 348–452, March 1984.
[17] J. Ziv, “Universal decoding for ﬁnite-state channels,” IEEE Trans. Inform. Theory, vol. IT–31, no. 4, pp. 453–460, July 1985.
[18] J. Ziv, “Compression, tests for randomness, and estimating the statistical
model of an individual sequence,” Proc. Sequences, R. M. Capocelli Ed.,
New York: Springer Verlag, pp. 366–373, 1990.
[19] J. Ziv and A. Lempel, “Compression of individual sequences via
variable-rate coding,” IEEE Trans. Inform. Theory, vol. IT–24, no. 5,
pp. 530–536, September 1978.
[20] J. Ziv and N. Merhav, “A measure of relative entropy between individual sequences with application to universal classiﬁcation,” IEEE Trans.
Inform. Theory, vol. 39, no. 4, pp. 1270–1279, July 1993.
[21] J. Ziv and N. Merhav, “On context–tree prediction of individual sequences,” IEEE Trans. Inform. Theory, vol. 53, no. 5, pp. 1860–1866,
May 2007.

(17)

The deﬁnition of the IL property can be slightly relaxed to a notion of “nearly IL” (NIL) property, which
allows recovery with small uncertainty for all large
enough n. In particular, we shall assume that given
∆
i+n
n+i−1
n+i−1
w = (zi , ki , f (zi , xn+i−1 , ki
), g(zi , xi
)), xi
ˆn+i−1
i
ˆ
must lie, with probability one, in a subset An (w) ⊂ X n ,
4
where
1
∆
log max |An (w)| = 0.
(18)
ηn = lim
w
n→∞ n
Perfect security should be deﬁned as statistical independence
between the cryptogram and both the source and reconstruction, i.e., the probability of any segment of {yi } should not
ˆ
depend on either x or x.
Again, the direct is obvious, and it implies that at least
asymptotically, there is nothing to gain from randomizing the
reconstruction: The best choice of xn is the one with minimum
ˆ
LZ complexity within the sphere of radius nD around xn .
This conclusion is not obvious a–priori as one might speculate
that a randomized reconstruction, depending on the key, may
potentially be more secure than a deterministic one.
Note that we have not assumed anything on the distortion
measure d, not even additivity. Another difference between
Theorem 1 of the lossless case and its present extension to
the lossy case, is that we are know longer able to characterize
the rate of convergence of δs (n), as it depends on the rate of
decay of ηm . In fact, we could have replaced the IL property
we assumed in the lossless case by the NIL property there too,
but again, the cost would be the loss of the ability to specify
the behavior of δn .
4 This might be the case if unambiguous reconstruction of xn+i−1 requires
ˆi
additional information from times later than t = n+i−1. For example, if the
n is deterministic, and n
encrypter works in blocks of ﬁxed size m, x
ˆ
m,
then by viewing the block code as ﬁnite–state machine as before, there might
be uncertainty in not more than the m last symbols of xn in case the last block
ˆ
is incomplete (e.g., when m does not divide n or the n–block considered is
ˆ
not synchronized to the m–blocks). In this case, |An (w)| ≤ |X |m , which is
ﬁxed, independent of n, and so ηn = O(1/n).

5


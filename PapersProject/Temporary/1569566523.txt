Title:          Paper_ver7.dvi
Creator:        dvips(k) 5.95a Copyright 2005 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Tue May 15 23:45:42 2012
ModDate:        Tue Jun 19 12:54:20 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      314936 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566523

Distributed Estimation in Multi-Agent Networks
Lalitha Sankar and H. Vincent Poor
Dept. of Electrical Engineering,
Princeton University, Princeton, NJ 08544.
{lalitha,poor}@princeton.edu
(measurements at the other agent) aware Wyner-Ziv encoding
[2] at each agent achieves both the minimal rate and the
minimal leakage for every choice of ﬁdelity (quantiﬁed via
mean-squared distortion).
Even without additional privacy constraints, the problem of
determining the set of all rate-distortion tuples in a multi agent
network is related to the distributed source coding problem
[3], [4] which remains open. Furthermore, for a relatively
simpler setting obtained by assuming that a central entity,
often referred to as a chief executive ofﬁcer (CEO), wishes
to estimate the states Xk , for all k, from the transmissions of
all agents, we obtain a multi-variate (vector) Gaussian CEO
problem which also remains open except for speciﬁc cases [5].
Circumventing these challenges, we focus on the ratedistortion-leakage behavior in the limit of large K for a
distributed protocol in which each agent encodes its measurements taking into account the prior broadcasts of the other
agents (henceforth referred to as progressive encoding) as well
as the side-information at the other agents. We compare the
performance of this protocol with a centralized protocol in
which the agents broadcast their encoded messages as if to
a virtual CEO. We consider a noisy Gaussian measurement
model at each agent with the same level of interference from
the states of the other agents. For this symmetric model, our
results demonstrate that the sum-rate achieved by distributed
protocol outperforms that for the centralized schemes with
asymptotic convergence with K. We also prove the sufﬁciency
of encoding local measurements for both protocols and present
outer bounds for the per user rate and leakage.
The paper is organized as follows. We introduce the model
and communication protocols in Section II. In Section III we
develop the achievable rate-distortion-leakage tuples for both
protocols as well as outer bounds. We conclude in Section IV.

Abstract— A problem of distributed state estimation at multiple agents that are physically connected and have competitive
interests is mapped to a distributed source coding problem with
additional privacy constraints. The agents interact to estimate
their own states to a desired ﬁdelity from their (sensor) measurements which are functions of both the local state and the states at
the other agents. For a Gaussian state and measurement model,
it is shown that the sum-rate achieved by a distributed protocol
in which the agents broadcast to one another is a lower bound on
that of a centralized protocol in which the agents broadcast as if
to a virtual CEO converging only in the limit of a large number
of agents. The sufﬁciency of encoding using local measurements
is also proved for both protocols.

I. I NTRODUCTION
We consider a network of K distributed agents in which
each agent observes sensor measurements from a distinct
part of a large interconnected physical network. Examples of
such networks include cyber-physical systems, speciﬁcally the
smart grid, in which an agent can be viewed as a regional
operator whose power measurements are affected by those at
other agents due to the physical grid connectivity. Agent k is
interested in estimating the state (deﬁned as a set of system
parameters; for e.g., voltages and phases in the electric grid)
of its local network from its measurements, Yk , which are a
function of both the local state Xk and the states Xl , l = k,
l, k ∈ {1, 2, . . . , K} of other agents in the network where the
states Xk are assumed to be independent of each other.
Estimating Xk at agent k with high ﬁdelity requires the
agents to interact and share data amongst themselves. While
the estimate ﬁdelity is crucial to the control decisions made
by the agents, in many distributed systems, for competitive
reasons, the agents wish to keep their state information private.
This leads to a problem of competitive privacy which captures
the tradeoff between the utility to the agent (estimate ﬁdelity)
that can be achieved via cooperation and the resulting privacy
leakage (quantiﬁed via mutual information).
Mapping utility to distortion and privacy to leakage quantiﬁed via mutual information, one can abstract the competitive
privacy problem as a distributed source coding problem with
additional leakage constraints. The set of all achievable rateﬁdelity-leakage tuples determines the utility-privacy tradeoff
region. In [1], we introduced and studied this problem for a
two-agent interactive system with Gaussian states and noisy
Gaussian measurements. We proved that side-information

II. P RELIMINARIES
A. Model and Metrics
We consider a network of K agents such that, at any time
instant i, i = 1, 2, . . . , n, the measurement Yk,i at agent k, k =
1, 2, . . . , K, is related to the states Xm,i , m = 1, 2, . . . , K, at
the agents as follows:
K

Yk,i = Xk,i +

√

hXl,i + Zk,i , k = 1, 2, . . . , K, (1)

l=1,l=k

The research was supported in part by the AFOSR under MURI Grant
FA9550-09-1-0643 and in part by the NSF under Grant CCF-10-16671.

where the state variables Xm,i ∼ N (0, σ 2 ), for all m and
i are assumed to be independent and identically distributed

1

(i.i.d.) and are also independent of the i.i.d. noise variables
Zk,i ∼ N (0, 1). The coefﬁcient h > 0 is assumed to be ﬁxed
for all time and known at all agents. We assume that the k th
n
agent observes a sequence of n measurements Yk = [Yk,1
Yk,2 . . . Yk,n ], for all k, prior to communications.
Utility: For the continuous Gaussian distributed state and
measurements, a reasonable metric for utility at the k th agent
is the mean square error Dk between the original and the
n
ˆn
estimated state sequences Xk and Xk , respectively.
Privacy: The measurements at each agent in conjunction
with the quantized data shared by the other agents while
enabling accurate estimation also leaks information about the
other agents’ states. We capture this leakage using mutual
information.

Let Mk denotes the size of Jk . The expected distortion Dk at
the k th agent is given by

B. Communication Protocol

Deﬁnition 1: The utility-privacy tradeoff region is the set
(2)
(K)
(1)
(K−1)
of all (D1 , . . . , Dk , L1 , . . . , L1 , . . . , LK , . . . , LK
) for
which there exists a coding scheme given by (2)-(4) with parameters (n, K, M1 , M2 , D1 +ǫ, . . . , DK +ǫ, L1 +ǫ, . . . , LK +
ǫ) for n sufﬁciently large such that ǫ → 0 as n → ∞.

Dk =

k−1
l=1 Jl

→ Jk ,

(l)

Lk =

Fk :

· Ip ) →

ˆn
Xk ,

2

, k = 1, 2, . . . K,

(5)

1
n
I (Xk ; J1 , J2 , . . . , JK , Yln ) , for all k = l.
n

(6)

The communication rate of the k th agent is denoted by
Rk = n−1 log2 Mk , k = 1, 2, . . . , K.

(7)

III. M AIN R ESULTS
We use the following proposition, lemma, and function
deﬁnition in the sequel to compute the achievable distortions
and rates.
Proposition 1: For (column) vectors A and B, let KAA =
and KAB =
var (A) = E (A − E [A]) AT − E AT
denote the covariance and
E (A − E [A]) B T − E B T
cross-correlation matrices, respectively. The conditional variance E[var(A|B)] is then given as E[var(A|B)] = KAA −
−1
T
KAB KBB KAB .
Lemma 1: For a K × K symmetric Toeplitz matrix whose
diagonal entries are all a, and off-diagonal entries are all b the
(K−1)
determinant is (a + (K − 1) b) (a − b)
.
Proof: The determinant is obtained by the following two
operations: i) add columns 2-K to column 1, and ii) subtract
row 1 from each of the remaining rows.
Deﬁnition 2: For some α, β ∈ R+ , the function f1 (k, c) ≡
α + (k − 2) β − (k − 1) c varies over k ∈ [1, K] and c ∈ R+ .
A. Distortion

(2)

We assume that each agent has the same distortion constraint D. The distortion D at each agent ranges from a minimum achieved when it has perfect access to the measurements
at all agents to a maximum achieved when it estimates using
only its own measurements. From the symmetry of the model
in (1), the minimal (resp. maximal) distortion achieved at each
agent is the same. Let Dmin and Dmax denote the minimal
and maximal distortions, respectively, at each agent. For the
Gaussian model considered here with minimum mean square
error (MSE) constraints, we have

(3)

such that at the end of the K broadcasts, one from each agent,
the decoding function Fk at the k th agent (or the CEO) is
a mapping from the received message sets (both protocols)
and the measurements (the distributed procotol) to that of the
reconstructed sequence denoted as
n
J1 ×. . .×JK ×(Yk

i=1

ˆ
Xk,i − Xk,i
(l)

is the index set at the k th agent for mapping the measurement
sequence, and the prior communications (progressive encoding), via the encoder fk , k = 1, 2, . . . K, deﬁned as
n
fk : Yk × Ienc ·

n

The privacy leakage, Lk , about state k at agent l, l = k, is
given by

We assume that each agent broadcasts a function of its
measurements (distributed procotol) to all agents and they do
so in a round-robin fashion. We assume that all agents encode
in one of the following two ways: i) local encoding in which
each agent quantizes only its measurements; or ii) progressive
encoding in which each agent encodes and transmits taking
into account both its measurements and prior communications
from other agents. In both cases, the agents transmit at a rate
that takes into account the correlated measurements and prior
communications of other agents.
To better understand the advantage of the above distributed
procotol, we also consider the case where the agents broadcast
as if communicating with a virtual central operator, say CEO,
henceforth referred to as the centralized protocol. This may
be viewed as the case in which the computing power at the
agents is limited and the CEO shares with each agent its
received messages (which are then decoded at each agent). For
either protocol, the encoding can be either local or progressive.
Let Ip ∈ {0, 1} and Ienc ∈ {0, 1} be random variables
that denote the choice of protocols and encodings such that
Ip = 1 and Ip = 0 for the distributed and centralized protocol,
respectively, and Ienc = 1 and Ienc = 0 for the progressive
and local encoding, respectively.
Formally, the encoder at agent k maps its measurements to
an index set Jk where
Jk ≡ {1, 2, . . . , Jk } , k = 1, 2, . . . , K,

1
E
n

Dmin = E [var(X1 |Y1 Y2 . . . YK )] , and
Dmax = E [var(X1 |Y1 )] .

k = 1, 2, . . . , K. (4)

2

(8)
(9)

˜n
˜
Uk sequences are generated via an i.i.d distribution of Uk,i
˜1,i = Y1,i + Q1,i and for all k > 1,
for all i such that U
k−1
˜
˜
Uk,i = Yk,i + l=1 ak,l Ul,i +Qk,i where ak,l ∈ R, and Qk,i ∼
2
N 0, σQ is independent of Yk,i for all k = 1, 2, . . . , K, and
i = 1, 2, . . . , n.
The achievable distortion D at agent k as a result of
n
estimating its state using both its measurements Yk and the
˜ n , for all l = k, is such that D ∈
received sequences Ul
[Dmin , Dmax ] where Dmax is achieved when Uln = 0 for all
2
l and D = Dmin for σQ = 0. On the other hand, for the
local encoding scheme, let Uk,i = Yk,i + Qk,i , for all k and
i, such that agent k maps only its measurement sequences to
n
one among a set of 2nRk Uk sequences chosen to satisfy the
distortion constraints.
Theorem 1: The sets D of all achievable distortions D for
the local and progressive encoding schemes for the distributed
protocol are the same.
Proof: For Gaussian codebooks and Gaussian measurements and from symmetry of the model, the distortion D at
each agent is given by

We now determine Dmin and Dmax . Let
2
α ≡ E(Yl2 ) = σX (1 + h (K − 1)) + 1, for all l
√
2
β ≡ E(Yl Yk ) = σX 2 h + h (K − 2) , l = k.

(10a)
(10b)

2
Note that for large K, α → h (K − 1) σX , and β →
2
h (K − 2) σX .
Computation of Dmax : Expanding (9), we obtain
2
Dmax = E [var(X1 |Y1 )] = σX

1−

2
σX
α

.

(11)

2
For large K, Dmax → σX .
Computation of Dmin : Expanding (8), we have

Dmin = E [var(X1 |Y1 Y2 . . . YK )]
|E [var(X1 Y2 . . . YK |Y1 )]|
=
|E [var(Y2 . . . YK |Y1 )]|

(12)
(13)

where the simpliﬁcation in (13) results from the assumption
of jointly Gaussian random variables. Applying Lemma 1, for
√
2
4
2
h − β/α ,
(14)
c1 = σX − σX /α, c2 = σX
c3 = α − β 2 /α, and c4 = β − β 2 /α,

we obtain the minimum distortion Dmin as


√
2
2
σX ( h−β/α)
(K − 1) 1−σ2 /α 

( X ) 
Dmin = Dmax 1 −
.

f1 (K, β 2 /α)

(15)

˜ ˜ ˜
˜
D = E var X1 |Y1 U1 U2 U3 . . . UK

(17)

= E [var (X1 |Y1 U1 U2 U3 . . . UK )] ∈ [Dmin , Dmax ] (18)
˜
where in (17) we have used that fact that U1 = U1 , and
conditioned on U1 ,it sufﬁces to condition on U2 , and similarly
for the remaining Uk , k > 2.
Computation of D: Using the independence of the quantization noise Qk for all k, as well as the independence of Qk
and Xk , we have E [Uk Ul ] = E [Yk Yl ] = β for all l = k and
2
2
2
E Uk = E Yk + E Q2 = α + σQ . Thus, D is obtained
k
in a manner analogous to the calculation of Dmin with the
2
replacement of c3 by c3 + σQ . Thus, we have

(16)

√
Remark 1: For K → ∞, Dmin → Dmax (1−(1− h)2 /h).

B. Distributed Protocol
A general coding strategy for this distributed source coding
problem needs to take into account: a) the order of agent
broadcasts; b) multiple encoding possibilities at each agent
depending on whether the received data is used alongwith
local measurements in encoding; c) exploiting the correlated
measurements at other agents in broadcasting just sufﬁcient
data for other agents to achieve their distortions; and d) multiple rounds of interactions. We present a distributed encoding
scheme with a single round of communication (for simplicity
of analysis) in which the agents broadcast in order (the source
permutation choice is irrelevant due to the symmetry of the
model). The local and progressive coding schemes differ in
including the received data in encoding at each agent, while
the centralized and distributed protocols differ in whether they
exploit the correlated measurements at the other agents.
The achievable distortion D in general depends on the
˜
encoding scheme chosen. Let Rk and Rk denote the rates
for the local and progessive encoding schemes, respectively.
We ﬁrst consider the progressive encoding scheme in which
each agent broadcasts (to all other agents) a noisy function of both its measurements and prior communications.
More precisely, agent k maps its measurement and prior
˜
˜n
communication sequences to one among a set of 2nRk Uk
sequences chosen to satisfy the distortion constraints. The



√
2
2
σX ( h−β/α)

(K − 1) 1−σ2 /α

( X )
D = Dmax 1 −

β2
2
f1 K, α + σQ




.


(19)

Rate Computation: We consider a round-robin protocol in
which agent 1 broadcasts a quantized function of its measurements and prior communications at a rate which takes into
account all the side information at all other agents. Thus, the
˜
rate R1 required is the maximal of the rates required to each
agent and is given by
˜
˜
˜
˜
R1 ≥ I(U1 ; Y1 ) − min I(U1 ; Y2 ), . . . , I(U1 ; YK )
= I(U1 ; Y1 ) − I(U1 ; Y2 ) = R1

(20a)
(20b)

where (20b) follows from the symmetry of the measurement
˜
model, the fact that U1 = U1 , and R1 is the minimal rate
required at agent 1 for the local scheme. Next, agent 2
analogously broadcasts a function of its measurements at a

3

rate R2 given by
˜
˜
˜
R2 ≥ I(U2 ; Y2 U1 ) −

min

˜
˜
I(U2 ; Yl U1 )

(21a)

min

˜
˜
I(U2 ; Y1 |U1 )

(21b)

l∈{1,...,K},l=2

˜
˜
= I(U2 ; Y2 |U1 ) −

where (25b) is obtained from (25a) by determining
T
|E [var (U K |Y1 )]| where U K−1 = [U2 U3 . . . UK ] denotes a column vector of length (K − 1). By expanding
E var U K−1 |Y1 using Proposition 1, one can verify that
|E [var (U K |Y1 )]| simpliﬁes to ﬁnding the determinant of the
(K − 1) × (K − 1) Toeplitz matrix with diagonal and off
2
2
2
diagonal entries α + σQ − β and β − β , respectively, which
α
α
2
from Lemma 1 is given by f1 K, β 2 /α (α + σQ − β)(K−2) .
2
One can similarly show that E [var (U1 |Y2 )] = α+σQ −β 2 /α.
2
In the limit of K → ∞, (K − 2) β − (K − 1) β → 0,
α
α − β 2 /α → h, α − β → h, and therefore, the second and
third log terms in (25b) scale as log (K) . Thus, in the limit,
Dist
the per agent rate R = Rsum /K is given by

l∈{1,...,K},l=2

= I(U2 ; Y2 ) − I(U2 ; Y1 ) = R2

(21c)

˜
˜
˜
˜
where (21c) follows from h(U2 |Y1 U1 ) − h(U2 |Y2 U1 ) =
h(U2 |Y1 ) − h(U2 |Y2 ) since U2 − Y2 − U1 form a Markov
chain and due to the symmetry of the model. It can be veriﬁed
easily that the bound in (21c) is the minimal rate R2 for the
local encoding scheme. One can similarly show that the rate
at which agent 3 broadcasts is
˜
˜
˜ ˜
R3 ≥ I(U3 ; Y3 U1 U2 ) −

min

l∈{1,...,K},l=3

˜
˜ ˜
I(U3 ; Y1 U1 U2 ) (22a)

= I(U3 ; Y3 ) − I(U3 ; Y1 U2 ) = R3

lim R =

(22b)

K→∞

where we have used the fact that U3 − Y3 − U1 U2 and U1 −
Y1 − U3 form Markov chains. Generalizing we have, for all
k > 1,
˜
Rk = Rk ≥ I(Uk ; Yk ) − I(Uk ; Y1 U1 . . . Uk−1 ),

1
log
2

αf1 K, β 2 /α
2
(α − σX ) f1 (K, c5 )

(23a)

(24c)

where (24b) is a result of the model symmetry, the code
construction and typicality arguments and is omitted for
brevity. The bound in (24c) follows from the relation of
the code constructions for the two encoding schemes and
√ 2
2
2
c5 = (β − hσx )2 α − σx + hσX .
Theorem 2: It is sufﬁcient to encode the local measurements at each agent in the distributed protocol.
Theorem 2 follows directly from the fact that for Gaussian
encoding, from (18), (23a), and (24c), we have that the set
of all rate-distortion-leakage tuples achieved by the local and
progressive encoding schemes is the same.
K
Dist
The sum-rate of the distributed scheme Rsum = k=1 Rk
can be simpliﬁed as
K
Dist
2
Rsum = h (U2 U3 . . . UK |Y1 ) + h(U1 |Y2 ) −
log 2πeσQ
2

K
=
log
2

+

2
α + σQ − β
2
σQ



.

(26)

n
Agent 2 takes into account the knowledge of U1 at all agents
and broadcasts at a rate

R2 ≥ I (U2 ; Y2 ) − I (U2 ; U1 ) .

(28)

Note that the agents broadcast taking into account the prior
transmissions (as if to a CEO) but not the side information at
the other agents. Continuing similarly, we have for all k ≥ 2,
Rk ≥ I (U2 ; Yk ) − I (Uk ; U1 U2 . . . Uk−1 ) .

CEO
The resulting sum rate Rsum =
as
CEO
Rsum =

K
k=1

I(Uk ; Yk ) −

K
k=1

K
k=2

(29)

Rk can be simpliﬁed
I(Uk ; U1 . . . Uk−1 )
(30)

(25a)

2

2
α + σQ − β
α
1

+ log 
2
2 −β
α + σQ
(25b)

1
2
log (f1 K, β 2 /α + σQ )
2

2
α + σQ − β
2
σQ

C. Distributed vs. Centralized
We now compare the distributed protocol to a centralized
protocol in which each agent broadcasts at a rate intended
for a (virtual) CEO, and thus, is oblivious of the correlated
measurements at the other agents. Here again, the agents
can use a progressive encoding scheme analogously to the
distributed protocol. As in the distributed protocol, here too
one can show that a local encoding scheme sufﬁces, in which
n
agent k generates a codebook Uk whose entries Uk,i are
generated in an i.i.d fashion such that Uk,i = Yk,i + Qk,i ,
Qk,i is independent of Yk,i and Ql,i , for all l = k, for all k,
and for all i. The compression rates are bounded as follows.
First, agent 1 transmits its quantized measurements at a rate
n
R1 such that for error-free decoding of U1 at the decoder, we
require
R1 ≥ I (U1 ; Y1 ) .
(27)

where the bound in (23a) is the minimal rate at which agent
n
k is required to broadcast when it only encodes Yk .
Calculation of Leakage: For the proposed progressive encoding, the leakage of the state of agent k at any other agent
j = k, for all such k, j, is bounded as
1
(j)
n
(24a)
Lk = I(Xk ; Yjn J1 J2 . . . JK ), j = k
n
˜
˜
≥ I(X1 ; Y2 U1 . . . UK ) = I(X1 ; Y2 U1 . . . UK ) (24b)
=

1
log
2

2
α + σQ − β

4

= h (UK , UK−1 . . . U1 ) −
=

K
2
log 2πeσQ
2

2
α + σQ − β
2
σQ


2
α + σQ + (K − 1) β
1
.
+ log 
2
2
α + σQ − β

K
log
2

(31)
(32)

CEO
Thus, the rate on average per user is RCEO = Rsum /K
which converges in the limit of a large number of agents K
to
2
α + σQ − β
1
.
(33)
lim RCEO = log
2
K→∞
2
σQ

0.5

Per-user Rate R and Leakage of Agent k (nats/sec)

Fig. 1.

+Z

0.35

0.3

0.25

0.2

0.15

0

50

100

150
Number of Agents K

200

250

300

Plot of per-user rate R and leakage Lk of any agent k vs. K.

where g1

≡ E

ˆ
X2 − Y2

2

2−1
(K − 1) (K − 2) bβ/2 + σZ )−1 ,

= (b2 (K − 1) α +
2

q1 ≡ α − g1 b2 β 2 (K − 1) , and
√
q2 = g1 b2 1 + (K − 2) h β (K − 1) .

(42)
(43)

Remark 2: Due to the lack of a pre-log factor K, the peruser rate R for the outer bound rapidly approaches 0 with K
(relative to the inner bounds).
The rate R and leakage Lk (for any k) as a function of K
2
are illustrated in Fig. 1 for h = 0.5 and σQ = 6.

ˆ
For jointly Gaussian Y1 , . . . , YK , X2 , we can write
K
l=1,l=2 bYl

0.4

0.1

D. Outer Bounds
From the symmetry of the model, it sufﬁces to bound the
rate R1 of agent 1 as
1
1
n
R1 ≥ H(J1 ) ≥ I(Y1n ; J1 |Y2n Y3n . . . YK )
(34)
n
n
1 n
ˆ
≥ h (Y1 |Y2 . . . YK ) −
h(Y1,i |X2,i Y2,i . . . YK,i ) (35)
n i=1
1
≥ h (Y1 |Y2 . . . YK ) − log(2πeΣ)
(36)
2
ˆn
ˆn
where (35) results from the fact that X2 , . . . XK can be estin
n
mated from J1 , Y2 , . . . YK , and that conditioning on only one
of the estimates is a lower bound on R1 , and (36) results from
using the fact that a jointly Gaussian distribution maximizes
the differential entropy for a ﬁxed variance, from the concavity
ˆ
of the log function for Σ ≡ E var Y1 |X1 Y2 Y3 . . . YK .

R Distributed
R Centralized
Lk Leakage

h = 0.5
σ2 = 6
Q

0.45

Comparing (25b) and (32), we can verify that for every
Dist
2
CEO
choice of σQ , and hence D, Rsum > Rsum . Furthermore, one
can also show that the leakage at each agent for the centralized
protocol is the same as the distributed protocol in (24) and is
the same for both the local and progressive encoding schemes.
The following theorem summarizes our results.
Theorem 3: The sum-rate of the centralized protocol is
strictly lower bounded by that for the distributed protocol and
converges to this lower bound only in the limit of large K.

ˆ
X2 = Y2 +

2

σX = 5

IV. C ONCLUDING R EMARKS
We have introduced a distributed state estimation problem
among K agents with ﬁdelity and privacy constraints. We have
shown that the sum-rate and per user rate achieved from a
distributed protocol in which the agents directly interact taking
into account the prior knowledge at all agents lower bounds
those achieved by a centralized protocol with convergence
for very large K. Tighter outer bounds that account for the
distributed coding are much needed.

(37)

2
where Z ∼ N 0, σZ is independent of Yk for all k, and
from symmetry, we choose the same scaling constant b in (37).
2
ˆ
For g ≡ E[ X2 − Y2 − bY3 . . . − bYK ] = b2 / b2 α + σ 2 ,
Z

2

c1 = β 2 g, and c2 = c1 + (β − βαg) / α − α2 g , we obtain

f1 (K, β 2 /α) (α − β)
1
(38)
log
2
f1 (K − 1, β 2 /α)
1
f1 (K, c2 )
− log
(39)
α − α2 g
2
f1 (K, c1 )
where we have used the orthogonality of the minimum MSE
ˆ
estimate and the measurements, i.e., E X1 − X1 Yl = 0,
for all l = 1, and the distortion constraint in (5).
(j)
(2)
ˆ
With X2 in (37), one can similarly bound L1 = L1 (from
symmetry), for all j, as
1
n
(40)
R1 ≥ I(X1 ; Y2n J1 J2 . . . JK )
n
1
ˆ
(41)
≥ h (X1 ) − log 2πeE var X1 |Y2 X2
2
2
√
1
2
2 2
= log q1
h − q2
1 − σ X q2 q1 − σ X
2
R1 ≥

R EFERENCES
[1] L. Sankar, S. Kar, R. Tandon, and H. V. Poor, “Competitive privacy in
the smart grid: An information-theoretic approach,” in Proc. 2nd IEEE
Intl. Conf. Smart Grid Commun., Brussels, Belgium, Oct. 2011.
[2] A. D. Wyner and J. Ziv, “The rate-distortion function for source coding
with side information at the decoder,” IEEE Trans. Inform. Theory,
vol. 22, no. 1, pp. 1–10, Jan. 1976.
[3] T. Berger, “Multiterminal source coding,” in Information Theory Approach to Communications, G. Longo, Ed. New York: Springer-Verlag,
1978.
[4] S. Tung, “Multiterminal rate-distortion theory,” Ph.D., Cornell University,
Ithaca NY, USA, 1978.
[5] A. B. Wagner, S. Tavildar, and P. Viswanath, “Rate region of the
quadratic gaussian two-encoder source-coding problem,” IEEE Trans.
Inform. Theory, vol. 54, no. 5, pp. 1938 –1961, May 2008.

5


Creator:         TeX output 2012.05.02:1510
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May  2 15:10:40 2012
ModDate:        Tue Jun 19 12:55:02 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      264877 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569558985

An Information Theoretic Perspective Over an
Extremal Entropy Inequality
Sangwoo Park, Erchin Serpedin, and Khalid Qaraqe
Electrical and Computer Engineering Department, Texas A&M University, College Station, TX 77843, USA
Email: swpark78@neo.tamu.edu, serpedin@ece.tamu.edu, khalid.qaraqe@qatar.tamu.edu

the moment generating function (MGF), the worst additive
noise lemma, and the classical EPI. By using data processing
inequality, worst additive noise lemma, and classical EPI, we
calculate an upper bound. Then, by applying the equality
condition in data processing inequality, we prove that the
upper bound can be achieved. MGFs are implemented to prove
the achievement of the equality condition in data processing
inequality.
The proposed proof in this paper connotes the following
major contributions. First, our proof is simpler and more
direct compared with the proofs in [1]. Second, we adapt a
more information-theoretic approach without using the KKT
conditions. The method based on data processing inequality
and MGF enables us to circumvent the step of using the KKT
conditions. Moreover, by simply analyzing some properties
of positive semi-deﬁnite matrices, we can omit the step of
proving the existence of the optimal solution which satisﬁes
the KKT conditions, a step which is very complicated to
accomplish. In addition, the optimal solution’s covariance
matrix structure is mentioned in detail by using properties
of positive semi-deﬁnite matrices. Third, the proposed proof
represents a novel investigation method not only for EEI
but also for applications such as establishing the Gaussian
broadcast channel capacity, secrecy capacity of Gaussian wiretap channel, etc. These considerations support the versatility
of EEI.
The rest of this paper is organized as follows. EEI without a
covariance constraint is considered in Section II. EEI and the
proposed new proof, which are the main results of this paper,
are presented in Section III. Applications are brieﬂy mentioned
in Section IV. Finally, Section V concludes the paper.

Abstract—This paper focuses on developing an alternative
proof for an extremal entropy inequality, originally presented in
[1]. The proposed alternative proof is simply based on the classical entropy power inequality and the data processing inequality.
Compared with the proofs in [1], the proposed alternative proof
is simpler, more direct, and information theoretic, and presents
the advantage of providing the structure of the optimal solution
covariance matrix. Also, the proposed proof might also be used as
a novel method to address applications such as calculation of the
vector Gaussian broadcast channel capacity, establishing a lower
bound for the achievable rate of distributed source coding with
a single quadratic distortion constraint, and the secrecy capacity
of the Gaussian wire-tap channel.

I. I NTRODUCTION
An extremal entropy inequality (EEI), involving the difference of two random vector differential entropies subject
to certain constraints and motivated by multi-terminal information theoretic problems such as the vector Gaussian
broadcast channel and the distributed source coding with a
single quadratic distortion constraint, was established recently
by Liu and Viswanath in [1]. This extremal entropy inequality
was ignited by the question: “What is the optimal solution
for the classical entropy power inequality (EPI) under a
covariance matrix constraint?” Even though it is assumed
that the expected solution is a Gaussian random vector, the
solution cannot be obtained by using the classical EPI due to
the covariance matrix constraint. Therefore, a novel method,
referred to as the channel enhancement technique [2], was
adopted in the proofs illustrated in [1].
The proposed proofs in [1] proceed as follows. First, EEI
is recast as an optimization problem (P ). An auxiliary opti˜
mization problem (P ) is considered for the original problem
(P ) by means of the channel enhancement technique, which
relies mainly on the Karush-Kuhn-Tucker (KKT) conditions.
˜
The maximum value of (P ) is shown to be larger than the
˜
maximum value of (P ). Then, (P ) is solved using the classical
EPI. Finally, the proof is accomplished by showing that the
˜
maximum value of (P ) is equal to the maximum value of
(P ). Even though Liu and Viswanath proposed two distinctive
proofs, a direct proof and a perturbation proof, both proofs are
commonly based on the channel enhancement technique, and
they are derived in a similar way.
The main theme of this paper is to prove the EEI without
using the channel enhancement technique. Our proof is mainly
based on four techniques: the data processing inequality,

II. E NTROPY P OWER I NEQUALITY
Since EEI is similar to the classical EPI, we ﬁrst investigate
a relationship between EEI and EPI. Without a covariance
constraint, EEI is equivalent to EPI as shown in Theorem 1.
Theorem 1: For an arbitrary random vector X with a covariance matrix ΣX and a Gaussian random vector WG with
a covariance matrix ΣW , there exist Gaussian random vectors
∗
˜
XG and XG which satisfy the following inequalities:
h(X) − µh(X + WG )

1

˜
˜
≤ h(XG ) − µh(XG + WG ), (1)
∗
∗
≤ h(XG ) − µh(XG + WG ), (2)

where the constant µ ≥ 1, h(·) stands for differential entropy,
all random vectors are independent of one another, ΣW is a
∗
˜
positive deﬁnite matrix, and XG and XG are Gaussian random
vectors satisfying the following conditions:
∗
˜
i) Covariance matrices of XG and XG are represented by
ΣX and ΣX ∗ , respectively.
˜
ii) ΣX is proportional to ΣW , and ΣX ∗ = (µ − 1)−1 ΣW .
˜
˜
iii) Differential entropy h(XG ) is the same as h(X).
Proof: Inequality (1) can be proved using EPI, and (2)
can be shown exploiting concavity. The details of the proof
are delegated to the journal paper version of this work [6].
As stated in Theorem 1, for µ ≥ 1, h(X) − µh(X + WG )
is maximized when random vector X is Gaussian. However,
when a covariance constraint is added to (1) and (2), we cannot
establish whether a Gaussian random vector still maximizes
h(X) − µh(X + WG ) or not, using the techniques previously
employed in the proof of Theorem 1. This is due to the fact
that the covariance constraint may impede the proportionality
relationship between the covariance matrices ΣX ∗ and ΣW .

Proof: The following lemma, which is necessary in
constructing the proof, will be ﬁrst presented.
Lemma 1 (Worst Additive Noise Lemma [1], [3], [4]):
For independent random vectors Y1 , YG1 , and YG2 ,
I(Y1 + YG2 ; YG2 ) ≥ I(YG1 + YG2 ; YG2 ),
where I(·; ·) denotes mutual information, Y1 is an arbitrary
random vector, YG1 and YG2 are Gaussian random vectors with
covariance matrices equal to Y1 and covariance matrix ΣY2 ,
respectively. Also, all random vectors are independent of one
another.
When R is a positive deﬁnite but singular matrix, i.e.,
|R| = 0, using eigenvalue decomposition, the inequality (4) is
equivalently changed into the inequality, which has a nonsingular covariance matrix constraint, as mentioned in [1].
When µ = 1, the inequality (4) is easily proved by the Lemma
1. Therefore, without loss of generality, we assume that µ > 1
and R is a positive deﬁnite matrix [1], [6].
Then, inequality (4) is proven as follows:
′
′
h(X+WG )−h(X+WG |WG )≥h(XG+WG )−h(XG+WG |WG )
˜
˜
⇔ h(X+WG )≥h(X+WG )+h(XG+WG )−h(XG+WG ), (5)

III. E XTREMAL E NTROPY I NEQUALITY
In [1], it is shown that a Gaussian random vector still
maximizes h(X) − µh(X + WG ) even when a covariance
constraint is considered. Inequality (2) can be reformulated
as an optimization problem with a covariance constraint as
follows:
max
p(X)

s.t.

where ⇔ denotes equivalence. Inequalities in (5) are due to
˜
Lemma 1 by replacing Y1 , YG1 , and YG2 with X + WG ,
′
˜
XG + WG , and WG , respectively. Also, notice that the Gaussian
random vector WG can be expressed as the sum of two
′
˜
independent Gaussian random vectors WG and WG whose
covariance matrices satisfy:

h(X + WG ) − µh(X + VG ),
ΣX ≼ R,

(3)

ΣW

where WG and VG are independent Gaussian random vectors
with positive deﬁnite covariance matrices ΣW and ΣV , respectively. The notation ≼ stands for positive (semi)deﬁnite
partial ordering between positive deﬁnite matrices. Also, all
random vectors in (3) are assumed independent of one another,
and the maximization is conducted over the distribution of random vector X. Two proofs, a direct proof and a perturbation
proof, are provided in [1]. Each proof approaches the problem
in a different way but both proofs share an important key common component, namely the channel enhancement technique
based on the KKT conditions and proposed originally in [2].
Unlike the proofs in [1], we will solve (3) without using
the channel enhancement technique. Before dealing with the
general problem (3), we ﬁrst consider a simpler case of it in
the form of Theorem 2.
Theorem 2: Assume that µ is an arbitrary but ﬁxed constant, where µ ≥ 1, and R is a positive semi-deﬁnite matrix. A
Gaussian random vector WG is assumed to have a covariance
matrix ΣW , which is positive deﬁnite. Given an arbitrary
random vector X, which is independent of WG , with a
covariance matrix ΣX ≼ R, there exists a Gaussian random
∗
vector XG with a covariance matrix ΣX ∗ which satisﬁes the
following inequality:
∗
∗
h(X) − µh(X + WG ) ≤ h(XG ) − µh(XG + WG ),

= ΣW + ΣW ′ ,
˜

(6)

where ΣW , ΣW , and ΣW ′ are the covariance matrices of WG ,
˜
′
˜
WG , and WG , respectively. Henceforth, the Gaussian random
′
˜
vector WG is represented as WG = WG + WG .
Based on (5) and (6), the left-hand side (LHS) of (4) is
upper-bounded as follows:
h(X)−µh(X+WG )
(
)
˜
˜
≤ h(X)−µh(X+WG )+µ h(XG+WG )−h(XG+WG ) . (7)
Using Theorem 1, if (µ−1)−1 ΣW ≼ R, (7) is upper-bounded
˜
as
(
)
˜
˜
h(X)−µh(X+WG )+µ h(XG+WG )−h(XG+WG )
(
)
∗
∗
˜
˜
≤ h(XG )−µh(XG+WG )+ µ h(XG+WG )−h(XG+WG ) , (8)
∗
where XG is a Gaussian random vector, whose covariance
matrix ΣX ∗ is deﬁned as (µ−1)−1 ΣW . Due to the covariance
˜
constraint, unlike Theorem 1, we additionally have to prove
∗
that there exists a random vector XG whose covariance matrix
ΣX ∗ satisﬁes

ΣX ∗ = (µ − 1)−1 ΣW ≼ R.
˜

(9)

Since ΣX ≼ R, instead of proving (9), we will show that
∗
there exists a random vector XG whose covariance matrix ΣX ∗
satisﬁes

(4)

where ΣX ∗ ≼ R.

ΣX ∗ = (µ − 1)−1 ΣW ≼ ΣX .
˜

2

(10)

If (10) is satisﬁed, then we can form the Markov chain:
′
′
∗
′
∗
′
˜
˜
XG → XG + XG + WG → XG + XG + WG + WG ,

In the Markov chain (13), since all random vectors are
Gaussian (without loss of generality, they are assumed to
have zero means), using Lemma 3, the following MGFs are
expressed in closed-form:
}
{
)
1 T(
−1
−1
T
MY1 |Y3(S) = exp S ΣY1ΣY3 Y3+ S ΣY1 −ΣY1ΣY3 ΣY1 S ,
2
{
}
)
1 (
MY2 |Y3(S) = exp S T ΣY2Σ−1 Y3+ S T ΣY2 −ΣY2Σ−1ΣY2 S ,
Y3
Y3
2

(11)

where all random vectors are independent of one another.
Since a Gaussian random vector XG can be expressed as the
′
∗
sum of two independent Gaussian random vectors XG and XG
whose covariance matrices satisfy
ΣX

= ΣX ′ + ΣX ∗ ,

′
′
∗
′
∗
˜
˜
where Y1 = XG , Y2 = XG +XG + WG , Y3 = XG +XG + WG +
′
WG , and their covariance matrices are represented by ΣY1 ,
ΣY2 , and ΣY3 , respectively. Since ΣW + ΣW ′ is a positive
˜
deﬁnite matrix, there exists the inverse of ΣY3 . On the other
hand, MGF of Y1 + Y2 given Y3 is expressed as

where ΣX , ΣX ′ , and ΣX ∗ stand for the covariance matrices
′
∗
of XG , XG , and XG , respectively, the Gaussian random vector
′
∗
XG will be represented as XG = XG + XG .
Based on the data processing inequality and the Markov
chain (11), we obtain

MY1 +Y2 |Y3 (S)

′
′
˜
I(XG ; XG+WG ) ≤ I(XG ; XG+WG )
∗
∗
˜ G )−h(XG+WG )+h(XG+WG )≤h(XG+WG ).(12)
˜
⇔ h(XG+W

= MY1 |Y3 (S)MY2 |Y3 (S)
{ (
) }
×exp S T ΣY1 −ΣY2 Σ−1 ΣY1 +ΣY1 −ΣY1 Σ−1 ΣY2 S . (15)
Y3
Y3

Although we need an upper bound of (8), (12) provides a
lower bound of (8) as follows:
(
)
∗
∗
˜
˜
h(XG )−µ h(XG+WG )−h(XG+WG )+h(XG+WG )

(A)

If (A) in (15) vanishes, Y1 and Y2 are independent given Y3 ,
and the Markov chain (13) is obtained. Using Lemma 11, (1)
in [2], deﬁne the covariance matrix ΣW as
˜
(
)−1
−1
ΣW = (ΣX + ΣW ) + L
− ΣX ,
(16)
˜

∗
∗
≥ h(XG )−µh(XG + WG ).

However, if we can also construct the following Markov chain:
∗
′
′
∗
′
′
˜
˜
XG → XG + XG + WG + WG → XG + XG + WG ,

where L ≽ 0, and 0 denotes an n-by-n zero matrix. The
positive semi-deﬁnite matrix L must be chosen to satisfy

(13)

the equality condition of the data processing inequality is
satisﬁed. It turns out that
′
′
˜
I(XG ; XG + WG ) = I(XG ; XG + WG ).

ΣX ∗ ≼ ΣX ,

where ΣX ∗ = (µ − 1)−1 ΣW , ΣX ′ = ΣX − ΣX ∗ , L ≽ 0.
˜
The next result proves the existence of such a matrix L.
Lemma 4: There exists a positive semi-deﬁnite matrix L
which satisﬁes:

(14)

Therefore, (14) leads us to a tight upper bound as follows:
(
)
˜
˜
h(X)−µ h(X+WG )−h(XG+WG )+h(XG+WG )
(
)
∗
∗
˜
˜
≤ h(XG )−µ h(XG+WG )−h(XG+WG )+h(XG+WG )

ΣX ∗ ≼ ΣX ,
ΣX ′ L = 0,
(
)−1
−1
where ΣW = (ΣX + ΣW ) + L
− ΣX , ΣX ∗ = (µ −
˜
1)−1 ΣW , ΣX ′ = ΣX − ΣX ∗ , and ΣX and ΣW stand
˜
for a positive semi-deﬁnite and a positive deﬁnite matrix,
respectively.
Proof: Proving ΣX ∗ ≼ ΣX is equivalent to proving the
following inequalities:

∗
∗
= h(XG )−µh(XG+WG ).

Now, we will show that we can actually construct the Markov
chain (13) using the following lemmas.
Lemma 2: For conditionally independent random vectors
Y1 and Y2 given a random vector Y3 , the following equality
between the MGFs is satisﬁed:
MY1 +Y2 |Y3 (S) =

LΣX ′ = ΣX ′ L = 0,

ΣW ≼ (µ − 1) ΣX
˜
−1

⇔ (ΣX + ΣW )

MY1 |Y3 (S)MY2 |Y3 (S),

+ L ≽ µ−1 Σ−1
X

(17)

Since there always exists a non-singular matrix which simultaneously diagonalizes two positive semi-deﬁnite matrices [7],
there exists a non-singular matrix Q which simultaneously
diagonalize both ΣX and ΣW :

T

where MX|Y (S) = EX|Y [eX S ], EX|Y [·] denotes the expectation with respect to random vector X given Y , and superscript
T denotes transposition. For jointly Gaussian random vectors
Y1 and Y2 given Y3 , this equality is a necessary and sufﬁcient
condition for the conditional independence between Y1 and Y2
given Y3 .
Lemma 3: For a Gaussian random vector YG with a mean
UY and a covariance matrix ΣY , MGF is expressed as
}
{
1 T
T
MY (S) = exp S UY + S ΣY S .
2

QT ΣX Q = I, and QT ΣW Q = DW ,

(18)

where I stands for the identity matrix, and DW is a diagonal
matrix. If we deﬁne DL as a diagonal matrix whose ith
diagonal entry is dLi ,
{
0,
if dWi ≤ µ − 1
dWi −(µ−1)
dLi =
(19)
, if dWi > µ − 1
µ(1+dWi )

3

ΣY1 −ΣY1 Σ−1 ΣY2
Y3
(
)
−1
−1
= ΣX ′ (ΣX +ΣW ) −ΣX ′ (ΣX +ΣW )
(ΣX +ΣW )
˜
˜
= 0.

where dWi denotes the ith diagonal element of DW , and
L

= QDL QT ,

(20)

(17) is equivalent to

and the proof is completed.
The more general problem, originally proved in [1], is now
considered in Theorem 3.
Theorem 3: Assume that µ is an arbitrary but ﬁxed constant, where µ ≥ 1, and R is a positive semi-deﬁnite matrix.
Gaussian random vectors WG and VG are assumed to be
independent of each other and have positive deﬁnite covariance
matrices ΣW and ΣV , respectively. Given an arbitrary random
vector X, which is independent of WG and VG , with a
covariance matrix ΣX ≼ R, there exists a Gaussian random
∗
vector XG with a covariance matrix ΣX ∗ which satisﬁes the
following inequality:

−1

(ΣX +ΣW ) +L ≽ µ−1 Σ−1
X
−1
⇔ (Q−T Q−1 +Q−T DW Q−1 ) +QDL QT ≽ µ−1 QQT
−1
⇔ (I+DW ) +DL ≽ µ−1 I.
(21)
Equation (21) always holds since DL and L are deﬁned as in
(19) and (20) to satisfy (21). Therefore, the ﬁrst inequality in
(17) is also satisﬁed.
Notice that ΣX ′ =ΣX −ΣX ∗ . Since ΣX ∗ =(µ − 1)−1 ΣW ,
˜
ΣX ′ is expressed as ΣX − (µ − 1)−1 ΣW , and
˜
)
(
−1
ΣX ′ L = ΣX − (µ − 1) ΣW L
˜
(
(
)−1)
−1
−1
= (µ−1) Q−T µI− (I+DW) +DL
DL QT (22)
= 0.

∗
∗
h(X+WG )−µh(X+VG ) ≤ h(XG+WG )−µh(XG+VG ), (28)

(23)

where ΣX ∗ ≼ R.
Proof: Due to the same reason mentioned in the proof of
Theorem 2, without loss of generality, we assume µ > 1 and
R is a positive deﬁnite matrix. The proof is generally similar
to the proof of Theorem 2. The LHS of (28) is upper-bounded
as

Equality (22) is due to (18) and (20), and equality (23) is due to
(19). Therefore, by deﬁning ΣW = ((ΣX+ΣW )−1+L)−1−ΣX ,
˜
we can make ΣW satisfy
˜
ΣW ≼ (µ − 1) ΣX ,
˜

ΣX ′ L = 0,

and the proof is completed.
Remark 1: Since the optimization problem in [1] is generally nonconvex, the existence of optimal solution must be
proved [1], [2], and this step is very complicated. However,
in our proof, Lemmas 4 and 5 serve as a substitute for this
step since we by-pass the KKT-conditions related parts using
the data processing inequality. This makes the proposed proof
much simpler.
Equation (16) can be re-written as
(
)−1
−1
ΣX + ΣW = (ΣX + ΣW ) + L
˜
−1
−1
⇔ (ΣX + ΣW ) = (ΣX + ΣW ) + L.
˜

h(X + WG ) − µh(X + VG )
∗
∗
˜
˜
≤ h(XG + WG ) − µh(XG + VG ) + h(WG ) − h(WG ) (29)
∗
∗
(30)
= h(XG + WG ) − µh(XG + VG ),
˜
where WG is chosen to be a Gaussian random vector whose
covariance matrix, ΣW , satisﬁes
˜
ΣW ≼ ΣW ,
˜

−1

−1

= (ΣX + ΣW ) ΣX ′ ,
−1
= ΣX ′ (ΣX + ΣW ) .

(31)

The inequality (29) is due to Lemma 1 and Theorem 2, and
the equality (30) will be proved using the equality condition
in data processing inequality. We will also prove that there
˜
exists a Gaussian random vector WG which satisﬁes (31) by
proving later Lemma 5.
To satisfy the equality in (30), the equality condition in data
processing inequality must be satisﬁed, and the following two
Markov chains must be formed:

(24)

Since LΣX ′ = ΣX ′ L = 0, by multiplying with ΣX ′ both
sides of (24),
(ΣX + ΣW ) ΣX ′
˜
−1
′ (ΣX + Σ ˜ )
ΣX
W

ΣW ≼ µ−1 ΣV .
˜

(25)
(26)

′
Since random vectors Y1 , Y2 , and Y3 are deﬁned as Y1 = XG ,
˜
Y2 = XG + WG , and Y3 = XG + WG , respectively, and they
are independent of each other, their covariance matrices are
represented as

1)
2)

∗
∗
∗
′
˜
˜
XG → XG + WG → XG + WG + WG ,
∗
∗
′
∗
˜
˜
XG → XG + WG + WG → XG + WG ,

(32)
(33)

˜
where all random vectors are normally distributed, WG and
′
′
∗
˜
WG are independent of each other, WG = WG + WG , and XG
is independent of other random vectors.
∗
The Markov chain (32) is naturally formed since XG ,
′
˜
WG , and WG are independent Gaussian random vectors. The
validity of the Markov chain (33) is proved using the concept
of MGF. In the Markov chain (33), since all random vectors
are Gaussian (without loss of generality, they are assumed to
have zero means), using Lemma 3, the following relationship

ΣY1 = ΣX ′ , ΣY2 = ΣX + ΣW , ΣY3 = ΣX + ΣW . (27)
˜
From (25) and (27), and (26) and (27), respectively, it turns
out
ΣY1 − ΣY2 Σ−1 ΣY1
Y
(3
)
−1
−1
= (ΣX + ΣW ) (ΣX+ΣW ) ΣX ′ −(ΣX+ΣW ) ΣX ′
˜
˜
= 0,

4

among the MGFs is established:

IV. A PPLICATIONS
Due to space limitations, the details of applications are not
shown herein paper. However, either by replacing the channel
enhancement technique by the proposed novel method or by
simply adapting the extremal entropy inequality without using
the channel enhancement technique, the following applications can be established: the capacity of the vector Gaussian
broadcast channel, the lower bound for the achievable rate
of distributed source coding with a single quadratic distortion
constraint [1], and the secrecy capacity of the Gaussian wiretap channel [8]. As a distinctive feature to what was shown
in [1], [2], [8], and [9], the channel enhancement technique is
bypassed by the simple equality condition of data processing
inequality.

MY1 +Y2 |Y3 (S)
= MY1 |Y3 (S)MY2 |Y3 (S)
{ (
) }
× exp S T ΣY1 −ΣY2 Σ−1 ΣY1 +ΣY1 −ΣY1 Σ−1 ΣY2 S . (34)
Y3
Y3
(B)

If (B) in (34) vanishes, Y1 and Y2 are independent given Y3 ,
and the Markov chain (33) is obtained. Using Lemma 11, (1)
in [2], we deﬁne a covariance matrix ΣW as follows:
˜
−1

= (Σ−1 + K)
W

ΣW
˜

,

(35)

where K ≽ 0, KΣX ∗ = ΣX ∗ K = 0, and 0 denotes an nby-n zero matrix. Then, there exists a positive semi-deﬁnite
matrix K which satisﬁes ΣW ≼ µ−1 ΣV and KΣX ∗ = 0,
˜
where ΣX ∗ = (µ−1)−1 (ΣV −µΣW ). The existence of matrix
˜
K is proved by the following lemma.
Lemma 5: There always exists a positive semi-deﬁnite matrix K which satisﬁes
ΣW ≼ µ−1 ΣV ,
˜

V. C ONCLUSIONS
The major contributions of this paper can be summarized
as follows. First, an alternative proof- simpler, more direct,
and information-theoretic than the original proofs- of EEI
was provided. The alternative proof is mainly based on data
processing inequality which enables to bypass the KKT conditions. Moreover, using properties of positive semi-deﬁnite
matrices, one can skip the step of proving the existence of
the optimal solution which satisﬁes the KKT conditions, a
step which is quite complicated to justify. Finally, this paper
proposed a novel method to investigate several applications
such as the capacity of the vector Gaussian broadcast channel,
the secrecy capacity of the Gaussian wire-tap channel, etc.
This novel technique is based on data processing inequality,
and it is very unique and creative in respect that it presents
a novel paradigm for lots of applications, which were proved
commonly based on the channel enhancement technique [1],
[2], [8], and [9].
Acknowledgment: This work was supported by Qtel.

KΣX ∗ = ΣX ∗ K = 0,
−1

where ΣX ∗ =(µ − 1)−1 (ΣV −µΣW ), and ΣW =(Σ−1 +K) .
˜
˜
W
Proof: The proof is similar to the one in Lemma 4. The
details of the proof are delegated to [6].
−1
Since ΣW is deﬁned as (Σ−1 + K) in (35), ΣW satisﬁes
˜
˜
W
−1

(ΣX ∗ + ΣW )
˜

=

−1

(ΣX ∗ + ΣW )

+ K,

(36)

based on Lemma 11, (1) in [2]. Since KΣX ∗ = ΣX ∗ K = 0,
multiplying with ΣX ∗ both sides of (36), (36) is expressed as
−1

−1

(ΣX ∗ + ΣW ) ΣX ∗ = (ΣX ∗ + ΣW ) ΣX ∗ ,
˜
−1
−1
ΣX ∗ (ΣX ∗ + ΣW ) = ΣX ∗ (ΣX ∗ + ΣW ) .
˜

(37)
(38)

In this case, random vectors Y1 , Y2 , and Y3 are deﬁned as
∗
∗
∗
˜
Y1 = XG , Y2 = XG + WG , and Y3 = XG + WG , respectively.
′
∗
′
˜ G , and XG , WG , and WG are independent of one
˜
W = WG + W
another. Therefore, their covariance matrices are represented
as

R EFERENCES
[1] T. Liu and P. Viswanath, “An Extremal Inequality Motivated by Multiterminal Information-Theoretic Problems,” IEEE Trans. Inf. Theory, vol.
53, no. 5, pp. 1839 - 1851, May 2007.
[2] H. Weingarten, Y. Steinberg, and S. Shamai (Shitz), “The Capacity Region
of the Gaussian Mutiple-Input Multiple-Output Broadcast Channel,” IEEE
Trans. Inf. Theory, vol. 52, no. 9, pp. 3936 - 3964, Sep 2006.
[3] O. Rioul, “Information Theoretic Proofs of Entropy Power Inequalities,”
IEEE Trans. Inf. Theory, vol. 57, no. 1, pp. 33 - 55, Jan 2011.
[4] S. N. Diggavi and T. M. Cover, “The Worst Additive Noise under a
Covariance Constraint,” IEEE Trans. Inf. Theory, vol. 47, no. 7, pp. 3072
- 3081, Nov. 2001.
[5] T. M. Cover and J. A. Thomas, Elements of Information Theory, New
York: Wiley-Interscience, 1991.
[6] S. Park, E. Serpedin, and K. Qaraqe, “An Alternative Proof of an Extremal
Inequality,” IEEE Trans. Inf. Theory, Aug. 2011 (submitted).
[7] R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge University
Press, 1985.
[8] T. Liu and S. Shamai (Shitz), “A Note on the Secrecy Capacity of the
Multiple-Antenna Wiretap Channel,” IEEE Trans. Inf. Theory, vol. 55,
no. 6, pp. 2547 - 2553, Jun 2009.
[9] E. Ekrem and S. Ulukus, “The Secrecy Capacity Region of the Gaussian
MIMO Multi-Receiver Wiretap Channel,” IEEE Trans. Inf. Theory, vol.
57, no. 4, pp. 2083 - 2114, Mar 2011.

ΣY1 = ΣX ∗ , ΣY2 = ΣX ∗ + ΣW , ΣY3 = ΣX ∗ + ΣW . (39)
˜
From (37) and (39), and (38) and (39), respectively, it turns
out that
ΣY1 − ΣY2 Σ−1 ΣY1
Y
(3
)
−1
−1
= (ΣX ∗ +ΣW ) (ΣX ∗ +ΣW ) ΣX ∗ −(ΣX ∗ +ΣW ) ΣX ∗
˜
˜
= 0,
ΣY1 − ΣY1 Σ−1 ΣY2
Y3
(
)
−1
−1
= ΣX ∗ (ΣX ∗ +ΣW ) −ΣX ∗ (ΣX ∗ +ΣW )
(ΣX ∗ +ΣW )
˜
˜
= 0.
Since the inverse of ΣW exists, (ΣX ∗ + ΣW )−1 also ex˜
˜
ists. Therefore, (B) in (34) is zero, and MY1 +Y2 |Y3 (S) =
MY1 |Y3 (S)MY2 |Y3 (S). It means Y1 and Y3 are independent
∗
∗
˜
given Y2 , i.e., XG and XG + WG are independent given
∗
′
˜
XG + WG + WG , and the Markov chain (33) is valid. The
equality in (30) is achieved by the above procedure, and the
proof is completed.

5


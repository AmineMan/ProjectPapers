Creator:         TeX output 2012.05.18:1929
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 19:29:30 2012
ModDate:        Tue Jun 19 12:54:54 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      283280 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565393

Variable-Length Extractors
Hongchao Zhou

Jehoshua Bruck

Department of Electrical Engineering
California Institute of Technology
Pasadena, CA 91125
Email: hzhou@caltech.edu

Department of Electrical Engineering
California Institute of Technology
Pasadena, CA 91125
Email: bruck@caltech.edu

stochastic process (an ideal source), it is know how to generate
a certain number of random bits very efﬁciently. But those
approaches can not be easily generalized for non-ideal sources
that have unknown inherent correlations and are affected by
measurement’s noise.
Extracting randomness from non-ideal sources has been an
active research topic in the last two decades. In 1990, Zuckerman introduced a general model of weak random sources,
called k-sources, namely whose min-entropy is at least k. It
was shown that given a source on {0, 1}n with min-entropy
k < n, it is impossible to devise a single function that extracts
even one bit of randomness. This observation led to the
introduction of seeded extractors, which using a small number
of additional truly random bits as the seed (catalyst). When
simulating a probabilistic algorithm, one can simply eliminate
the requirement of truly random bits by enumerating all the
possible strings for the seed and taking a majority vote on the
ﬁnal results. There are a variety of very efﬁcient constructions
of seeded extractors, summarized in [2], [9], [13]. On the
other hand, people study seedless extractors for some speciﬁc
classes of random sources, including independent sources [11],
bit-ﬁxing sources [6], and samplable sources [7]. Almost all
known constructions of seeded or seedless extractors have
ﬁxed input length and ﬁxed output length, hence, we call
them ﬁxed-length extractors. For many weak random sources,
the maximal number of random bits extracted based on a
ﬁxed-length construction is upper bounded by the source’s
min-entropy. However, the information theoretic limit for
randomness extraction is the source’s entropy, which is larger
than the min-entropy. The concept of min-entropy and entropy
is deﬁned as follows.

Abstract—We study the problem of extracting a prescribed
number of random bits by reading the smallest possible number
of symbols from non-ideal stochastic processes. The related
interval algorithm proposed by Han and Hoshi has asymptotically
optimal performance; however, it assumes that the distribution
of the input stochastic process is known. The motivation for
our work is the fact that, in practice, sources of randomness
have inherent correlations and are affected by measurement’s
noise. Namely, it is hard to obtain an accurate estimation of the
distribution. This challenge was addressed by the concepts of
seeded and seedless extractors that can handle general random
sources with unknown distributions. However, known seeded
and seedless extractors provide extraction efﬁciencies that are
substantially smaller than Shannon’s entropy limit. Our main
contribution is the design of extractors that have a variable inputlength and a ﬁxed output length, are efﬁcient in the consumption
of symbols from the source, are capable of generating random bits
from general stochastic processes and approach the information
theoretic upper bound on efﬁciency.

I. I NTRODUCTION
The problem of generating random bits from imperfect random sources, essential for many randomized applications [8],
has been extensively studied. It dates back to von Neumann
[15] in 1951, who ﬁrst considered the problem of simulating
unbiased coins by using a biased coin with unknown probability. The optimal algorithms were later derived by Elias [3] and
Peres [10]. In those algorithms the expected number of random
bits generated is asymptotically equal to the entropy of source.
In 1986, Blum [1] studied the problem of generating random
bits from a correlated source, speciﬁcally, he considered ﬁnite
Markov chains. We generalized Blum’s method and proposed
the ﬁrst known algorithm that generates random bits from
an arbitrary Markov chain, runs in expected linear time and
achieves the information-theoretic upper bound on efﬁciency
[16]. However, the above algorithms, although very efﬁcient,
only work for blocks, namely, input sequences with ﬁxed
length, and they generate variable numbers of random bits. In
many occasions, it requires to generate a prescribed number
of random bits by reading the smallest possible number of
symbols from the source. In [17], we introduced an optimal
algorithm that generates a prescribed number of random bits
for an arbitrary biased coin with unknown bias. In [5], Han
and Hoshi proposed an interval algorithm for generating a
prescribed number of random bits from any known stochastic
process (whose distribution is given) with an optimal performance. Generally, given an arbitrary biased coin or a known

Deﬁnition 1. Given a random source X on {0, 1}n , the minentropy of X is deﬁned as
Hmin (X) =

min

x∈{0,1}n

log

1
.
P [X = x]

The entropy of X is deﬁned as
∑
H(X) =
P [X = x] log
x∈{0,1}n

1
.
P [X = x]

Example 2. Let X be a random variable such that P [X =
0] = 0.9 and P [X = 1] = 0.1, then Hmin (X) = 0.152 and

1

H(X) = 0.469. In this case, the entropy of X is three times
its min-entropy.
2

B. Seeded-Extractors
Seeded-extractors were introduced to extract randomness
from a single source by using a small number additional truly
random bits [9], [13]. A seeded extractor is a function

In this paper, we focus on the notion and constructions
of variable-length extractors, namely, extractors with variable
input-length and ﬁxed output-length. Here, we would like to
ﬁx the output length because the demand of random bits is
application-dependent and usually ﬁxed. The input length can
be variable because many natural sources for randomness extraction are stochastic processes. So our goal is to extract a prescribed number of random bits from a slightly-unpredictable
stochastic process by reading the smallest possible number of
symbols. Since the source may not be stationary ergodic, we
deﬁne the efﬁciency η of a variable-length extractor as the
asymptotic ratio between its output length and the entropy of
its input sequence, which is upper bounded by 1.
Given a real stochastic process R, we use β indicate its nonidealness, deﬁned by the minimum distance between R and
ideal sources (such as stationary ergodic processes). This β is
an important parameter, which can be easily estimated in real
applications. In this paper, we introduce several constructions
of variable-length extractors, whose efﬁciencies can reach
η ≥ 1 − β. For instance, let R be an arbitrary independent
process such that the probability of each bit is slightly unpredictable, i.e., P [xi = 1] ∈ [0.9, 0.91] for i ≥ 1. For this
source R, β ≥ 0.0315, hence, we can construct a variablelength extractor with efﬁciency at least 0.9685, very close to
the theoretical upper bound 1. But the existing constructions
of seeded or seedless extractors can only achieve 0.3117 on
efﬁciency. In general, variable-length extractors are proposed
for two purposes: they are generalizations of the algorithms for
ideal sources (as those proposed in [17] and [5]) to manage
general noisy sources; and they are improvements of ﬁxedlength extractors for ﬁlling the gap between min-entropy and
entropy on efﬁciency.
The remainder of this paper is organized as follows. Section
II goes over some preliminaries and lists some related results.
Section III, Section IV and Section V present and analyze
different constructions of variable-length extractors. Due to
space limitation, we omit some of the proofs.

E : {0, 1}n × {0, 1}d → {0, 1}m
such that for every distribution X on {0, 1}n with Hmin (X) ≥
k, the distribution E(X, Ud ) is ϵ-close to the uniform distribution Um . Here, d is the seed length, and we call such an
extractor as a (k, ϵ) extractor. There are a lot of works focusing
on constructions of seeded-extractors. A standard application
of the probabilistic method [12] shows that there exists a
seeded-extractor that can extract asymptotically Hmin (X) random bits with log(n − Hmin (X)) additional truly random bits.
Recently, Guruswami, Umans and Vadhan [4] provided an
explicit construction of seeded-extractors, whose efﬁciency is
very close to the bound obtained based on the probabilistic
method. Their main result is described as follows:
Lemma 3. [4] For every constant α > 0, and all positive
integers n, k and all ϵ > 0, there is an explicit construction
of a (k, ϵ) extractor E : {0, 1}n × {0, 1}d → {0, 1}m with
d ≤ log n + O(log(k/ϵ)) and m ≥ (1 − α)k.
C. Variable-length Extractors
A seeded variable-length extractor is a function
VE : Sp × {0, 1}d → {0, 1}m
such that given a real source R, the output sequence is ϵclose to the uniform distribution Um . Here, Sp is the set of all
possible input sequences. It forms a preﬁx-free code such that
for any sequence y ∈ {0, 1}∞ , there is exactly one sequence
x ∈ Sp such that x is a preﬁx of y. The general procedure
of extracting randomness by using a variable-length extractor
can be divided into two steps:
1) First, we read bits from the source R one by one until
the current input sequence x is a sequence in Sp . In this case,
we construct a function
V : Sp → {0, 1}n
to map the current input sequence into a binary sequence of
ﬁxed length. Our goal is to get a random sequence Z with
length n and min-entropy k.
2) Second, by applying a seeded extractor

II. P RELIMINARIES
A. Statistical Difference

E : {0, 1}n × {0, 1}d → {0, 1}m

Statistical Difference is used in computer science to measure
the difference between two distributions. Let X and Y be
two random sequences with range {0, 1}m , then the statistical
difference between X and Y is deﬁned as
∥X − Y ∥ =

max

T :{0,1}m →{0,1}

to the random sequence Z, we extract a sequence of m almostrandom bits that is ϵ-close to the uniform distribution Um .
We can see that the construction of a variable-length extractor is a cascade of a function V and a seeded extractor E,
namely,
⊗
VE = E
V.

|P [T (X) = 1] − P [T (Y ) = 1]|

over a boolean function T . We say that X and Y are ϵclose if ∥X − Y ∥ ≤ ϵ. In this paper, we want to extract m
almost-random bits such that they are ϵ-close to the uniform
distribution Um on {0, 1}m with speciﬁed small ϵ > 0.

Note that our requirement is to extract m almost-random
bits that are ϵ-close to Um . According to the constructions
of seeded extractors, see Lemma 3, the value of k can be

2

predetermined by m and ϵ. So the key of constructing variablelength extractors is to ﬁnd the input set Sp and the function V
such that the min-entropy of the random sequence Z is at least
k and the expected length of the elements in Sp is minimized.
For some speciﬁc types of sources, including independent
sources and samplable sources, by applying the ideas in [11]
and [7] we can remove the requirement on seed without
degrading the asymptotic performance. As a result, we have
seedless variable-length extractors. For example, if R is an
independent source, we can ﬁrst apply the method in [11] to
extract d almost-random bits from the ﬁrst Θ(log m ) bits, and
ϵ
then use them as the seed of a seeded variable-length extractor
to extract randomness from the rest bits of the process. Due
to space limitation, in this paper we mainly focus on seeded
constructions of variable-length extractors.

E. Efﬁciency
In our construction, n = Θ(k) and the seed length d is
very small. To consider the performance of a construction, we
are interested in the expected cost (we ignore d because it
can be treated as ﬁxed in our constructions), so we deﬁne its
efﬁciency as
m
η = lim
m→∞ HR (Xm )
such that the output sequence is ϵ-close to the uniform
distribution Um on {0, 1}m , where m is the output length
and HR (Xm ) is the entropy of the input sequence Xm .
Lemma 5. For any construction of variable-length extractors
with d = o(m), its efﬁciency η ≤ 1.
If R is a stationary ergodic process, we can deﬁne its
entropy rate as
H(X l )
h(R) = lim
,
l→∞
l

D. Model Approximation
The main idea of constructing variable-length extractors is
base on model approximation, namely, given a real source
R, we use a simple model M to approximate it and then
based on which we construct the extractor. The performance of
the resulting variable-length extractor strongly depends on the
difference between R and M. In this paper, we use βt (R, M)
to measure the distance between the source R and the model
M, deﬁned by
βt (R, M) =

max

PR (x)
PM (x)
1
log2 PM (x)

log2

|x|≥t,x∈{0,1}∗

where X l is a random sequence of length l generated from
the source R. In this case, the entropy of the input sequence
is proportional to the expected input length.
Lemma 6. Given a stationary ergodic source R, let Xm be
the input sequence of a variable-length extractor with output
length m. Then

,

HR (Xm )
= h(R),
m→∞ ER [|Xm |]
lim

where t is a constant, PR (x) is the probability of generating
x from R when the sequence length is |x|, PM (x) is the
probability of generating x from M when the sequence length
1
is |x|, and the term log2 PM (x) is used for normalization. Then

where ER [|Xm |] is the expected input length.
III. C ONSTRUCTION BASED ON K NOWN P ROCESSES
In this section, we consider those sources that can be
approximated by a known stochastic process M. Here, we say
a process M is known if its distribution is given, i.e., PM (x)
can be easily calculated for any x ∈ {0, 1}∗ . Note that this
process M is not necessary to be stationary or ergodic. For
instance, M can be an independent process z1 z2 ... ∈ {0, 1}∗
such that
1 + sin(i/10)
∀i ≥ 1, PM (zi = 1) =
.
2
Our goal is to extract randomness from an imperfect random
source R. The problem is that we don’t know the exact distribution of R, but we know that it can be approximated by a
known process M, speciﬁcally, we assume that β(R, M) ≤ β
for a constant β. Then we can use the knowledge about M
and β to construct a variable-length extractor. As a result, we
have the following procedure to extract m almost-random bits.

0 ≤ βt (R, M) ≤ 1
for any source R and model M. Note that βt (R, M) is a
non-increasing function of t. In our applications, we only care
about those input sequences in Sp , so we consider
β(R, M) = βt∗ (R, M),
where t∗ = minx∈Sp |x|, namely, only the sequences that
reach a certain length. According to this deﬁnition, for any
sequence x ∈ Sp ,
PR (x) ≤ PM (x)1−β(R,M) .
If M is an ideal process, then β reﬂects the non-idealness
of the real source R, which can be easily estimated in real
applications without knowing PR (x).
Example 4. Let x1 x2 ... ∈ {0, 1}∗ be a sequence generated
from an arbitrary indepedent source R such that

Construction 7. Assume the real source is R, and there exists
a known stochastic process M such that β(R, M) ≤ β for a
constant β.
1) Read input bits one by one from R until we get an input
sequence x ∈ {0, 1}∗ such that

∀i ≥ 1, P [xi = 1] ∈ [0.8, 0.82].
If we let M be a biased coin with probability 0.8132, then
βt (R, M) = β(R, M)
≤ max(

log2
log2

0.2
0.1868
1
0.1868

,

log2
log2

0.82
0.8132
1
0.8132

) = 0.0405, ∀t ≥ 1.

log2

3

k
1
≥
,
PM (x)
1−β

where k is an integer such that there exists a ﬁxed-length
(k, ϵ) extractor that generates a random sequence of
length m from a source with min-entropy k.
2) Let n be the maximum length of all the possible input
sequences, then
n = arg min{l ∈ N|∀y ∈ {0, 1}l , log2
l

This completes the proof.
We see that the gap β on efﬁciency in the above theorem
is introduced by the difference between the source R and the
known stochastic process M. In some sense, it reﬂects the
model uncertainty of the real source R.

k
1
≥
}.
PM (y)
1−β

IV. C ONSTRUCTION BASED ON B IASED -C OINS
In this section, we use a general ideal model such as a
biased coin or a Markov chain to approximate the real source
R. Here, we don’t care about the speciﬁc parameters of the
ideal model. The reason is, in some cases, the source R is
very close to an ideal source but we cannot (or don’t want to)
estimate the parameters accurately. As a result, we introduce
a construction by exploring the characters of biased coins or
Markov chains. For simplicity, we only discuss the case that
the ideal model is a biased coin, and the same idea can be
generalized when the ideal model is a Markov chain. Let Gb.c.
denote the set consisting of all the models of biased coins
with different probabilities, then the following procedure is
provided to extract m almost=random bits.

If |x| < n, we can extend the length of x to n by adding
n − |x| trivial zeros at the end. Since x is randomly
generated, from the above procedure, we get a random
sequence Z of length n.
3) Applying a (k, ϵ) extractor to Z yields a binary sequence
of m that is ϵ-close to the uniform distribution Um . 2
The following example is provided for comparing this
construction with ﬁxed-length constructions.
Example 8. Let M be a biased coin with probability 0.8 (of
k
being 1). If 1−β = 0, then we can get the input set
Sp = {0, 10, 110, 1110, 11110, 111110, 1111110, 1111111}.

Construction 11. Assume the real source is R such that
minM ∈Gb.c. β(R, M ) ≤ β for a constant β.
1) Read input bits one by one from R until we get an input
sequence x ∈ {0, 1}∗ such that
(
)
k0 + k1
k
log2
≥
,
max(1, min(k0 , k1 ))
1−β
where k0 is the number of zeros in x and k1 is the
number of ones in x. k is an integer such that there exists
a ﬁxed-length (k, ϵ) extractor that generates a random
sequence of length m from a source with min-entropy k.
2) Since the input sequence x can be very long, we map it
into a sequence z of ﬁxed length n such that

In this case, the expected input length is strictly smaller than
7. For ﬁxed-length constructions, to get a random sequence
with min-entropy at least 2, we have to read 7 input bits
independent of the content. It is less efﬁcient than the former
method.
2
Theorem 9. Construction 7 generates a random sequence of
length m that is ϵ-close to the uniform distribution Um .
Theorem 10. Given a real source R such that there exists a
known stochastic process M with β(R, M) ≤ β, then the
efﬁciency of Construction 7 is 1 − β ≤ η ≤ 1.
Proof: We only need to show that η ≥ 1−β. According to
k
Lemma 3, as m → ∞, to make ϵ → 0, we have limm→∞ m =
1.
Now, let’s consider the number of elements in Sp , namely,
|Sp |. To calculate |Sp |, we let

z = [I(k0 ≥k1 ) , min(k0 , k1 ), r(x)],
where I(k0 ≥k1 ) = 1 if and only if k0 ≥ k1 , and r(x)
is the rank of x among all the permutations of x with
respect to the lexicographic order. Since x is randomly
generated, the above procedure leads us to a random
sequence Z of length n.
3) Applying a (k, ϵ) extractor to Z yields a random sequence of m that is ϵ-close to the uniform distribution
Um .
2

′
Sp = {x[1 : |x| − 1]|x ∈ Sp },

where x[1 : |x| − 1] is the preﬁx of x with length |x| − 1, then
k
′
for all y ∈ Sp , log2 PM1(y) < 1−β . Hence,
′
log2 |Sp | <

k
.
1−β

Let 1a denote the all-one vector of length a, then we get
the following result.

′
It is easy to see that |Sp | ≤ 2|Sp |, so

log2 |Sp | <

Theorem 12. Construction 11 generates a random sequence
of length m that is ϵ-close to the uniform distribution Um if
k
PR (1a ), PR (0a ) ≤ 2−k for a = 2⌊ 1−β ⌋ .

k
+ 1.
1−β

Theorem 13. Given a real source R such that
minM ∈Gb.c. β(R, M ) ≤ β. If there exists a model M ∈ Gb.c.
1
with probability p ≤ 2 of being 1 or 0 such that
√
1 ln 2
p > β(R, M ) log2
,
p 2

Let Xm be the input sequence, then
lim

k→∞

log2 |Sp |
1
HR (Xm )
≤ lim
≤
,
k→∞
k
k
1−β

Finally, it yields
m
≥ 1 − β.
m→∞ HR (Xm )

η = lim

then the efﬁciency of Construction 11 is 1 − β ≤ η ≤ 1.

4

m from a source with min-entropy k, and ε → 0 as
k → 0.
2) Applying a (k, ϵ) extractor to Z yields a random sequence of length m that is ϵ-close to the uniform
distribution Um .
2
It can be proved that the min-entropy of Z approaches k
as k → ∞ and ε → 0, so that we can continue to apply a
ﬁxed-length extractor to ‘purify’ the sequence.

V. C ONSTRUCTION BASED ON S TATIONARY E RGODIC
P ROCESSES
In this section, we consider imperfect sources that are
approximately stationary and ergodic. In [14], Visweswariah, Kulkarni and Verd´ showed that optimal variable-length
u
source codes asymptotically achieve optimal variable-length
random bits generation in the sense of normalized divergence.
Although their work only focuses on ideal stationary ergodic
processes and generates ‘weaker’ random bits, it motivates us
to combine universal compression with ﬁxed-length extractors
for efﬁciently extracting random bits from noisy stochastic
processes. In this section, we will ﬁrst introduce Lempel-Ziv
code and then present its application in constructing variablelength extractors.
Lempel-Ziv code is a universal data compression scheme
introduced by Ziv and Lempel [18], which is simple to
implement and can achieve the asymptotically optimal rate for
stationary ergodic sources. The idea of Lempel-Ziv code is to
parse the source sequence into strings that haven’t appeared
so far, as demonstrated by the following example.

Theorem 16. When k → ∞ and ε → 0, Construction 15
generates a random sequence of length m that is ϵ-close to
the uniform distribution Um .
Theorem 17. Given a real source R such that there exists a
stationary ergodic process M with β(R, M) ≤ β, then the
efﬁciency of Construction 15 is
1 − β ≤ η ≤ 1.
R EFERENCES
[1] M. Blum, “Independent unbiased coin ﬂips from a correlated biased
source: - a ﬁnite state Markov chain”, Combinatorica, vol. 6, pp. 97108, 1986.
[2] Z. Dvir and A. Wigderson. “Kakeya sets, new mergers and older
extractors”, In Processings of IEEE Symposium on Foundations of
Computer Science (FOCS), 2008.
[3] P. Elias, “The efﬁcient construction of an unbiased random sequence”,
Ann. Math. Statist., vol. 43, pp. 865-870, 1972.
[4] V. Guruswami, C. Umans, and S. Vadhan. “Unbalanced expanders and
randomness extractors from parvaresh-vardy codes”, In Proceedings of
IEEE Conference on Computational Complexity (CCC), pp. 96-108,
2007.
[5] T. S. Han, and M. Hoshi, “Interval algorithm for random number
generation,”, vol. 32, pp. 599-611, 1997.
[6] J. Kamp and D. Zuckerman, “Deterministic extractors for bit-ﬁxing
sources and exposure-resilient cryptography”, SIAM Journal on Computing, 36:1231-1247, 2006.
[7] J. Kamp, A. Rao, S. Vadhan and D. Zuckerman, “Deterministic extractors for small-space sources”, Journal of Computer and System Sciences,
vol. 77, pp. 191-220, 2011.
[8] R. Motwani and P. Rgahavan. Randomized Algorithms, Combridge
University press, 1995.
[9] N. Nisan, “Extracting randomness: how and why: a survey”, In Proceedings of IEEE Conference on Computational Complexity (CCC), pp.
44-58, 1996.
[10] Y. Peres, “Iterating von Neumann’s procedure for extracting random
bits”, Ann. Statist., vol. 20, pp. 590-597, 1992.
[11] A. Rao, “Randomness extractors for independent sources and applications”, Ph.D. Thesis, Department of Computer Science, University of
Texas at Austin, 2007.
[12] J. Radhakrishnan and A. Ta-Shma. “Bounds for dispersers, extractors,
and depth-two supperconcentrators”, SIAM Journal on Discrete Mathmatics, 13(1): 2-24, 2000.
[13] R. Shaltiel, “Recent developments in explicit constructions of extractors”, BULLETIN-European Association For Theoretical Computer
Science, vol. 77, pp. 67-95, 2002.
[14] K. Visweswariah, S.R. Kulkarni and S. Verd´ , “Source codes as random
u
number generators”, IEEE Trans. on Information Theory, vol. 44, no. 2,
pp. 462-471, 1998.
[15] J. von Neumann, “Various techniques used in connection with random
digits”, Appl. Math. Ser., Notes by G. E. Forstyle, Nat. Bur. Stand., vol.
12, pp. 36-38, 1951.
[16] H. Zhou and J. Bruck, “Efﬁciently generating random bits from ﬁnite
state markov chains”, IEEE Trans. on Information Theory, Apr, 2012.
[17] H. Zhou and J. Bruck, “Streaming Algorithms for Optimal Generation
of Random Bits,” Technical Report, Electrical Engineering, California
Institute of Technology, 2012.
[18] J. Ziv and A. Lempel. “Compression of individual squences by varaible
rate coding”, IEEE Trans. on Information Theory, vol. 24, pp. 530-536,
1978.

Example 14. Assume the input is 010111001110000..., then
we parse it as strings 0, 1, 01, 11, 00, 111, 000, ..., where each
string is the shortest string that never appear before. That
means all its preﬁxes have occurred earlier.
Let c(n) be the number of strings obtained by parsing a
sequence of length n. For each string, we describe its location
with log c(n) bits. Given a string of length l, it can described
by (1) the location of its preﬁx of length l − 1, and (2) its last
bit. Hence, the code for the above sequence is
(000, 0), (000, 1), (001, 1), (010, 1), (001, 0), (100, 1), (101, 0), ...

Typically, Lempel-Ziv code is applied to an input sequence
of ﬁxed-length. Here, we are interested in Lempel-Ziv code
with ﬁxed output-length and variable input-length. As a result,
we can apply a single ﬁxed-length extractor to the output of
Lempel-Ziv code for extracting randomness. In our algorithm,
we read raw bits one by one from an imperfect source
until the length of the output of Lempel-Ziv code reaches a
predetermined length. In another word, the number of strings
after parsing is a predetermined number c. For example, if
the source is 1011010100010... and c = 4, then after reading
6 bits, we can parse them into 1, 0, 11, 01. Now, we get
an output sequence (000, 1), (000, 0), (001, 1), (010, 1), which
can be used as the input of a ﬁxed-length extractor. We call
this Lempel-Ziv code as variable-length Lempel-Ziv code,
based on which we have the following procedure to extract
m almost-random bits.
Construction 15. Assume the real source is R and there exists
a stationary ergodic process M such that β(R, M ) ≤ β.
1) Read input bits one by one based on the variable-length
Lempel-Ziv code such that we get an output sequence
k
Z whose length reaches n = 1−β (1 + ε), where k is
an integer such that there exists a ﬁxed-length (k, ϵ)
extractor that generates a random sequence of length

5


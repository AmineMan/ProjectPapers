Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Thu May 17 12:48:12 2012
ModDate:        Tue Jun 19 12:54:38 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      403213 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569564481

Polynomials and Computing Functions of
Correlated Sources
Sheng Huang, Mikael Skoglund
School of Electrical Engineering
KTH Royal Institute of Technology
Stockholm, Sweden
Email: sheng.huang@ee.kth.se, skoglund@ee.kth.se
Xin → 1, 2nRi , ∀1 ≤ i ≤ s, and one decoder D, where D :

Abstract—We consider the source coding problem of computing functions of correlated sources, which is an extension of the
Slepian–Wolf coding problem. We observe that all the discrete
functions are in fact restrictions of polynomial functions over
some ﬁnite ﬁeld. Based on this observation, we demonstrate how
to use Elias’ Lemma to enlarge the coding rate region (compared
to the Slepian–Wolf region) for a certain class of polynomial
functions.
We present a classiﬁcation result about polynomial functions
regarding this coding problem. The result is conclusive in the
two-sources scenario and, in fact, gives another interpretation of
a result by Han and Kobayashi [1, Theorem 1].

s

i=1

i=1

Xi . The original source cod-

ˆn
Xi

Pr
i=1

n
Xi

=

< , where

Xin , such that
i=1

n
Xi

s

ˆn
Xi are the

and

i=1

i=1

input of Ei and the output of D, respectively.
The classic Slepian–Wolf theorem [2] says that the coding
rate region of the above problem is given by
R[X1 ,X2 , · · · , Xs ] =

(R1 , R2 , · · · , Rs ) ∈ Rs

Rj ≥ H(XJ |XJ c ), ∀ ∅ = J ⊆ {1, 2, · · · , s} .
j∈J

We consider a similar source coding problem in which a
discrete function of the output data is to be recovered, instead
s

Xi → Ω be

of the original data. To be more precise, let f :

i=1

.
j=1

Subsequently, by combining the standard source coding technique and Elias’ Lemma, Ahlswede and Han [6] gave an inner
bound of R[⊕] which is larger than K¨ rner and Marton’s in
o
general.
Other variations of the general function computing problem
have been studied in existing literature. As a special case, the
original source coding problem is the ﬁrst example with the
identity function to be computed. In [7], the scenario where
one of the two sources is intactly known by the decoder is
considered. The idea of characteristic graph [8] is used in
random code construction. On the other hand, the corresponding channel coding problems in which the receiver(s) is (are)
interested in reproducing a function (distinct functions) of
the output data of the source(s) from the channel or network
output(s) are considered in [9], [10], [11]. The original MAC
channel coding problem where the function to be reproduced is
the identity function is a special case of this class of problems.
A certain type of MAC channel is studied in [9]. [10], [11]
consider the problem in the setting of network coding.
In this paper, we focus on the source coding problem of
computing a discrete function of several correlated sources.
The observation that all discrete functions are restrictions
of polynomial functions serves as an important factor which
works underneath our method. Basically, the structure of the
function considered is what distinguishes this function computing problem from the Slepian–Wolf source coding problem.
Hence, the algebraic structure of a function unveiled by its

s

i=1

s

s

f

R⊕ = (R1 , R2 ) ∈ R2 | R1 , R2 ≥ H(X1 ⊕ X2 ) .

ing problem for correlated sources considers the following
scenario: given s sources t1 , t2 , · · · , ts which generate correlated random data X1 , X2 , · · · , Xs , what is the coding rate
region R, such that for any (R1 , R2 , · · · , Rs ) ∈ R and
> 0, there exists a big enough n, s encoders E1 , E2 , · · · , Es ,
where Ei : Xin → 1, 2nRi , ∀1 ≤ i ≤ s, and one
→

=

(j)
Xi

Let R[f ] be the coding rate region for computing function
f . It is expected that R[f ] = R[X1 , X2 , · · · , Xs ] if f is
the identity function, while in general it is easily seen that
R[X1 , X2 , · · · , Xs ] ⊆ R[f ]. Based on the method of Elias [3]
(cf. [4]), K¨ rner and Marton [5] show that if f is the moduloo
two sum ⊕, R[⊕] contains the convex hull of the union of
R[X1 , X2 ] and R⊕ , where

i=1

1, 2nRi

n

s

n
Xi

respectively, and f

s

s

< ,

i=1

s

Let (X1 , X2 , · · · , Xs ) ∼ p be discrete random variables

decoder D, where D :

n
Xi

n
where Xi and ω are the input of Ei and the output of D,

I. I NTRODUCTION

deﬁned over sample space

s

1, 2nRi → Ωn , such that Pr ω = f

i=1

the function considered. Then what is the coding rate region
R, such that for any (R1 , R2 , · · · , Rs ) ∈ R and > 0, there
exists a big enough n, s encoders E1 , E2 , · · · , Es , where Ei :
This work was funded in part by the Swedish Research Council.

1

k

polynomial presentation is of great importance. Introducing
this algebraic tool to the computing problem is one of the
contributions of this paper. By treating discrete functions as
polynomial functions, we point out that it is possible to enlarge
the coding rate region (compared to the Slepian–Wolf region)
for certain types of polynomial functions (see Theorem III.3,
Theorem III.4 and Corollary III.5). In particular, inspired by
the research of Han and Kobayashi [1], for the two-sources
scenario, we show that the coding rate region for a given
function is strictly bigger than the Slepian–Wolf region if and
only if this function admits a “nice” polynomial presentation
(see Theorem III.6 for more details).

i=1
k

i=1

only if there exist bijections φi : Xi → Yi , ∀ 1 ≤ i ≤ k, and
ψ : Ω1 → Ω2 , such that
f (x1 , x2 , · · · , xk ) = ψ −1 (g(φ1 (x1 ), φ2 (x2 ), · · · , φn (xk ))).
Remark 1. The equivalency deﬁned does not preserve all
the mathematical properties of two equivalent functions. For
instance, it does not preserve orders of the domain and the
codomain. However, it does preserve all the mathematical
properties that concern the encoders and the decoder. In other
words, it can be easily proved that two equivalent functions
share the “same” coding method, consequently, their coding
rate regions are the same. From now on, we will simply refer
to two equivalent functions as one function.

Capital letters X, Y, Z, · · · are used to denote random
variables and X , Y, Z, · · · correspond to their respective sample spaces. In addition, lower case letters x, y, z, · · · are
used to denote instances of random variables. For a ﬁxed
n, X n , Y n , Z n , · · · represent i.i.d sequences of length n,
respectively. X (i) is for the ith term of X n , X [i) is for the
i.i.d sequence X (i) , X (i+1) , · · · , X (n) and X [i] is for the i.i.d
sequence X (1) , X (2) , · · · , X (i−1) , X (i+1) , X (i+2) , · · · , X (n) .
Let X1 , X2 , · · · , Xs be s correlated random variables. XJ is
deﬁned to be the array of random variables Xj1 , Xj2 , · · · , Xjk ,
where {j1 , j2 , · · · , jk } = J ⊆ {1, 2, · · · , s}. The deﬁnitions
(i)
[i)
[i]
n
of XJ , XJ , XJ and XJ resemble such a deﬁnition.
In this paper, we assume that, for (X1 , X2 , · · · , Xs ) ∼ p,
pXi (x) > 0, ∀ x ∈ Xi and ∀ 1 ≤ i ≤ s, where pXi is the
marginal of p, because if pXi (x) = 0 for some x ∈ Xi , then
we can assume that Xi is deﬁned on Xi \{x} rather than Xi . In
addition, Xi is not a function of X1 , · · · , Xi−1 , Xi+1 , · · · , Xs
for all feasible i. This implies that |Xi | ≥ 2, ∀ 1 ≤ i ≤ s.
The support of a distribution p is set to be supp(p) =

Deﬁnition II.3. Given function f : D → Ω and ∅ = S ⊆ D,
the restriction of f on S is deﬁned to be the function f |S :
S → Ω such that f |S : x → f (x), ∀ x ∈ S.
Lemma II.4. Any discrete function f (x1 , x2 , · · · , xk ) dek

ﬁned on domain D =

tions of some polynomial function h ∈ Zp [k], with p ≥
max {|f (D)|, |Xi | | 1 ≤ i ≤ k } being a prime.
Remark 2. In the above lemma, Zp can be replaced by Fpm ,
where pm ≥ max {|f (D)|, |Xi | | 1 ≤ i ≤ k }. Interested readers can refer to [12, Lemma 7.40] for details. For completeness
and to help understanding, we provide a proof of Lemma II.4.
Proof of Lemma II.4: By (1) and Fermat’s little theorem,
it is easy to see that the number of polynomial functions in
k
Zp [k] is pp . Moreover, the number of distinct functions with
k
domain Zk and codomain Zp is also pp . Hence, any function
p
g : Zk → Zp is a polynomial function.
p
In the mean while, it is easy to ﬁnd injections ηi : Xi →
Zp , ∀ 1 ≤ i ≤ k and θ : f (D) → Zp , which implies that
f is equivalent to a restriction of some polynomial function
h : Zk → Zp .
p
In Lemma II.4, polynomial function h is called the polynomial presentation of f .

p(x) > 0 . The support is crucial to the cod-

i=1

ing problem for computing, since only those values (of the
function) deﬁned over the support inﬂuence the problem.
II. P OLYNOMIAL F UNCTIONS
In this section, we demonstrate how a discrete function can
be treated as a polynomial function. Readers who are familiar
with algebra could skip this section.
Deﬁnition II.1. A polynomial function1 of k variables over a
ﬁnite ﬁeld F is a function g : Fk → F of the form

Remark 3. Lemma II.4 says that all discrete functions are
polynomial functions up to the equivalent relation deﬁned
in Deﬁnition II.2. This gives a more sophisticated algebraic
structure to those functions to be considered. Such a structure
will facilitate our further discussion.

m
m1j

g(x1 , x2 , · · · , xk ) =

aj x1

m2j

x2

m

· · · xk kj ,

(1)

j=0

where aj ∈ F and m and mij ’s are non-negative integers.
From now on, Zp denotes the ﬁeld of integers modulo prime
p, Fq is a ﬁnite ﬁeld of order2 q, and F[k] is deﬁned to be
the set of all the polynomial functions of k variables over the
ﬁnite ﬁeld F.

III. F UNCTIONS WITH B IGGER C ODING R ATE R EGIONS
By Lemma II.4, we can restrict our discussion to polynomial functions, so all the functions considered later have
domains and codomains as subsets of ﬁnite ﬁelds. For s
random variables X1 , X2 , · · · , Xs , it is easily seen that the
SW region R[X1 , X2 , · · · , Xs ] gives an inner bound of the

1 polynomial
2 the

Xi is equivalent to a restrici=1

s

Xi

Yi → Ω2 , f and g are said to be equivalent if and

g :

A. Notation

x∈

Xi → Ω1 and

Deﬁnition II.2. Given two functions f :

and polynomial function are distinct concepts (cf. [12]).
number of elements of a ﬁnite ﬁeld.

2

s

coding rate region for computing any discrete function f , i.e.,
R[X1 , X2 , · · · , Xs ] ⊆ R[f ]. We consider, in the following,
under what condition(s) that R[X1 , X2 , · · · , Xs ] R[f ].

Xi → Ω and a

Theorem III.3. Given a function f :

i=1
f |supp(p)

distribution (X1 , X2 , · · · , Xs ) ∼ p, if
is a restriction
of some F ∈ F[s] which admits the structure

Lemma III.1 (Elias’ Lemma [3]). Given an i.i.d process
Z1 , Z2 , · · · , Zm , · · · , where Zi ∼ p, ∀ i, for any > 0, there
nH(Z)
exists N , such that for all n > N and k >
, there
log |Z|
k
n
exists a k × n matrix M and function ψ : Z → Z such that

F (x1 , x2 , · · · , xs ) = g(h1 (x1 ), h2 (x2 ), · · · , hs (xs )),

(5)

where g ∈ F[s], and (h1 , h2 , · · · , hs )|supp(p) is not injective,
then R[X1 , X2 , · · · , Xs ] R[f ].
Proof: Since (h1 , h2 , · · · , hs )|supp(p) is not injective,

Pr ψ(MZ n ) = Z n < ,

H (X1 , X2 , · · · , Xs ) >H (h1 (X1 ), h2 (X2 ), · · · , hs (Xs ))

where Z n is a random vector [Z1 , Z2 , · · · , Zn ]T .

=⇒

Theorem III.2. Let N = {1, 2, · · · , s}. For the polynomial
function

R[X1 , X2 , · · · , Xs ]

R[h1 (X1 ), h2 (X2 ), · · · , hs (Xs )]
⊆R[f ],

s

f (x1 , x2 , · · · , xs ) = g

hi (xi )

∈ F[s],

which establishes the theorem.

(2)

i=1

Remark 6. For (X1 , X2 , · · · , Xs ) ∼ p and 1 ≤ l ≤ s, let




OL(a, b, l, p) = c ∈
Xi (a, c), (b, c) ∈ supp(p) .



where g, hi ∈ F[1], ∀ 1 ≤ i ≤ s, R[f ] is inner bounded by the
region given by, ∀ ∅ = S ⊆ N ,
Rj ≥ I(VS ; YS |VS c ) + |S| H(Z|VN ),

i=l

(3)

j∈S

(h1 , h2 , · · · , hs )|supp(p) is not injective in Theorem III.3 implies that there exists 1 ≤ l ≤ s and a = b ∈ Xl , such that
OL(a, b, l, p) = ∅ and hl (a) = hl (b). Consequently,

where ∀ 1 ≤ j ≤ s, Yj = hj (Xj ), Vj ’s are discrete random
variables such that

f a, x(l) = f b, x(l) , ∀ x(l) ∈ OL(a, b, l, p).

p(y1 , y2 , · · · , ys , v1 , v2 , · · · , vs )
s

=p(y1 , y2 , · · · , ys )

p(vj |yj ),

(6)

In fact, if a discrete function f satisﬁes (6) for some nonempty
OL(a, b, l, p), then f |supp(p) has polynomial presentation (5)
and hl |Xl is not injective. See Lemma A.1 for details.

(4)

j=1

and Z = Y1 + Y2 + · · · + Ys .

Theorem III.4. Fix the support of all distributions considered
i=1

i=1

restriction of some F = g ◦ h ∈ F[s], where g ∈ F[1] and
s

h(x1 , x2 , · · · , xs ) =

ki (xi ), with
i=1

|Λ| > |h(Λ)|,

(7)

i.e., h|Λ is not injective, then there exists distribution
(X1 , X2 , · · · , Xs ) ∼ q with supp(q) = Λ such that
R[X1 , X2 , · · · , Xs ] R[f ].
Proof: Let (X1 , X2 , · · · , Xs ) ∼ q and supp(q) = Λ.
(7) implies that H(Z) < H(X1 , X2 , · · · , Xs ), where Z =
h(X1 , X2 , · · · , Xs ). Furthermore, there exists some small
δ > 0 and q0 with supp(q0 ) = Λ, such that sH(Z) + δ <
H(X1 , X2 , · · · , Xs ) if q = q0 . Meanwhile, it can be shown
δ
that by Elias’ Lemma Ri =
+ H(Z), ∀1 ≤ i ≤ s,
s+1s

R(F, VN ) ,
F ∈F VN ∈V

where R(F, VN ) is given by (3) and cov(D) is deﬁned to be
the convex hull of set D ⊆ Rs .
s

hi (xi )

Proof of Theorem III.2: By (2), if the sum

Xi → Ω, if f |Λ is a

Xi . For function f :

to be Λ ⊆

Remark 5. A function could have different polynomial presentations over distinct ﬁnite ﬁelds. For example, the function
min{x, y} deﬁned on {0, 1} × {0, 1} can either be seen as
F1 (x, y) = xy on Z2 or be treated as the restriction of
2
F2 (x, y) = x + y − (x + y)2 (on Z2 ) to the domain {0, 1} ×
3
{0, 1} ⊂ Z2 . Consequently, let F be the set of all polynomial
3
presentations of f with format (2) and V be the set of all VN
satisfying (4). We have R[f ] ⊇ cov

s

s

Remark 4. The Theorem III.2 is a direct generalization of the
result by Ahlswede and Han [6, Theorem 10], and its proof is
similar to theirs. It resumes [6, Theorem 10] if s = 2, F = Z2
and g and hi ’s are identity functions.

i=1

Ri < sH(Z) + δ <

is achievable (see [5]). Therefore,

is successfully recovered by the decoder, so is the function f . Therefore, achievability follows from the fact that
(R1 , R2 , · · · , Rs ) that satisﬁes (3) is achievable for computing

i=1

H(X1 , X2 , · · · , Xs ) which implies R[X1 , X2 , · · · , Xs ]
R[f ] if q = q0 .

s

hi (xi ).

Remark 7. In fact, any discrete function can be written as a
restriction of some F = g ◦h ∈ F[s] with h(x1 , x2 , · · · , xs ) =

i=1

3

s

expect that this polynomial approach will provide additional
insight into this problem in the future.

ki (xi ) (see Appendix A for more details). However, (7)
i=1

is not satisﬁed in general, and examples include the identity
function and the function f {0, 1} × {0, 1} → {0, 1, 2} given
0;
if b = 0,
by f (a, b) =
a + b; if b = 1.

A PPENDIX A
D ISCRETE F UNCTIONS AS P OLYNOMIAL F UNCTIONS
Lemma A.1. Let (X1 , X2 , · · · , Xs ) ∼ p. A discrete function f
satisﬁes (6) for some nonempty OL(a, b, l, p) with a = b ∈ Xl
(1 ≤ l ≤ s), if and only if f |supp(p) has polynomial presentation
(5) and (h1 , h2 , · · · , hs )|supp(p) is not injective.

Corollary III.5. In Theorem III.4, condition (7) can be
replaced by
Deg(g) < max
ω∈Ω

f −1 (ω)

Λ

,

Proof: Without loss of generality, assume that l = 1, and
let Λ = supp(p). If f satisﬁes (6) for a nonempty OL(a, b, 1, p)

(8)

s

while the statement holds true.
Proof: Let ω0 = arg max
ω∈Ω

f −1 (ω)

˜
with a = b ∈ X1 , then there exists f :
Λ

and ρ =

s

˜
˜
f (a, c) = f (b, c), ∀ c ∈

f −1 (ω0 ) Λ . We know that the polynomial function g −ω0
has at most Deg(g − ω0 ) = Deg(g) zeros. Thus, h|Λ is not
injective, otherwise, g − ω0 has at least ρ zeros. ρ > Deg(g),
a contradiction.

Xi → Ω such that
i=1

˜
Xi , and f |Λ = f |Λ . Let g be any
i=2

˜
polynomial presentation of f , and deﬁne function h1 : F → F
a; if c ∈ {a, b},
by h1 (c) =
Then F = g(h1 , h2 , · · · , hs ),
c; otherwise.
where hi (x) = x, ∀ 2 ≤ i ≤ s, is the required polynomial
presentation of f |Λ . Moreover, (h1 , h2 , · · · , hs )|Λ is not injective, since h1 (a) = h1 (b). Sufﬁciency is obvious.

Remark 8. The min function in Remark 5 together with
its polynomial presentation F2 gives an example of functions
illustrated in Theorem III.4 and Corollary III.5.
Theorem III.6. Fix the support of all distributions considered
to be Λ ⊆ X1 × X2 . For function f : X1 × X2 → Ω, there
exists joint distribution (X1 , X2 ) ∼ p with supp(p) = Λ such
that R[X1 , X2 ] R[f ], if and only if f |Λ is a restriction of
some F ∈ F[2] which admits the structure

k

[0, mi −

Lemma A.2. Given any discrete function f :

i=1
k

mi be a prime. There exists g ∈

1] → Ω, and let p ≥
i=1

F (x1 , x2 ) = g(k1 (x1 ) + k2 (x2 )),

Zp [1], such that f is equivalent to g ◦ hp |D1 ×D2 ×···×Dk , where

(9)

k

where g ∈ F[1], and (k1 + k2 )|Λ is not injective.

xi ∈ Zp [k] and

hp (x1 , x2 , · · · , xk ) =
i=1

Remark 9. Theorem III.6 says that, in the case s = 2, the
necessary condition of Theorem III.4 is also sufﬁcient.

i−1

Di = {0, di , 2di , · · · , (mi − 1)di } ⊆ Zp , di =

Proof of Theorem III.6: ⇐ follows from Theorem III.4.
The other direction, ⇒, is proved in Appendix B.

mj .
j=1

Moreover, if f (a1 , a2 , · · · , ak ) = f (b1 , b2 , · · · , bk ) and ai =
bi , ∀ 1 ≤ i ≤ k, then f is equivalent to g ◦ hp |D1 ×D2 ×···×Dk
˜
for some g ∈ Zp [1], where

Remark 10. Essentially, Theorem III.6 gives another interpretation of [1, Theorem 1]. In the case of Λ = X1 ×X2 , one way
to prove Theorem III.6 is to show that its sufﬁcient condition
is equivalent to the inverses of (3.1), (3.11) and (3.13) of [1].

˜
Dk = {0, dk − 1, 2dk , · · · , (mk − 1)dk } ⊆ Zp .
Proof: Let D = D1 ×D2 ×· · ·×Dk and ηi be the bijection
from [0, mi − 1] to Di given by ηi : t → tdi . It is easy to
verify that hp |D is injective. Besides, there exists injection θ :

IV. C ONCLUSION
Considering discrete functions as polynomial functions provides several advantages. A polynomial presentation gives
a function a much more sophisticated algebraic structure
compared to its matrix presentation. This could facilitate
investigating properties of the function and unearthing useful
information. In Theorem III.4, we have seen that a “nice”
summing structure can be utilized via the linear random coding
technique to obtain a bigger coding rate region. For the twosources scenario, Theorem III.6 claims that such a summing
structure is the only valuable one in terms of enlarging the
coding rate region. To the best of our knowledge, for the
multiple-sources scenario, the question of what is the sufﬁcient and necessary condition so that the coding rate region
coincides with the SW region is unanswered (see [1]). We

k

k

[0, mi − 1]

f
i=1

→ Zp , since f

Let g be a function satisfying

[0, mi − 1]

≤ p.

i=1

g(hp (η1 (t1 ), η2 (t2 ), · · · , ηk (tk ))) = θ(f (t1 , t2 , · · · , tk )),
∀ ti ∈ [0, mi − 1], ∀ 1 ≤ i ≤ k.

(10)

By Lemma II.4, g ∈ Zp [1]. By deﬁnition, f is equivalent to
g ◦ hp |D1 ×D2 ×···×Dk .
Furthermore, if f (a) = f (b1 , b2 , · · · , bk ), where a =
(a1 , a2 , · · · , ak ), and ai = bi , ∀ 1 ≤ i ≤ k, without loss
of generality, assume that ai = mi − 1, ∀ 1 ≤ i ≤ k − 1,
ak = b1 = b2 = · · · = bk−1 = 0 and bk = 1. Redeﬁne

4

tdk ;
if t = 1,
We
dk − 1; if t = 1.
is injective and

and V2, whose sample spaces are bounded by a ﬁxed ﬁnite
number ρ. At the same time, H(XJ |V1, , V2, , XJ c , QJ c , ) >
δ, > H(Z|V1, , V2, , XJ c , QJ c , ) and V1, , −X1 − X2 −
V2, , |Qj, forms a Markov chain for all j = 1, 2.
Now, let
go to 0, the continuity of entropy guar˜ ˜
˜
antees that H(XJ |V1 , V2 , XJ c , QJ c ) ≥ δ and 0 =
˜1 , V2 , XJ c , QJ c ) for new random variables Q1 , Q2 , V1
˜
˜
˜ ˜ ˜
H(Z|V
˜2 whose pmf ’s are the limit pmf ’s of Q1, , Q2, , V1,
and V
˜
˜ ˜
and V2, , respectively. Furthermore, V1 − X1 − X2 − V2 |Qj
forms a Markov chain for all j = 1, 2.
˜ ˜
˜
1) If J = {1}, i.e., H(X1 |V1 , V2 , X2 , Q2 ) ≥ δ, then there
exist a = b ∈ X1 and c ∈ X2 , such that f (a, c) = f (b, c)
˜ ˜
˜
since 0 = H(Z|V1 , V2 , X2 , Q2 ). On the other hand,
˜
˜
˜
since V1 − X1 − X2 − V2 |Q2 is a Markov chain,
then ∀ x2 ∈ X2 with p(a, x2 )p(b, x2 ) > 0, f (a, x2 ) =
˜ ˜
˜
f (b, x2 ). Otherwise, 0 < H(Z|V1 , V2 , X2 , Q2 ), a contradiction. By Lemma A.1 and the ﬁrst half of Lemma
A.2, f |Λ has polynomial presentation (9) with a noninjective k1 . Hence, k1 + k2 |Λ is not injective. Similar
conclusion can be drawn if J = {2}.
˜ ˜
2) If J = {1, 2}, i.e., H(X1 , X2 |V1 , V2 ) ≥ δ, then there
exist (a, c) = (b, d) ∈ Λ, such that f (a, c) = f (b, d). If
˜ ˜
˜
c = d (or a = b), then H(X1 |V1 , V2 , X2 , Q2 ) ≥ δ (or
˜ ˜
˜
H(X2 |V1 , V2 , X1 , Q1 ) ≥ δ). Then the same conclusion
is reached by the same argument as in 1). Assume that
a = b and c = d. By the second half of Lemma A.2,
f |Λ processes a polynomial presentation (9) with noninjective k1 + k2 |Λ .

˜
ηk : [0, mk − 1] → Dk by ηk (t) =
have hp |{D1 ×D2 ×···×Dk }\{a}
˜

hp (η1 (a1 ), η2 (a2 ), · · · , ηk (ak ))
=hp (η1 (b1 ), η2 (b2 ), · · · , ηk (bk )),
i.e., hp |D1 ×D2 ×···×Dk is non-injective. Similarly, there exists
˜
g ∈ Zp [1], such that (10) holds true for g = g . Therefore, f
is equivalent to g ◦ hp |D1 ×D2 ×···×Dk .
˜
A PPENDIX B
P ROOF OF S UFFICIENCY OF T HEOREM III.6
R[X1 , X2 ]
R[f ] implies that there exists (R1 , R2 ) ∈
R[f ] while (R1 , R2 ) ∈ R[X1 , X2 ]. That is, for some ﬁxed
/
δ > 0, R1 < H(X1 |X2 ) − δ, or R2 < H(X2 |X1 ) − δ or R1 +
R2 < H(X1 , X2 ) − δ. Moreover, ∀ > 0, there exists a big
enough n, two encoders E1 , E2 and one decoder D, such that
n
n
Pr {Z n = D(E1 (X1 ), E2 (X2 ))} < , where Z = f (X1 , X2 ).
n
Let Wi = Ei (Xi ), i = 1, 2. We have, for ∅ = J ⊆ {1, 2},
n
n
n
Rj ≥ H(WJ ) ≥ H(WJ |XJ c ) ≥ I(WJ ; XJ |XJ c )

n
j∈J

n
n
n
n
=H(XJ |XJ c ) − H(XJ |WJ , XJ c )
n
n
n
n
=H ( XJ | XJ c ) − H ( XJ | WJ , WJ c , XJ c )
n

n

i−1
i
n
H XJ WJ , XJ , WJ c , XJ c

i
i
H XJ XJ c −

=

i=1

i=1
(A)

=nH XJ

(A)

XJ c , A

(A)

− nH XJ

R EFERENCES

A−1
n
WJ , XJ , WJ c , XJ c , A ,

[1] T. S. Han and K. Kobayashi, “A dichotomy of functions f(x, y) of
correlated sources (x, y) from the viewpoint of the achievable rate
region,” IEEE Transactions on Information Theory, vol. 33, pp. 69–76,
Jan. 1987.
[2] D. Slepian and J. K. Wolf, “Noiseless coding of correlated information
sources,” IEEE Transactions on Information Theory, vol. 19, pp. 471–
480, July 1973.
[3] P. Elias, “Coding for noisy channels,” IRE Conv. Rec., pp. 37–46, March
1955.
[4] R. G. Gallager, Information Theory and Reliable Communication. New
York: Wiley, 1968.
[5] J. K¨ rner and K. Marton, “How to encode the modulo-two sum of binary
o
sources,” IEEE Transactions on Information Theory, vol. 25, pp. 219–
221, Mar. 1979.
[6] R. Ahlswede and T. S. Han, “On source coding with side information via
a multiple-access channel and related problems in multi-user information
theory,” IEEE Transactions on Information Theory, vol. 29, pp. 396–411,
May 1983.
[7] A. Orlitsky and R. Roche, “Coding for computing,” IEEE Transactions
on Information Theory, vol. 47, Mar. 2001.
[8] H. Witsenhausen, “The zero-error side information problem and chromatic numbers,” IEEE Transactions on Information Theory, vol. 22,
pp. 592–593, September 1976.
[9] B. Nazer and M. Gastpar, “Computation over multiple-access channels,”
IEEE Transactions on Information Theory, vol. 53, Oct. 2007.
[10] R. Appuswamy, M. Franceschetti, N. Karamchandani, and K. Zeger,
“Network coding for computing: Cut-set bounds,” IEEE Transactions
on Information Theory, vol. 57, Feb. 2011.
[11] R. Appuswamy, M. Franceschetti, N. Karamchandani, and K. Zeger,
“Linear codes, target function classes, and network computing capacity,”
IEEE Transactions on Information Theory, Submitted.
[12] R. Lidl and H. Niederreiter, Finite Fields. New York: Gambridge
University Press, 2nd ed., 1997.

where A is a random variable which is uniformly distributed
over {1, 2, · · · , n} and independent to all the other random
variables. Suppose
Rj < H(XJ |XJ c ) − δ, then
j∈J
(A)

(A)

[A]

A−1
A−1
H(XJ |WJ , XJ , WJ c , XJ c , XJ c , XJ c , A) > δ. (11)

Besides, by Fano’s inequality,
n

n ≥H ( Z n | WJ , WJ c ) =

H Z (i) WJ , WJ c , Z i−1
i=1

n
i−1
n
H Z (i) WJ , WJ c , Z i−1 , XJ , XJ c

≥
i=1

A−1
n
=nH Z (A) WJ , XJ , WJ c , XJ c , A .

Thus,
(A)

[A]

A−1
A−1
> H(Z (A) |WJ , XJ , WJ c , XJ c , XJ c , XJ c , A). (12)
(A)

[A]

Let Xi = Xi , Z = Z (A) , Qi = (Xi , A) and Vi =
A−1
(Wi , Xi ) for i = 1, 2. Then H(XJ |V1 , V2 , XJ c , QJ c ) > δ
and > H(Z|V1 , V2 , XJ c , QJ c ) by (11) and (12). Moreover,
V1 − X1 − X2 − V2 |Qj forms a Markov chain for all j = 1, 2.
By the Caratheodory theorem, Q1 , Q2 , V1 and V2 can be replaced, respectively, by new random variables Q1, , Q2, , V1,

5


Title:          untitled
Creator:        'Certified by IEEE PDFeXpress at 05/14/2012 10:45:29 AM'
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon May 14 10:45:19 2012
ModDate:        Tue Jun 19 12:54:30 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      342282 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569565113

On Lossless Universal Compression of
Distributed Identical Sources
Ahmad Beirami and Faramarz Fekri
School of Electrical and Computer Engineering
Georgia Institute of Technology, Atlanta GA 30332, USA
Email: {beirami, fekri}@ece.gatech.edu
Abstract—Slepian-Wolf theorem is a well-known framework
that targets almost lossless compression of (two) data streams
with symbol-by-symbol correlation between the outputs of (two)
distributed sources. However, this paper considers a different
scenario which does not ﬁt in the Slepian-Wolf framework. We
consider two identical but spatially separated sources. We wish to
study the universal compression of a sequence of length n from
one of the sources provided that the decoder has access to (i.e.,
memorized) a sequence of length m from the other source. Such
a scenario occurs, for example, in the universal compression of
data from multiple mirrors of the same server. In this setup, the
correlation does not arise from symbol-by-symbol dependency
of two outputs from the two sources. Instead, the sequences are
correlated through the information that they contain about the
unknown source parameter. We show that the ﬁnite-length nature
of the compression problem at hand requires considering a notion
of almost lossless source coding, where coding incurs an error
probability pe (n) that vanishes with sequence length n. We obtain
a lower bound on the average minimax redundancy of almost
lossless codes as a function of the sequence length n and the
permissible error probability pe when the decoder has a memory
of length m and the encoders do not communicate. Our results
demonstrate that a strict performance loss is incurred when the
two encoders do not communicate even when the decoder knows
the unknown parameter vector (i.e., m → ∞).

Fig. 1.

The basic scenario for the compression of distributed sources.

The Slepian-Wolf theorem naturally suits applications where
the (new) sequence xn from S2 (in Fig. 1) can be viewed
as a noisy version of the (previously seen) sequence y m that
could possibly be exploited as side information to reduce the
code length of xn . Data gathering from sensors that measure
the same phenomenon is one example. However, in many
scenarios, the compression of distributed sources cannot be
modeled by the SW framework. As an example, consider
the universal compression of data from the mirrors of the
same server, where the sources are exact copies of each other.
Hence, it is plausible to assume that the sources (S1 and S2
in Fig. 1) follow the same statistical model. On the other
hand, the source model might be unknown requiring universal
compression [5]–[7]. The question is, assuming two identical
sources S1 and S2 and having y m from S1 at the decoder,
what is the achievable universal compression performance on
xn at S2 provided that the encoders at S1 and S2 do not
communicate.
We stress that the nature of this problems is fundamentally
different from those addressed by the Slepian-Wolf (SW)
theorem in [1]. Here, instead of symbol-by-symbol correlation
between the sequences as in SW setup, the redundancy is due
to the fact that when the source parameter is a priori unknown
there is signiﬁcant overhead in the universal compression of
ﬁnite-length sequences [7]–[9]. Considering the example in
Fig. 1 with two identical sources S1 and S2 , y m and xn
would be independent given that the source model is known.
However, when the source parameter is unknown, y m and xn
are correlated with each other through the information they
contain about the unknown source parameter. The question is
whether or not this correlation can be potentially leveraged by
the encoder of S2 and the decoder at R in the decoding of xn
using y m in order to reduce the code length of xn .
In this paper, we study the universal compression of distributed identical sources. By identical we mean that the
sources (S1 and S2 ) share the same unknown source parameter.
By distributed we mean that the sources are spatially separated
and the encoders do not communicate with each other. This
problem can also be viewed as universal compression with
training data that is only available to the decoder. It is known
that forming a statistical model from a training data set would
improve the performance of universal compression [10], [11].
In [9], [12], we theoretically derived the gain that is obtained

I. I NTRODUCTION
Many practical applications involve compression of data that
are taken from multiple spatially separated sources. A key
challenge in most of such applications is that the sources usually cannot communicate with each other. Theoretical results
by Slepian and Wolf demonstrate that if the data streams from
two sources have symbol-by-symbol correlation, the sequences
can be compressed to their joint entropy even when the two
encoders do not communicate [1]. In other words, as in Fig. 1,
assume that sources S1 and S2 wish to transmit the sequences
y n and xn , respectively, to a node R. As the length n of the
sequences increases, the decoding of xn at R with the help
of y n can be performed using a code with the average length
that asymptotically approaches the conditional entropy, (i.e.,
H(X n |Y n )) with asymptotically zero error probability. If the
decoder did not choose to use y n in decoding, the encoder at
S2 would have to encode the sequence xn irrespective to y n
with an average length that is lower bounded by H(X n ). Note
that the conditional entropy H(X n |Y n ) may be signiﬁcantly
smaller than the individual entropy H(X n ). After recent
development of practical Slepian-Wolf (SW) coding schemes
by Pradhan and Ramchandran [2], SW coding has drawn a
great deal of attention as a promising technique for sensor
networks [3] and distributed video coding [4].
This material is based upon work supported by the National Science
Foundation under Grant No. CNS-1017234.

1

ˆ
there exists a reverse mapping dpe (·) : {0, 1}∗ → An such
n
that
E{1e (X n )} ≤ pe (n),

in the universal compression of the new sequence xn from S2
by memorizing (i.e., having access to) y m from S1 at both
the decoder (at R) and the encoder (at S2 ). This corresponds
to the reduced case of our problem where the sources S1
and S2 are either co-located (a single source) or allowed to
communicate. For the reduced problem case, in [11], [13], we
further extended the setup to a network with a single source
and derived bounds on the network-wide gain where a small
fraction of the intermediate nodes in the network are capable
of memorization. However, as we demonstrate in the present
paper, the extension to the multiple spatially separated sources,
where the training data is only available to the decoder, is
non-trivial and raises a new set of challenges that we aim
to address. The rest of this paper is organized as follows.
In Sec. II, we brieﬂy review the necessary background. In
Sec. III, we describe the problem setup. In Sec. IV, we present
our main results. In Sec. V, we provide discussion on the
results. In Sec. VI, we present the technical analysis of the
results. Finally Sec. VII concludes the paper.
II. BACKGROUND R EVIEW
In this section, we review the necessary background, notations, and deﬁnitions followed by the formal problem setup.
Following the notation in [12], let A be a ﬁnite alphabet.
Let d be the number of the source parameters. Further, let
θ = (θ1 , ..., θd ) denote the d-dimensional parameter vector associated with the parametric source (that is a priori unknown).
Let Θd denote the space of d-dimensional parameter vectors.
We denote μθ as the probability measure that is deﬁned by
the parameter vector θ. Let P d denote the family of sources
that are described with the d-dimensional unknown parameter
vector θ ∈ Θd . We use the notation xn = (x1 , ..., xn ) ∈ An to
present a sequence of length n from the alphabet A. We further
denote X n as a random sequence of length n (that follows the
probability distribution μθ ). Let Hn (θ) be the source entropy
1
given θ, i.e., Hn (θ) = E log μθ (X n ) .1
Let cn : An → {0, 1}∗ be an injective mapping from
the set An of the sequences of length n over A to the set
{0, 1}∗ of binary sequences. Next, we present the notions of
strictly lossless and almost lossless source codes, which will
be needed for the study of UC-DIS.

where 1e (xn ) denotes the error indicator function, i.e,
1e (xn ) =

1
0

ˆ c
dpe (ˆpe (xn )) = xn ,
n
n
otherwise.

The almost lossless codes allow a non-zero error probability
pe (n) for any n while they are almost surely asymptotically
error free. Note that strictly lossless codes correspond to
pe (n) = 0. The proofs of Shannon [14] for the existence of
entropy achieving source codes are based on almost lossless
random codes. Further, the proof of the SW theorem [1]
also uses almost lossless codes. Further, all of the practical
implementations of SW source coding are based on almost
lossless codes (cf. [2], [3]). We stress that the nature of the
almost lossless source coding is different from that incurred
by the lossy source coding (i.e., the rate-distortion theory). In
the rate-distortion theory, a code is designed to asymptotically
achieve a given distortion level as the length of the sequence
grows to inﬁnity. Therefore, since the almost lossless coding
asymptotically achieves a zero-distortion, in fact, it coincides
with the special case of zero-distortion in the rate-distortion
curve.
III. P ROBLEM S ETUP
We present the problem setup in the most basic scenario,
shown in Fig. 1, consisting of two identical sources located
in nodes S1 and S2 , and the destination node R. We let
the information sources at S1 and S2 be parametric with an
identical d-dimensional parameter vector that is unknown a
priori to the encoder and the decoder. Let y m and xn denote
two sequences with lengths m and n, respectively, that are
generated by the unknown information source model. In the
sequel, we describe the communication scenario for universal
compression of distributed identical sources. We assume that
S1 has transmitted the sequence y m to R. Next, at some later
time, S2 wishes to send xn to R. We further assume that R
is a memory unit and is capable of memorizing the sequence
y m . We investigate the achievable saving in the compression of
xn in the S2 -R link when R has memorized the sequence y m .
Note that S2 does not have access to the sequence y m . If the
node R did not have a memory unit, S2 would have to apply
an end-to-end universal compression to xn . However, the side
information provided by y m at R about the source parameter
can potentially result in a reduction in the amount of bits
required to be transmitted in the S2 -R link. Throughout the
paper, we refer to this problem setup as Universal Compression
of Distributed Identical Sources (UC-DIS).
In the study of coding strategies for UC-DIS, we compare
the following cases for the compression of xn at S2 . Note that
we assume that y m is already universally compressed at S1
and transmitted and decoded at R.
• UComp (Universal compression), which only applies
end-to-end lossless universal compression to xn at S2
without regard to y m .
• UCompM (Universal compression with memorization at
both the encoder and the decoder), which assumes that

Deﬁnition 1 The code cn (·) : An → {0, 1}∗ is called
strictly lossless (also called zero-error) if there exists a reverse
mapping dn (·) : {0, 1}∗ → An such that
∀xn ∈ An : dn (cn (xn )) = xn .
All of the practical data compression schemes are examples of
strictly lossless codes, namely, the arithmetic coding, Huffman,
Lempel-Ziv, and Context-Tree-Weighting algorithms.
On the other hand, due to the distributed nature of the
sources, we are concerned with the slightly weaker notion of
almost lossless source coding in this paper.
Deﬁnition 2 The code cpe (·) : An → {0, 1}∗ is called almost
ˆn
lossless with permissible error probability pe (n) = o(1), if
1 Throughout this paper, all expectations are taken with respect to the
probability measure µθ , and log(·) denotes the logarithm in base 2.

2

the encoder (at S2 ) and the decoder (at R) have access to
a common memory (i.e., sequence y m ), which is utilized
in the lossless compression of xn at S2 .
• DUCompM (Distributed universal compression with
memorization at the decoder), which assumes that decoder (at R) has memorized (i.e., has access to) y m while
the encoder (at S2 ) only knows the length m of the side
information but does not know the exact sequence y m .
The encoder then applies an almost lossless code to xn
that is decoded at R with permissible error probability pe
using y m .
Note that UComp does not beneﬁt from the memorization and
is the conventional scheme. Further, UCompM is introduced as
the benchmark for the purpose of evaluating the performance
of DUCompM and is not practically useful since it requires
the sequence y m from S1 to be available at the encoder of S2 .
Let ln (xn ) denote the strictly lossless length of the codeword associated with the sequence xn . Further, let Ln denote
the space of strictly lossless universal length functions on a
sequence of length n. Denote Rn (ln , θ) as the expected redundancy of such strictly lossless codes on a sequence of length n
for the parameter vector θ, i.e., Rn (ln , θ) = Eln (X n )−Hn (θ).
¯
Further, denote RUComp (n) as the average minimax redundancy
as given by
¯
RUComp (n)

min sup Rn (ln , θ).

ln ∈Ln θ∈Θd

In the case of strictly lossless UComp, Clarke and Barron
¯
derived the expected minimax redundancy RUComp (n) for
memoryless sources [15], which was later generalized by
Atteson for Markov sources, as the following [16]:
Theorem 1 The average minimax redundancy of strictly lossless UComp coding strategy is given by
n
d
¯
+ log
RUComp (n) = log
2
2πe

min

sup Rn (ln|m , θ).

ln|m ∈Ln|m θ∈Θd

min

sup Rn (ˆn|m , θ).
lpe

ˆ e ∈Lpe θ∈Θd
ˆ
lp
n|m
n|m

,

In the case of strictly lossless UCompM (i.e., when the two
encoders can communicate), we obtain the average minimax
redundancy in the following theorem.
Theorem 2 The average minimax redundancy of strictly lossless UCompM coding strategy is given by
n
d
¯
RUCompM (n, m) = log 1 +
+O
2
m

1
1
+
m n

.

In the next proposition, we conﬁne ourselves to strictly
lossless codes in the DUCompM strategy.
Proposition 3 The average minimax redundancy of strictly
lossless DUCompM coding strategy is equal to that of UComp
¯
¯
coding strategy. That is RDUCompM (n, m) = RUComp (n).

(1)

Finally, in the case of almost lossless DUCompM, our main
result is given in the following theorem.
Theorem 4 The average minimax redundancy of almost lossless DUCompM coding strategy is upper bounded by
1
1
+
,
m n

¯ pe
¯
RDUCompM (n, m) ≤ RUCompM (n, m)+F (d, pe ) +O

where F (d, pe ) is the penalty term due to the encoders not
communicating, which is given by

(2)

In DUCompM, let ˆn|m denote the almost lossless universal
lpe
length function with a memorized sequence of length m that is
only available to the decoder, where the permissible error probˆ
ability on decoding xn is pe . Further, denote Lpe as the space
n|m
lpe
of such universal length functions. Denote Rn (ˆn|m , θ) as the
expected redundancy of encoding a sequence xn of length n
¯ pe
using the length function ˆn|m . Denote RDUCompM (n, m) as the
lpe
expected minimax redundancy as given by
¯ pe
RDUCompM (n, m)

1
n

where In (θ) is the Fisher information matrix.

In UCompM, let ln|m be the strictly lossless universal
length function with a memory sequence of length m. Denote
Ln|m as the space of such strictly lossless universal length
functions. Let Rn (ln|m , θ) be the expected redundancy of
encoding a sequence of length n form the source μθ using
¯
the length function ln|m . Further, let RUCompM (n, m) denote
the corresponding average minimax redundancy, i.e.,
¯
RUCompM (n, m)

1

|In (θ)| 2 dθ + O

F (d, pe ) =

2
1
d
log 1 +
log
2
d log e
pe

.

(4)

V. D ISCUSSION ON THE R ESULTS
In this section, we provide some discussion on the significance of the results for different UC-DIS coding strategies.
Figures 2 and 3 demonstrate the redundancy rate for the three
coding strategies, namely, UComp, UCompM, and DUCompM
for memoryless sources and ﬁrst-order Markov sources with
alphabet size k = 256, respectively. In the case of UComp,
Theorem 1 deﬁnes the achievable average minimax redundancy for the compression of a sequence of length n encoded
without regard to the previously seen sequence y m .
According to Theorem 2, if the encoder and the decoder
have access to a common memory y m , i.e., UCompM coding
strategy, the average minimax redundancy could be much
smaller than that of UComp depending on how large m is. In
¯
particular, when m → ∞ we have limm→∞ RUCompM (n, m) =
2
0. This corresponds to the case where the parameter vector

(3)

¯
¯0
Note that we denote RDUCompM (n, m)
RDUCompM (n, m)
as the expected minimax redundancy of strictly lossless
DUCompM coding strategy.
IV. P ERFORMANCE E VALUATION OF UC-DIS:
R ESULTS ON THE AVERAGE M INIMAX R EDUNDANCY
In this section, we provide results on the average minimax
redundancy of the different coding strategies introduced in the
previous section for the UC-DIS problem. Discussion on the
implications of the results and the proof sketches are deferred
to Sec. V and Sec. VI, respectively.

2 In this paper, we ignored the integer constraint on the length functions,
which results in a negligible O(1) redundancy analyzed in [17], [18].

3

0.7

0.7
UComp
DUCompM (pe = 10−40 )
DUCompM (pe = 10−6 )
UCompM

UComp
DUCompM (pe = 10−40 )
DUCompM (pe = 10−6 )
UCompM

0.6

Redundancy rate

Redundancy rate

0.6
0.5
0.4
0.3

0.5
0.4
0.3

0.2

0.2

0.1

0.1

0
512B

4kB

n

32kB

0
128kB

256kB

for the UC-DIS problem. Memory size is m = 32kB and the source
is memoryless with alphabet size k = 256.

64MB

Proof: It can be shown that the minimax redundancy is
equal to the capacity of the channel between the unknown
parameter vector θ and the sequence xn given the sequence
y m (cf. [8] and the references therein). Thus,
¯
RUCompM (n, m) = sup I(X n ; θ|Y m )
ω(θ)

= sup{I(X n , Y m ; θ) − I(Y m ; θ)}
ω(θ)

≥ {I(X n , Y m ; θ) − I(Y m ; θ)}|θ∝ωJ (θ)
¯
¯
= RUComp (n + m) − RUComp (m),
(5)
1
2

|I(θ)|
where ωJ (θ)
denotes the Jeffreys’ prior, and
1
|I(β)| 2 dβ
¯
RUComp (·) is given in Theorem 1. Further simpliﬁcation of (5)
leads to the desired result in Lemma 1.

B. Sketch of the Proof of Proposition 3
Since the source is assumed to be from the family P d
of d-dimensional parametric sources, in particular, it is also
an ergodic source. Thus, any pair (xn , y m ) occurs with nonzero probability and the support set of (xn , y m ) is equal to
An × Am . Therefore, Proposition 3 trivially follows from the
known results on strictly lossless compression (cf. [20] and
the references therein).
C. Sketch of the Proof of Theorem 4
We provide a constructive optimal coding strategy at the encoder and obtain its achievable average minimax redundancy,
which provides with an upper bound on the average minimax
redundancy of the almost lossless DUCompM coding strategy.
ˆ
ˆ
Let θ(xn ) (or θ(y m )) denote the Maximum Likelihood
(ML) estimate for the unknown source parameter given
ˆ
that the sequence xn (or y m ) is observed, i.e., θ(xn )
ˆ
ˆ
ˆ
ˆ
arg maxλ μλ (xn ). Further, let θX θ(xn ) and θY
θ(y m ).
As discussed earlier μθ (xn ) is the probability distribution
induced by the parameter vector θ on the sequence xn . It is
ˆ
straightforward to derive the pmf of the ML-estimate p(θX |θ)
from μθ (xn ) by summing over all the sequences that correˆ
spond to the same ML-estimate. Note that θX follows a discrete distribution only taking values on a ﬁnite set of (n + 1)d
points in the space Θd . For any λ, θ ∈ Θd , let Dn (μλ ||μθ )
n
μθ
be the KL-divergence, i.e., Dn (μλ ||μθ )
E log μλ(X n) .
(X )

VI. T ECHNICAL A NALYSIS
A. Sketch of the Proof of Theorem 2
We prove that the RHS is both an upper bound and a lower
¯
bound for RUCompM (n, m). The upper bound is obtained using
the KT-estimator [19] along with a proper Shannon code [14]
and the proof follows the analysis of the redundancy of the
KT-estimator. In the next lemma, we obtain the lower bound.
Lemma 1 The average minimax redundancy of UCompM is
lower-bounded by
1
1
+
m n

8MB

for the UC-DIS problem. Memory size is m = 16MB and the source
is ﬁrst-order Markov with alphabet size k = 256.

is known to both the encoder and the decoder, and thus, the
redundancy is zero similar to a perfect Shannon code. Hence,
the fundamental limits are those of known source parameters
and universality no longer imposes a compression overhead.
This is also demonstrated in Figs. 2 and 3, where m has been
chosen to be sufﬁciently large.
Proposition 3 demonstrates that if strictly lossless
DUCompM coding strategy (i.e., pe = 0) is to be used for the
compression of xn from S2 , the memorization of y m from
S1 only at the decoder does not provide any compression
beneﬁt, assuming that the two encoders at S1 and S2 do not
communicate. In other words, the best that S2 can do is to
simply apply a traditional universal compression on xn .
Finally, according to Theorem 4, unlike the asymptotic
behavior of the Slepian-Wolf problem, the distributed nature in
this problem incurs an extra redundancy on the compression.
As can be seen in Fig. 2, the overhead can be signiﬁcant in
the compression of memoryless sources. For example, when
n = 512B, m = 32kB, and pe = 10−6 , the redundancy rate is
around 0.05, as compared with the almost zero redundancy rate
of UCompM. On the other hand, as demonstrated in Fig. 3,
when d is relatively larger, for medium length sequences even
with extremely small error probability, DUCompM performs
fairly close to UCompM. Further, DUCompM by far outperforms UComp in the compression of short to medium
length sequences with reasonable permissible error probability,
1
d,
justifying usefulness of DUCompM in practice. If log pe
the penalty term can be further simpliﬁed to be approximately
1
equal to F (d, pe ) ≈ log pe for the practical ranges of pe .

n
d
¯
+O
RUCompM (n, m) ≥ log 1 +
2
m

1MB

n
Fig. 3. The redundancy rate for the three coding strategies of interest

Fig. 2. The redundancy rate for the three coding strategies of interest

.

4

ˆ
It can be shown that expectations with respect to p(θX |θ)
˜X (with uniformly
can be performed using a continuous RV θ
vanishing error) whose distribution conditioned on θ is given
by

VII. C ONCLUSION
In this paper, we introduced and studied the problem of
Universal Compression of Distributed Identical Sources (UCDIS), which is a more favorable framework as compared to
the Slepian-Wolf (SW) framework in several applications, such
as the compression of data from mirrors of a data server.
In UC-DIS, the correlation among outputs of the sources
is due to the ﬁnite-length universal compression constraint,
departing from the nature of the correlation in the SW
framework. For UC-DIS, involving two identical sources, we
introduced DUCompM coding strategy (compression using
the side information at the decoder when the two encoders
do not communicate) and obtained an upper bound on its
average minimax redundancy. We demonstrated that for ﬁnitelength sequences with reasonable permissible error probability,
DUCompM coding strategy by far outperforms traditional
universal compression, and hence, justifying the usefulness of
DUCompM coding strategy in practice.

n d
2
exp(−Dn (μθX ||μθ )), (6)
˜
2π
where n has to be large enough so that Stirling’s approximation can be applied. Further, it is straightforward to show that
this distribution can be approximated using a Gaussian distribution with mean θ and inverse covariance matrix nIn (θ).
Next, we will obtain an approximation for the distribution
ˆ
ˆ
of θX conditioned on θY .
˜
˜
p(θX |θ) = |In (θX )| 2
1

ˆ
ˆ
Lemma 2 Let θX and θY denote the ML-estimate parameter
given observed sequences xn and y m , respectively. Further, let
ˆ
˜ ˆ
p(θX |θY ) follow a Gaussian distribution with mean θY and
nm
ˆY ). Then, all expectations
inverse covariance matrix n+m Im (θ
˜ ˆ
ˆ ˆ
with respect to p(θX |θY ) can be performed using p(θX |θY )
with uniformly vanishing error.

R EFERENCES

Now, we are equipped to deﬁne Sn (y m , pe ) as the set with
smallest Lebesgue volume such that
˜
θX ∈Sn (y m ,pe )

˜ ˆ
˜
p(θX |θY )dθX ≥ 1 − pe .

[1] D. Slepian and J. K. Wolf, “Noiseless coding of correlated information
sources,” IEEE Trans. Info. Theory, vol. 19, no. 4, pp. 471–480, 1973.
[2] S. Pradhan and K. Ramchandran, “Distributed source coding using
syndromes (DISCUS): design and construction,” IEEE Trans. Info.
Theory, vol. 49, no. 3, pp. 626 – 643, Mar 2003.
[3] M. Sartipi and F. Fekri, “Distributed source coding using short to
moderate length rate-compatible LDPC codes: the entire Slepian-Wolf
rate region,” IEEE Trans. Commun., vol. 56, no. 3, pp. 400–411, 2008.
[4] B. Girod, A. Aaron, S. Rane, and D. Rebollo-Monedero, “Distributed
video coding,” Proceedings of the IEEE, vol. 93, no. 1, pp. 71–83, 2005.
[5] M. Weinberger, J. Rissanen, and M. Feder, “A universal ﬁnite memory
source,” IEEE Trans. Info. Theory, vol. 41, no. 3, pp. 643 –652, 1995.
[6] J. Rissanen, “Universal coding, information, prediction, and estimation,”
IEEE Trans. Info. Theory, vol. 30, no. 4, pp. 629 – 636, Jul 1984.
[7] A. Beirami and F. Fekri, “Results on the redundancy of universal
compression for ﬁnite-length sequences,” in 2011 IEEE International
Symp. on Info. Theory (ISIT ’2011), July 2011, pp. 1604–1608.
[8] N. Merhav and M. Feder, “A strong version of the redundancy-capacity
theorem of universal coding,” IEEE Trans. Info. Theory, vol. 41, no. 3,
pp. 714 –722, May 1995.
[9] A. Beirami and F. Fekri, “Memory-assisted universal source coding,” in
2012 Data Compression Conference (DCC ’2012), April 2012, p. 392.
[10] G. Korodi, J. Rissanen, and I. Tabus, “Lossless data compression using
optimal tree machines,” in 2005 Data Compression Conference (DCC
’2005), March 2005, pp. 348 – 357.
[11] M. Sardari, A. Beirami, and F. Fekri, “Memory-assisted universal
compression of network ﬂows,” in IEEE INFOCOM 2012, March 2012,
pp. 91–99.
[12] A. Beirami, M. Sardari, and F. Fekri, “Results on the fundamental gain of
memory-assisted universal source coding,” in 2012 IEEE International
Symposium on Information Theory (ISIT ’2012), July 2012.
[13] M. Sardari, A. Beirami, and F. Fekri, “On the network-wide gain of
memory-assisted source coding,” in 2011 IEEE Information Theory
Workshop (ITW’ 2011), October 2011, pp. 476–480.
[14] C. E. Shannon, “A Mathematical Theory of Communication,” The Bell
System Technical Journal, vol. 27, pp. 379–423, 623–656, Jul, Oct 1948.
[15] B. Clarke and A. Barron, “Information-theoretic asymptotics of Bayes
methods,” IEEE Trans. Info. Theory, vol. 36, no. 3, pp. 453 –471, 1990.
[16] K. Atteson, “The asymptotic redundancy of Bayes rules for Markov
chains,” IEEE Trans. Info. Theory, vol. 45, no. 6, pp. 2104 –2109, 1999.
[17] M. Drmota and W. Szpankowski, “Precise minimax redundancy and
regret,” IEEE Trans. Info. Theory, vol. 50, no. 11, pp. 2686–2707, 2004.
[18] W. Szpankowski, “Asymptotic average redundancy of Huffman (and
other) block codes ,” IEEE Trans. Info. Theory, vol. 46, no. 7, pp. 2434–
2443, 2000.
[19] R. E. Krichevsky and V. K. Troﬁmov, “The performance of universal
encoding,” IEEE Trans. Info. Theory, vol. 27, no. 2, pp. 199–207, 1981.
[20] N. Alon and A. Orlitsky, “Source coding and graph entropies,” IEEE
Trans. Info. Theory, vol. 42, no. 5, pp. 1329 –1339, September 1996.

(7)

The following lemma shows as to how Sn (y m , pe ) is determined.
ˆ
Lemma 3 Let θY denote the ML-estimate for the unknown
parameter vector given sequence y m is observed. Then,
Sn (y m , ) is given by
ˆ
ˆ
ˆ
Sn (y m , pe ) = φ : r(φ − θY ) Im (θY )(φ − θY ) ≤ δd (pe ) ,
where r =

nm
n+m ,

δd (pe ) satisﬁes Γ

d
2 , δd (pe )

= pe Γ

d
2

.3

The next lemma determines the probability measure of the set
Sn (y m , pe ) under Jeffreys’ prior.
Lemma 4 Assume that the parameter vector θ follows Jeffreys’ prior. Then, the probability measure PS (pe ) of the set
Sn (y m , pe ) is given by
PS (pe ) =
where r =

Cd

ωJ (θ)dθ =

1

|I(β)| 2 dβ

θ∈Sn (y m ,pe )
nm
n+m

and Cd =

Γ( 1 )
2

2δd (pe )
r log e

d
2

,

d

Γ( d +1)
2

.

Next, consider the following coding scheme. Let the space
be partitioned into ellipsoids of the form Sn (y m , pe ). Then,
each sequence is encoded within its respective ellipsoid without regard to the rest of the parameter space. The decoder
ˆ
chooses the decoding ellipsoid using the ML estimate θY and
the permissible decoding error probability pe . The probability
measure covered by each ellipsoid is PS (pe ) is independent
ˆ
of θY , and provides with − log PS (pe ) reduction in the redundancy. Further, simpliﬁcation of PS (pe ) and the fact that
1
δd (pe ) ≈ d log e + log pe will lead to the desired result.
2
3 Γ(s, x)

x s−1 −t
e dt
0 t

denotes the incomplete Gamma function.

5


Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May 16 12:09:11 2012
ModDate:        Tue Jun 19 12:54:35 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      338104 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565771

Mutual Information and Relative Entropy over the
Binomial and Negative Binomial Channels
Camilo G. Taborda and Fernando P´ rez-Cruz
e
Department of Signal Theory and Communications
University Carlos III Madrid
Legan´ s, Madrid, Spain 28911
e
Email:{cgil,fernando}@tsc.uc3m.es

Guo et al. [3] have extended these results for the Poisson
channel. In this analogy, conditioned on X = x, Y ∼ P(γx),
where X is a positive random variable and γ represents an
input scaling factor. In this case the derivative of the inputoutput mutual information with respect to γ is given by

Abstract—We study the relation of the mutual information
and relative entropy over the Binomial and Negative Binomial
channels with estimation theoretical quantities, in which we
extend already known results for Gaussian and Poisson channels.
We establish general expressions for these information theory
concepts with a direct connection with estimation theory through
the conditional mean estimation and a particular loss function.

X
dI(X; Y )
= EPXY X log
.
dγ
EPX|Y [X|Y ]

I. I NTRODUCTION

This result was extended to the relative entropy between
two marginals PY and QY in [4], in which the authors
showed that it can be rewritten following the Gaussian channel
results by exchanging the square-loss function by (a, a) =
ˆ
a log(a/ˆ)−(a−ˆ). The derivative of relative entropy between
a
a
the marginals PY and QY , which are respectively, random
transformations of PX and QX through a Poisson channel
can be expressed by

I

N recent years it has been found fundamental relationships
between estimation and information theories. Explicitly, in
the Gaussian channel, the input-output mutual information is
closely related with the conditional mean estimation through
the mean square error (mse) [1]. The derivative of the mutual
information, as a function of the signal to noise ratio (snr), is
proportional to the minimum mse (mmse) regardless the input
distribution, i.e.,
dI(X; Y )
1
= EPXY
dsnr
2

dD(PY ||QY )
= mleQ (γ) − mleP (γ)
dγ
= EPY (EPX|Y [X|Y ], EQX|Y [X|Y ]) , (5)

1
mmse(snr),
2
(1)
√
where conditioned on X = x, Y ∼ N ( snrx, 1) and PX is
any distribution such that EPX [X 2 ] < ∞.
This result can be extended to the relative entropy between
two distributions PY and QY , that are, respectively, the
transformation of two generic input distributions PX and QX
through a Gaussian channel [2]. The derivative of the relative
entropy between the marginals PY and QY with respect to the
snr is
1
dD(PY ||QY )
= (mseQ (snr) − mmse(snr))
dsnr
2
1
= EPY (EPX|Y [X|Y ] − EQX|Y [X|Y ])2 , (2)
2
where mseQ (snr) represents the mismatch mean square error
incurred when we estimate X from Y assuming a prior
distribution QX different to the actual PX , i.e.,
X − EPX|Y [X|Y ]

2

=

mseQ (snr) = EPXY (X − EQX|Y [X|Y ])2 .

(4)

where
mleQ (γ) = EPXY

(X, EQX|Y [X|Y ]) .

(6)

In this paper, we show that these results can be extended to
the Binomial and Negative Binomial channels. Although these
distributions are similar the results are dissimilar enough to be
presented separately. The derivative is taken with respect to a
different parameter and the loss functions are also different.
For the Binomial channel, in which the outcome Y is
governed by the sum of n independent Bernoulli trials and
a probability of success p = αx/n where X is a positive
bounded random variable, we ﬁnd expressions for the derivative of mutual information and relative entropy with respect to
α in terms of the expectation of the following loss function:
ˆ
n (a, a)

(3)

Notice that mmse(snr) is given by mseP (snr).

= a log

ˆ
a(n − a) n(a − a)
ˆ
−
,
a(n − a)
ˆ
n−a
ˆ

a, a ∈ (0, n). (7)
ˆ

Besides the actual proofs, the only remarkable difference
between the results in this paper with respect to the Gaussian
and Poisson distributions is that the expectation of the loss
function is taken with respect to a Binomial distribution with

This work was partially funded by Spanish government (Ministerio de
Educaci´ n y Ciencia, TEC2009-14504-C02-01 and Consolider-Ingenio 2010
o
CSD2008-00010)

1

n − 1 trials instead of n.
In the second part of the paper, we show that these relationships can be extended to a Negative Binomial channel. In
this case, conditioned on X = x, Y ∼ N B(r, p(αx)), where
p(αx) stands for the probability of success of each Bernoulli
trial for a ﬁxed number of failures r. On the one hand, setting
p(αx) = αx, as in the Binomial channel yields a closed-form
expression without a connection with a well-behaved lossfunction. On the other hand, setting p(αx) = αx/(αx + r),
such that the mean of the distribution is αx (as in previous
results), we can derive similar results to those for the Gaussian,
Poisson and Binomial channels, using the loss function:
ˆ
r (a, a)

= a log

ˆ
a(r + a) r(a − a)
ˆ
−
,
a(r + a)
ˆ
(r + a)
ˆ

Lemma 2. Consider a positive random variable A over the
A
interval (0, n) such that E A log n−A < ∞. Then, for any
a ∈ (0, n)
ˆ
E[ n (A, a)] = E[ n (A, E[A])] +
ˆ

ˆ
n (E[A], a).

(9)

Proof:
E[ n (A, a)]
ˆ

=

a, a ∈ (0, ∞). (8)
ˆ

=

The rest of the paper is divided as follows. We, respectively,
present the results for the Binomial and Negative Binomial
Channels in Sections II and III. Both sections are divided in
three subsections. In Subsections II-A and III-A, we show
that the proposed loss functions have similar properties to
the square loss and the loss used for the Poisson channel
relations. The results obtained for the mutual information are
shown in Subsections II-B and III-B, and the extensions for the
relative entropy are presented in Subsections II-C and III-C.
We conclude the paper in Section IV with a discussion.

A(n − a) n(ˆ − a)
ˆ
a
+
(n − A)ˆ
a
n−a
ˆ
A(n − E[A]) n(ˆ − a)
a
E A log
+
(n − A)E[A]
n−a
ˆ
n(E[A] − A)
E[A](n − a)
ˆ
+
+ E[A] log
n − E[A]
(n − E[A])ˆ
a
E[ n (A, E[A])] + n (E[A], a).
ˆ
(10)

= E A log

The main consequence of this lemma is that the mean
minimizes the expected loss, i.e.,
E[A] = argmin E [ n (A, a)] ,
ˆ

(11)

a
ˆ

which is a property possessed by all the Bregman divergences
[6]. Notice in this case that
min E [ n (A, a)] = E [ n (A, E[A])]
ˆ
a
ˆ

= E A log

II. B INOMIAL C HANNEL
A. A Natural Loss Function

E[A]
A
− E[A] log
.
n−A
n − E[A]

(12)

B. Mutual Information

In this section we propose a natural loss function that is
closely related with the functional that describes the relative
entropy and mutual information over the Binomial channel.
In addition, we formulate different properties fulﬁlled by this
loss function, showing by this way similarities between the
behavior stated for the Gaussian and Poisson channel and the
scheme proposed for the Binomial channel.

The Binomial distribution is a discrete probability distribution that measures the number of successes in n independent
trials of a Bernoulli random variable with a probability of
success p. The framework considered in this paper assumes
that p = αx/n where x is a realization of a positive bounded
random variable X, distributed over the set X = [a, b] where
a, b ∈ R+ , and n is a positive integer value. Accordingly the
conditional probability mass of the channel is given by

Lemma 1. The function n (a, a) proposed in (7) has the
ˆ
following properties:
1) Non-negativity: n (a, a) ≥ 0 with equality if and only if
ˆ
a = a.
ˆ
2) Convexity: n (a, a) is convex on its ﬁrst argument.
ˆ
3) Unboundedness of loss for underestimation: For any a ∈
(0, n), lima→0+ n (a, a) = ∞.
ˆ
ˆ
4) Unboundedness of loss for overestimation: For any a ∈
(0, n), lima→n− n (a, a) = ∞.
ˆ
ˆ

PY |X (y|x) =

n
y

αx
n

y

1−

αx
n

n−y

.

(13)

Notice that the constraint over the parameter p makes that 0 <
αx < n for all values of x ∈ X . Therefore, the set of feasible
values of α is given by the set A = {α ∈ R+ |0 < αb < n}.
Theorem 1. Let X be a random variable distributed over the
X
set X = [a, b] where a, b ∈ R+ with EPX X log 1−X/b <
∞. Consider a Binomial channel with parameters (n, αx/n).
Then, for a ﬁxed n ∈ N and for all α ∈ A, the derivative of
the input-output mutual information with respect to α is given
by


X n − αEP n−1 [X|Y ]
dI(X; Y )


X|Y
=EP n−1 X log

XY
dα
(n − αX) EP n−1 [X|Y ]

Proof: We can build the loss function in (7) as follows:
n (a)
ˆ
(a, a) = φn (a) − φn (ˆ) − dφda |a=ˆ (a − a), where
ˆ
a
n
a
a
φn (a) = a log n−a . This loss is the difference between the
points φn (a) and its ﬁrst order Taylor approximation at φn (ˆ).
a
Due to the strictly convexity of the function φn (a) we can state
the non-negativity of the function n (a, a), see Section 3 in
ˆ
[5]. Properties 2, 3 and 4 are straightforward.
The unboundedness behavior of the loss function when a →
ˆ
0+ and a → n− for some a ∈ (0, n) is a desired property when
ˆ
measuring goodness of reconstruction of a bounded quantity,
not possessed by the more common loss functions.

X|Y

1
= EP n−1
α XY

2

n (αX, EP n−1 [αX|Y
X|Y

]) ,

(14)

Theorem 2. Let X be a random variable over the set X =
[a, b] where a, b ∈ R+ distributed according to PX or QX with
X
X
EPX X log 1−X/b < ∞ and EQX X log 1−X/b < ∞,
and consider a Binomial channel with parameters (n, αx/n).
Then, for a ﬁxed n ∈ N and for all α ∈ A, the derivative of
n
the relative entropy between the marginals PY and Qn with
Y
respect to α is given by

where EP n−1 [·] represents the expectation with respect to the
XY
joint distribution PXY when PY |X is the sum of n−1 Bernoulli
trials with probability of success p = αx/n, i.e.
EP n−1 [g(X, Y )]
XY

n−1

=

g(x, y)
y=0

n−1
y

αx
n

y

1−

αx
n

n−1−y

dPX (x).

d
n
D(PY ||Qn ) =
Y
dα

(15)
Notice that the −1 only affects the number of trials of the
underlying Bernoulli random variable, but the probability of
success still depends on n and not n − 1.
The theorem is proven in [7]. The deﬁnition in (15) implies
that the expectations carried out at the right hand side of (14)
consider that PY |X is a Binomial distribution with n−1 trials,
while on the left hand side of (14), the mutual information
employs the conditional distribution PY |X with n independent
Bernoulli trials.
The non-negativity property of the loss function n (a, a)
ˆ
let us state that the mutual information increases as long
as the scaling factor increases. Theorem 1 is the Binomial
counterpart of (1) and (4) to the Gaussian and Poisson channels
respectively.

EP n−1 EP n−1 [X|Y ] log
Y

−n

n − αEP n−1 [X|Y ] EQn−1 [X|Y ]
X|Y

EP n−1 [X|Y ] − EQn−1 [X|Y ]
X|Y

X|Y

n − αEQn−1 [X|Y ]

= mleQ (α) − mleP (α)
1
= EP n−1 n (EP n−1 [αX|Y ], EQn−1 [αX|Y ]) ,
X|Y
X|Y
α Y

(18)

where
1
E n−1 n (αX, EQn−1 [αX|Y ]) .
(19)
X|Y
α PXY
The theorem is proven in [7]. One immediate implication
of this theorem is that, due to the non-negativity property of
the loss function, the relative entropy between the marginals
increases as a function of the scaling factor α.
mleQ (α) =

X|Y

(16)

III. N EGATIVE B INOMIAL C HANNEL
A. A Natural Loss Function
In this section we propose a natural loss function that is
closely related with the functional that describes the relative
entropy and mutual information over the Negative Binomial
channel. In addition we formulate different properties fulﬁlled
by the loss function proposed, showing by this way similarities
between the behavior stated for the Gaussian, Poisson and
Binomial channels and the scheme proposed for the Negative
Binomial channel.

Proof: By theorem 1 we have that
n − αEP n−1 [X|Y ]
dI(X; Y )
X|Y
= EP n−1 EP n−1 [X|Y ] log
Y
X|Y
dα
αEP n−1 [X|Y ]
X|Y

I

> 0.

X|Y

X|Y

n − αEP n−1 [X|Y ]
dI(X; Y )
X|Y
< EP n−1 EP n−1 [X|Y ] log
Y
X|Y
dα
αEP n−1 [X|Y ]

n − αX
αX

X|Y

X|Y

Corollary 1. For 0 < αx/n < 1/2, then

− EPX X log

X|Y

EP n−1 [X|Y ] n − αEQn−1 [X|Y ]

(17)

II

Since 0 < αx/n < 1/2 for all x ∈ X the result follows by
noting that (I) and (II) in (17) are strictly positive terms.

Lemma 3. The function r (a, a) in (8) has the following
ˆ
properties:
1) Non-negativity: r (a, a) ≥ 0 with equality if and only if
ˆ
a = a.
ˆ
2) Convexity: r (a, a) ≥ 0 is convex on its ﬁrst argument.
ˆ
3) Unboundedness of loss for underestimation: for any a >
0, lima→0+ r (a, a) = ∞.
ˆ
ˆ

C. Relative Entropy
Now, consider two probability distributions, PX and QX
deﬁned over a positive and bounded set X . Furthermore,
consider a random transformation that takes an outcome of X
and produces an output Y according to a Binomial distribution
of n independent Bernoulli trials and probability of success
αx/n, i.e. conditioned on X = x, Y ∼ B(n, αx/n). The
n
output Y can follow two different distributions PY and Qn ,
Y
which have been respectively generated from PX and QX .
For the sake of clarity, superscript n on the notation of the
n
marginals PY and Qn , makes reference to the number of
Y
independent Bernoulli trials used to construct the conditional
PY |X that generates each output distribution.

Proof: Observe that r (a, a) = φr (a) − φr (ˆ) −
ˆ
a
a
− a), where φr (a) = a log r+a . Due to the
ˆ
strictly convexity of the function φr (a) we can state the
non-negativity of the function r (a, a), see Section 3 in [5].
ˆ
Properties 2 and 3 are straightforward.
The unboundedness behavior of the loss function when
a → 0+ for some a > 0 is a desired property when measuring
ˆ
goodness of reconstruction of non-negative quantities, not
possessed by the more common loss functions.
dφr (a)
a
da |a=ˆ (a

3

Lemma 4. For any non-negative random variable A such that
A
ˆ
E A log r+A < ∞ and any a ∈ (0, ∞),
E[ r (A, a)] = E[ r (A, E[A])] +
ˆ

ˆ
r (E[A], a)

output mutual information with respect to α is given by


X r + αEP r+1 [X|Y ]
dI(X; Y )


X|Y
= EP r+1 X log

XY
dα
EP r+1 [X|Y ] (r + αX)

(20)

X|Y

Proof: Suppose that E[A] > 0, then
A(r + a) r(ˆ − a)
ˆ
a
E[ r (A, a)] = E A log
ˆ
+
(r + A)ˆ
a
r+a
ˆ
A(r + E[A]) r(ˆ − a)
a
= E A log
+
(r + A)E[A]
r+a
ˆ
E[A](r + a)
ˆ
r(E[A] − A)
+ E[A] log
+
r + E[A]
(r + E[A])ˆ
a
= E[ r (A, E[A])] + r (E[A], a)
ˆ

1
E r+1 r (αX, EP r+1 [αX|Y ] ,
(24)
X|Y
α PXY
where EP r+1 [·] represents the expectation with respect to the
XY
joint distribution PXY where PY |X is a Negative Binomial
distribution with parameters (r + 1, αx/(αx + r)), i.e.,
=

EP r+1 [g(X, Y )]
XY

∞

(21) =
y=0

Observe that a = E[A] minimizes the loss function, hence
ˆ
min E[ r (A, a)] = min E[ r (A, E[A])] +
ˆ
a
ˆ

a
ˆ

= E A log

r+y
y

αx
αx + r

y

r
αx + r

r+1

dPX (x).
(25)

ˆ
r (E[A], a)

A
E[A]
.
− E[A] log
r+A
r + E[A]

g(X, Y )

Notice that the +1 only affects the number of failures of
the underlying Bernoulli random variable, but the probability
of success still depends on r and not r + 1.
The theorem is proven in [7]. For the sake of clarity, notice
that the conditional distribution PY |X used at the left hand
side of (24) uses as parameters (r, αx/(αx + r)) while at the
right hand side the conditional distribution used correspond to
a Negative Binomial with parameters (r + 1, αx/(αx + r)).
An immediate consequence of the result obtained in (24) is
that the mutual information increases as long as the the input
scaling factor increases.

(22)

Equation (22) indicates that the functional E[ r (A, a)] is quite
ˆ
similar to its counterparts developed for the Gaussian, Poisson
and Binomial channels, where, in the former we use the square
loss function and in the second we use a loss function based on
the convex conjugate of the log-moment generating function
of the Poisson distribution. One consequence of the previous
analysis is the fact that the conditional expectation E[A|A ]
is the unique minimizer of the loss function r (A, a) when
ˆ
we construct an estimator of A based on the observation A .
This property, as has been shown in [6] is common to all the
Bregman loss functions.

Corollary 2.
r + αEP r+1 [X|Y ]
dI(X; Y )
X|Y
< EP r+1 EP r+1 [X|Y ] log
.
Y
X|Y
dα
αEP r+1 [X|Y ]
X|Y

(26)
Proof: Due to the previous theorem the derivative of the
mutual information with respect to α can be written as

B. Mutual Information
The Negative Binomial distribution is a discrete probability
distribution of the number of successes in a sequence of
Bernoulli trials before a number of failures r occur. In this
framework we set the parameter p of each Bernoulli trial so
that the mean of the conditional distribution is equal to αx,
as has been common in the previous results obtained for the
Poisson and Binomial channels. Accordingly the conditional
probability mass of the channel is given by
PY |X (y|x) =

y+r−1
y

αx
αx + r

y

1−

αx
αx + r

r + αEP r+1 [X|Y ]
dI(X; Y )
X|Y
= EP r+1 EP r+1 [X|Y ] log
Y
X|Y
dα
αEP r+1 [X|Y ]
X|Y

I

− EPX

r + αX
X log
αX

> 0.

(27)

II

r

Clearly, since the random variable X is positive, both terms
at the right hand side of (27) (I and II) are positive, therefore
we can state the conclusion given by the corollary.
A similar reasoning in the Gaussian channel shows that the
derivative of the input-output mutual information with respect
to an input scaling factor α is upper bounded by a half the
second moment of the input X. For more details, see [8].

(23)
where r ∈ N is a ﬁxed integer value, α > 0 is an input scaling
parameter and x is a realization of a positive random variable
X. Notice that in this case the set of feasible values of α is
the set A = R+ .

C. Relative Entropy

Theorem 3. Let X be a positive random variable with
X
EPX X log r+X < ∞ and consider a Negative Binomial
channel with parameters (r, αx/(αx + r)). Then, for a ﬁxed
value r ∈ N and for all α ∈ A, the derivative of the input-

Let PX and QX be two distributions over a positive
bounded random variable X. Consider a random transformation of the Negative Binomial type with parameters

4

r
(r, αx/(αx+r)). We denote as PY and Qr each marginal disY
tribution followed by the output of the random transformation
under the distributions PX and QX , respectively. Superscript
r makes reference to the number of failures in the conditional
distribution of the channel PY |X .

Bregman divergences [9] and, if so, it might also provide an
answer to the ﬁrst question, as well. Although the loss-function
used for the Binomial and Negative Binomial channels is
different from the ones proven in [9] and it would lead to
a different connection between exponential families and the
Bregman divergences.

Theorem 4. Let X be a positive random variable,
which can be distributed according to PX or QX with
X
X
EPX X log r+X < ∞ and EQX X log r+X < ∞.
Consider a Negative Binomial channel with parameters
(r, αx/(αx + r)). Then, for a ﬁxed value r ∈ N and for all
α ∈ A, the derivative of the relative entropy between the
r
marginals PY and Qr with respect to α is given by
Y

R EFERENCES
[1] D. Guo, S. Shamai, and S. Verd´ , “Mutual Information and Minimum
u
Mean-Square Error in Gaussian Channels,” IEEE Trans. Inf. Theory,
vol. 51, no. 4, pp. 1261–1282, 4 2005.
[2] S. Verd´ , “Mismatched Estimation and Relative Entropy,” IEEE Trans.
u
Inf. Theory, vol. 56, no. 8, pp. 3712–3720, 8 2010.
[3] D. Guo, S. Shamai, and S. Verd´ , “Mutual Information and Conditional
u
Mean Estimation in Poisson Channels,” IEEE Trans. Inf. Theory, vol. 54,
no. 5, pp. 1837–1849, 5 2008.
[4] R. Atar and T. Weissman, “Mutual Information, Relative Entropy, and
Estimation in the Poisson Channel,” in Information Theory Proceedings
(ISIT), 2011 IEEE International Symposium on, 31 2011-aug. 5 2011, pp.
708 –712.
[5] S. Boyd and L. Vandenberghe, Convex Optimization. New York, NY,
USA: Cambridge University Press, 2004.
[6] A. Banerjee, X. Guo, and H. Wang, “On the Optimality of Conditional
Expectation as a Bregman Predictor,” IEEE Trans. Inf. Theory, vol. 51,
no. 7, pp. 2664 –2669, july 2005.
[7] C. G. Taborda and F. P´ rez-Cruz, “Mutual Information and Relative
e
Entropy over the Binomial, Negative Binomial and Poisson Channels,”
submitted to IEEE Trans. Information Theory.
[8] D. Guo, Y. Wu, S. Shamai, and S. Verd´ and, “Estimation in Gaussian
u
Noise: Properties of the Minimum Mean-Square Error,” IEEE Trans. Inf.
Theory,, vol. 57, no. 4, pp. 2371 –2385, april 2011.
[9] A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh, “Clustering with
Bregman Divergences,” J. Mach. Learn. Res., vol. 6, pp. 1705–1749,
December 2005. [Online]. Available: http://dl.acm.org/citation.cfm?id=
1046920.1194902

r
dD(PY ||Qr )
Y
=
dα

EP r+1 EP r+1 [X|Y ] log
Y

X|Y

EP r+1 [X|Y ] r + αEQr+1 [X|Y ]
X|Y

X|Y

r + αEP r+1 [X|Y ] EQr+1 [X|Y ]
X|Y

−r

X|Y

EP r+1 [X|Y ] − EQr+1 [X|Y ]
X|Y

X|Y

r + αEQr+1 [X|Y ]
X|Y

= mleQ (α) − mleP (α)
1
= Er+1 r (EP r+1 [αX|Y ], EQr+1 [αX|Y ]) , (28)
X|Y
X|Y
α PY
where
1
E r+1 r (αX, EQr+1 [αX|Y ]) .
(29)
X|Y
α PXY
One immediate implication of this theorem is that, due to
the non-negativity property of the loss function, the relative
entropy between the marginals increases as long as the scaling
input increases.
mleQ (α) =

IV. D ISCUSSION
In this paper, we have proven the relation between the
relative entropy and estimation theoretic quantities holds for
the Binomial and Negative Binomial Channels. For the Gaussian distribution the connection is through the well-known
square loss, while the Poisson and Negative Binomial two
loss functions for nonnegative reals are needed and a bounded
nonnegative loss function is needed for the Binomial channel.
All these relationships present some common properties. First,
the derivative is taken with respect to the mean parameter
of the conditional distribution between the input and output.
This result becomes obvious when we deal with the Negative
Binomial channel. Second, the conditional mean minimizes
the proposed loss functions, which is a unique property of the
Bregman Divergences [6]. Third, the derivative is proportional
to the expectation of the loss function, between the input
and its conditional mean estimate. Two obvious questions
arise: Can we extend these results to other distributions and
loss-functions? Is there a unique representation that links
the conditional distribution between X and Y and the loss
function? We believe that the second question can be answered
using the relation between the exponential families and the

5


Title:          est_help_conf_v3.dvi
Creator:        dvips(k) 5.99 Copyright 2010 Radical Eye Software
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 06:39:21 2012
ModDate:        Tue Jun 19 12:54:35 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      416585 bytes
Optimized:      no
PDF version:    1.7
ISIT'2012 1569566641

Estimation with a Helper Who Knows the
Interference
Yeow-Khiang Chia

Rajiv Soundararajan

Tsachy Weissman

Modulation and Coding Dept.
Institute for Infocomm Research
Email: yeowkhiang@gmail.com

ECE Dept.
University of Texas at Austin
Email: rajivs@utexas.edu

EE Dept.
Stanford University
Email: tsachy@stanford.edu

Abstract—We consider the problem of estimating a
signal corrupted by independent interference in which
the estimator (decoder) is aided by a helper (encoder)
with a limited power budget that has knowledge of the
interfering signal noncausally. In the Gaussian case, this
problem is equivalent to the problem of Assisted Interference Suppression considered by Grover and Sahai and we
obtain an improved lower bound for this problem using
Verdu’s relationship between mismatched estimation and
´
relative entropy. We also extend our analysis to consider
the case when the signal to be estimated, in addition to the
interference, is known to the helper. We establish a lower
bound that improves on those recently derived by Huang
and Narayanan.

S1 , S2
n
S2

Enc.

Xn

n
S1

n
PY |X,S1 ,S2 Y

Dec.

ˆn
S1

Fig. 1: Estimation with a helper that knows the interference.
construction site and transmit it to a noise cancellation
speaker situated in Bob’s ofﬁce. Since electromagnetic
waves travel faster than sound, the noise cancellation
speaker knows the interfering signal noncausally. Due to
a power constraint on the speaker, it cannot cancel the
interference fully. What then, is the minimum distortion
that can be achieved by Bob in trying to reconstruct
Alice’s speech?
Our setup is closely related to several strands of work
involving communication over channels with state. [1]
and [2] considered the problem of State Ampliﬁcation,
where a message is to be sent to the decoder and the den
coder also wishes to reconstruct S2 subjected to a list or
distortion constraint. This setting is similar to our setting,
with the main difference being that the decoder wishes
to reconstruct S2 instead of S1 . When our setting is specialized to the Gaussian case with the distortion measure
being the mean square error between the reconstruction
and the signal, our setting becomes equivalent to the
problem of Assisted Interference Suppression considered
in [3]. As detailed in [3], this problem is closely related
to Witsenhausen’s counterexample in Optimum Control
Theory [4]. In this paper, we focus on the following:
1) Gaussian Estimation with a helper: In the Gaussian
case, where S1 and S2 are independent Gaussian random
variables with ﬁnite variance and the distortion measure
is the mean square error, we note that our setting
is equivalent to the problem of Assisted Interference

I. I NTRODUCTION
Consider a joint source channel coding problem as
depicted in Figure 1. We have two memoryless sources
S1 (the desired signal) and S2 (the interfering signal). The decoder (or estimator)’s aim is to reconn
struct the source sequence S1 from Y n , with the
goal of minimizing the average per symbol distorˆ
tion E( n d(S1i (Y n ), S1i ))/n. The encoder (helper),
i=1
which knows the interfering signal S2 , aids the decoder
in reconstructing the signal S1 by modifying the interference through X, subject to a cost constraint ρ(X), so
as to minimize the distortion that an optimum decoder
incurs.
Potential applications for such a setting may arise
in sensor networks or cognitive radio systems. As a
motivating example, suppose Alice is talking to Bob
in his ofﬁce. As a result of ongoing construction work
near Bob’s ofﬁce, there is high interference which makes
it hard for Bob to listen to Alice. Fortunately, Bob
recently purchased a noise cancellation device which
has a microphone placed near the construction site. The
microphone measures the interfering signal from the
This work was partially supported by NSF under grant CCF0916713, AFOSR under grant FA95500910063, and the Center for
Science of Information (CSoI), an NSF Science and Technology
Center.

1

Suppression problem. For this setting, we give a lower
bound on the minimum achievable distortion that can
be strictly larger than the lower bound given in [3],
and its improved version given in [5]. Interestingly, the
proof of our lower bound relies on an application of
a recently established result between relative entropy
and mismatched estimation in Gaussian noise [6]. Recently, there has been interest in applying relationships
between Estimation Theory and Information Theory to
problems in Information Theory (see for e.g. [7]). Our
lower bound, which seems difﬁcult to obtain by traditional techniques such as the Entropy Power Inequality
[8, Chapter 2], provides another application of these
information-estimation relations.
2) Gaussian Estimation with a helper that knows both
the interference and the source: We also extend our
analysis to consider the related setting when the encoder
has access to S1 noncausally in addition to S2 . This
setting turns out to be a special case of a problem
considered in [9]. We give a lower bound for this setting
that contains the previous bounds in [9] and can be
strictly better than the previous bounds. Furthermore, we
also prove constant gap results between the achievable
distortion and our lower bound.
The rest of the paper is as follow. We ﬁrst provide
the formal deﬁnitions in the next section. Section III
deals with the case of Gaussian estimation with a helper
that knows only the interfering signal. Section IV deals
with the setting of Gaussian estimation with a helper
that knows both the interference and the source. Several
proofs and other results omitted from this paper are given
in the extended version [10] posted online.

A distortion D is said to be achievable if there exists a
sequence of (n, C + n ) codes, where n → 0 as n → ∞,
such that
n ˆn
lim sup E d(S1 , S1 ) ≤ D.
n→∞

The minimum achievable distortion, D(C)min , is then
deﬁned as the inﬁnum of the set of achievable distortions
under the cost constraint C.
B. Estimation with source and interference known at the
helper
This setting is shown in Figure 2 for the Gaussian
case. For this setting, As the deﬁnitions are similar to the
previous setting, we only mention the difference. That is,
n
n
the encoder now maps both S1 and S2 to X n :
n
n
f : S1 × S2 → X n .
n
S2

n
S1

Zn

Xn

Yn

Enc.

Dec.

ˆn
S1

Fig. 2: Gaussian estimation with a helper that knows
both the interference and the source.
For both settings, since we only consider the Gaussian
case in this paper, we assume S1 and S2 are independent Gaussian random variables, S1 ∼ N (0, P1 ) and
S2 ∼ N (0, P2 ), both known noncausally at the encoder.
The distortion measure is the mean square error between
S1 and it’s reconstruction, d(s1 , s1 ) = (s1 − s1 )2 . The
ˆ
ˆ
channel is speciﬁed by Y = X + S1 + S2 for the setting
deﬁned in subsection II-A and Y = X + S1 + S2 + Z,
where Z ∼ N (0, N ) is independent of S1 and S2
for the setting deﬁned in subsection II-B. The cost
constraint in both cases is the expected power constraint:
n
E( i=1 Xi2 /n).

II. D EFINITIONS
In this section, we give formal deﬁnitions for our
problem settings. We will follow the notation of [8],
and assume throughout this paper that the channel in
consideration is memoryless. That is, p(y n |xn , sn , sn ) =
1
2
n
n
i=1 p(yi |xi , s1i , s2i ). We also assume that S1 and
n
S2 are independent and identically distributed (i.i.d.)
sequences.

III. I NTERFERENCE ONLY

KNOWN AT HELPER

As shown in [3], it sufﬁces to consider only P1 = 1
for this setting. We ﬁrst state a number of results without
proof.
Theorem 1: An achievable distortion for the problem
of Gaussian estimation with a helper is given by

A. Estimation with interference known at the helper
A (n, C) code for the setting shown in Figure 1 when
the interference is known noncausally consists of
n
• An encoder that maps the interference S2 to X n ,
n
n
f : S2 → X ;
• A decoder that maps the output Y n to the reconˆn
ˆn
struction sequence S1 , g : Y n → S1 ;
n
such that E i=1 ρ(Xi )/n ≤ C. The expected per
n ˆn
symbol distortion, D, is given by D = E d(S1 , S1 ) =
n
ˆ
E i=1 d(S1i , S1i )/n.

D(P )min ≤ inf 1 −
where
√
√
−2αβ P +
P =
E U 2 = P + 2γβ
2

2

E U2
,
E Y 2 E U 2 − (E U Y )2

4α2 β 2 P + 4(1 − α2 )P
,
2
P P2 + γ 2 P2 ,

√
E(U Y ) = αβ P P + P + αγ
+β

P P2 + γβ

P P2

To derive our bound, we ﬁrst consider a more general
source S1 ∼ N (0, 1/γ) and let S2 ∼ N (0, P2 ) as before.
The value of γ that we are concerned about is γ = 1,
which will appear later in the proof.
Deﬁne M SEQ (γ) as

P P2 + γP2 ,

2

E Y = P + 1 + P2 + 2α P P2 + 2β

P P2 .

and the minimization is over −1 ≤ α ≤ 1, −1 ≤ β ≤ 1
and γ ∈ R satisfying the constraint

n
M SEQ (γ) := E S1 −

(E(U Y ))2
(1 − β )P > E U −
.
EY2
2

2

1
γ
1
γ

2

+ P2

n
n
(X n + S1 + S2 )

.

1

γ
ˆ
Let α = 1 +P2 and note that S1 = αY is the Minimum
γ
Mean Square Error (MMSE) estimate of S1 that the
decoder would employ if it assumes that X n ≡ 0. We
ﬁrst give a lower bound for M SEQ (γ). Using the fact
that under the true distribution, E ||X n ||2 ≤ nP , we can
show that

The achievable distortion-cost region presented here
is similar to the scheme presented in [3], but we derive
it via different means in [10], where we used a hybrid
coding scheme proposed in [11]. We now state the
following lower bound given in [3].
Theorem 2: [3] A lower bound for the problem of
n
n
n
Gaussian estimation with helper is given by
E ||S1 − α(X n + S1 + S2 )||2 ≥ nαP2 − 2nα2 P2 P .
2

(1)
+
√
P2
n
√
− P  ,
D(P )min ≥ 
˜
Now, let S n = S2 + X n and let PS n denote the
˜
P2 2 + 2 P P2 + P + 1
n
distribution of S2 + X n under the optimum encoding
scheme. Let QS n denote the corresponding distribution
˜
where [.]+ denotes the positive part.
under the encoding scheme of X n ≡ 0. It can be shown
We now turn to our lower bound for this setting. For
that
clarity, we ﬁrst present a proof of a special case of
our lower bound before turning to the more general
˜
˜
M SEQ (γ) = E ||S n − EQ (S n |Y n )||2
expression.
:= M SEQ,S n (γ).
(2)
˜
Proposition 1: A lower bound for the problem of
˜
Gaussian estimation with helper is given by
Further, even when the decoder knows that S n is dis√
tributed according to PS n , we still have
˜
1 + γP2
2 P
(γ − 1)D(P )min ≥ ln
+√
n
n
1 + P2
M M SE(γ) := E ||S1 − EP (S1 |Y n )||2
P2 (1 + γP2 )
√
˜
˜
2 P
= E ||S n − EP (S n |Y n )||2
− γP,
−√
P2 (1 + P2 )
:= M M SEP,S n (γ).
(3)
˜
for any γ ≥ 1.
It should be noted that any γ ≥ 1 constitutes a lower
bound for D(P )min . Hence, Proposition 1 is computable
and in fact, gives a family of lower bounds.
Proof Sketch (See [10] for details): The proof
hinges on an application of a relationship between
mismatched estimation and relative entropy given in
[6, Equality 14]. The main idea behind the proof lies
in considering a decoder that performs the estimation
(and reconstruction) using a wrong (or mismatched)
n
distribution for PS1 |Y n . In particular, we will consider
n
a mismatched decoder that attempts to estimate S1
n
assuming that X ≡ 0. The estimation error incurred by
the mismatched decoder, M SEQ , is clearly larger than
that incurred by an optimum decoder that uses the correct
(true) distribution, D(P )min . We then rely on results in
[6] to lower bound the difference between D(P )min and
M SEQ , thereby giving us a lower bound on D(P )min .

Note that nD(P )min = M M SE(1). Next, using [6,
Equality 14], we have
(γ )

(γ )

0
0
D(PY n ||QY n ) =

1
2

γ0

M SEQ,S n (γ)
˜

0

−M M SEP,S n (γ)dγ
˜

(4)

Here, PY n represents the distribution of Y n induced by
PS n . Similarly, QY n represents the distribution of Y n
˜
induced by QS n . It can be shown that
˜
(γ)

(γ)

D(PY n ||QY n ) ≤

nγP
.
2

(5)

From (4), we have
γ1
γ0

M SEQ,S n (γ) − M M SEP,S n (γ)dγ
˜
˜
(γ )

(γ )

(γ )

(γ )

1
1
0
= 2D(PY n ||QY n ) − 2D(PY n0 ||QY n )

3

3

Comparison of bounds for P2 = 10

for γ1 ≥ γ0 . Using the fact that M M SEP,S n (γ)
˜
is a non-increasing function in γ and hence,
γ1
˜
˜
γ0 M M SEP,S n (γ)dγ ≤ (γ1 − γ0 )M M SEP,S n (γ0 ) =
(γ1 − γ0 )M M SE(γ0 ), the bounds given in (1)
and (5), setting γ0 = 1 and the fact that divergence is
non-negative then give us our lower bound.
Proposition 1 can be generalized by using a different
mismatched distribution for X n , leading to our generalized lower bound in Theorem 3. The mismatched
distribution for X n is chosen as an i.i.d. sequence
with X = cS2 + Z, where c is any real number and
Z ∼ N (0, rP ) with r ≥ 0 and independent of S2 . Proof
is given in [10].
Theorem 3: A lower bound for the problem of Gaussian estimation with helper is given by

1

0.8

Distortion

0.6

0.4

0.2

0

−0.2

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

P

Fig. 3: Distortion v.s. P .

(γ − 1)D(P )min
1 + γPI
1
1
≥ log(
)+
−
1 + PI
(1 + γPI ) (1 + PI )
P2
c2 γ
P2
+
−
P2
−
PI (1 + γPI ) PI (1 + PI ) 1 + γrP
1
1
−
+ 1 + log(
)
1 + γ1 rP
1 + γ1 rP
+ ax∗2 − bx∗ ,

and we minimize over −1 ≤ α ≤ 1 and −1 ≤ β ≤ 1.
The next two results are lower bounds for this setting,
one of which were established in [9]. For details, readers
may see [9] or the proof of Theorem 5 below.
Proposition 2: [9]. A lower bound for the distortioncost region is given by
D(P )min ≥

γ
1
1
where a = PI (1+PI ) − PI (1+γPI ) − 1+γrP , b =
√
cγ
1
1
|2( PI (1+PI ) − PI (1+γPI ) + 1+γrP )| P2 and
 √
P if a ≤ 0

√
x∗ =
b/2a if a > 0 and b/2a < P ,
 √
P otherwise

P1
1+

P1
P2

1+

P
N

.

Proposition 3: A lower bound for the distortion-cost
region is given by
D(P )min ≥

1+

P1
√
√
( P + P1 )2
N

.

We now give our lower bound, which contains the
bounds stated above.
Theorem 5: A lower bound for the problem of estimation with a helper who knows both the interference
and the signal noncausally is given by

for any γ ≥ 1, real number c and r ≥ 0.
Comparison of bounds: Figure 3 shows the distortion
versus P for P2 = 10. As we see from the plots,
Theorem 3 can be strictly better than Theorem 2 for a
wide range of P . Comparisons with the improved version
of Theorem 2, reported in [5], is given in the extended
version [10].
IV. W HEN S1

Theorem 1
Theorem 3
Proposition 1
Theorem 2

2

D(P )min ≥

α P1 P2
( P1 +α2 P2 )N

M SE(α)

for any α = 0, and

IS ALSO AVAILABLE AT THE ENCODER

M SE(α) := max P + (1 − α)2 P2 + 2(1 − α)ρXS2
((1 − α)αP2 + αρXS2 + ρXS1 )2
,
+N −
P1 + α2 P2
√
where we maximize over |ρXS1 | ≤ P P1 , |ρXS2 | ≤
√
P1
D(P )min ≤
, where
P P2 .
P (1−α2 −β 2 )
κ 1+
It can be shown that setting α = 1 and α → ∞ recover
N
 the bounds in Propositions 2 and 3, respectively.

2
P
α P1 + 1 P1
We note that while computation of the lower bound re

κ := 1 +
 quires solving an optimization problem, the optimization
2
P
β P2 + 1 P2 + P (1 − α2 − β 2 ) + N
problem is quadratic and can be efﬁciently solved [12].
We now turn our attention to the setting deﬁned
in II-B. We ﬁrst state a number of results without proof.
Theorem 4: (See also [9]) An acheivable distortioncost region is given by

4

4

Proof Sketch (See [10] for details): The idea behind
n
n
the proof lies in giving S1 + αS2 to the decoder, and
reﬁning the bound using linear estimation and convex
n
n
optimization. It can be shown that, by giving S1 + αS2
to the decoder, the following is a bound on the achievable
distortion:
α2 P1 P2
1
1
− log 2πeD(P )min
log 2πe
2
P1 + α2 P2
2
1
≤ h(Y |S1 + αS2 ) − log 2πeN.
2
Focusing on the term h(Y |S1 + αS2 ), we can show that

Comparsion of bounds
0.35

0.3

Distortion

0.25

0.2

0.15

0.1

0.05

Theorem 4
Proposition 2
Propostion 3
Theorem 5

h(Y |S1 + αS2 )
0
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
≤ log(2πe(E(X + (1 − α)S2 + Z − k(S1 + αS2 ))2 )),
Power of S2
2
Fig. 4: Distortion v.s. P2
where k is deﬁned as the coefﬁcient of the linear
(P +N )
minimum mean square estimator:
P
, 0 ≤ ≤ P +N , then the multiwith γ =
2P
(1 − α)αP2 + αρXS2 + ρXS1
plicative gap between the upper bound in Theorem 4,
k :=
Dachievable , and the lower bound in Proposition 2, Dlb ,
P1 + α2 P2
and ρXS1 := E(XS1 ), ρXS2 :=√
E(XS2 ). By Cauchy is at most 1/(1− ). That is, Dachievable /Dlb ≤ 1/(1− ).
Schwartz inequality, |ρXS1 | ≤ P P1 and |ρXS2 | ≤
ACKNOWLEDGMENT
√
P P2 . In turn, we have
We thank Mr Gowtham Kumar and Professor Sriram
E(X + (1 − α)S2 + Z − k(S1 + αS2 ))2
Vishwanath for helpful discussions.
2
= P + (1 − α) P2 + 2(1 − α)ρXS2
R EFERENCES
((1 − α)αP2 + αρXS2 + ρXS1 )2
[1] Y. H. Kim, A. Sutivong, and T. Cover, “State ampliﬁcation,” IEEE
+N −
P1 + α2 P2
Trans. Inf. Theory, vol. 54, no. 5, pp. 1850–1859, May 2008.
[2] C. Choudhuri, Y.-H. Kim, and U. Mitra, “Causal state ampliﬁ:= M SE(α, ρXS1 , ρXS2 ).
Note now that for α ﬁxed, M SE(α, ρXS1 , ρXS2 ) is a
concave (quadratic) function of ρXS1 and ρXS2 , and
the constraints on ρXS1 and ρXS2 are linear constraints.
Hence, we can ﬁnd the maximum value using convex
optimization. Denoting the maximum by M SE(α), we
arrive at the lower bound stated in the Theorem.
Comparison of bounds: We let P1 = 1, N = 1 and
P = 1, and vary P2 and compare the bounds obtained
with different values of P2 . Figure 4 shows a plot of
distortion v.s. P2 for various bounds. As can be seen
from Figure 4, the lower bound given by Theorem 5
can be strictly better than that given by previous lower
bounds.
From Figure 4, we see that the upper and lower bounds
are close when P2 is large. We now state a condition
on P2 for a constant multiplicative gap of between the
upper and lower bounds.
Theorem 6: If

[3]
[4]
[5]
[6]
[7]
[8]
[9]

[10]
[11]

P2
≥

√
√
√
γ 2 P + γ P (2 + γ P )(P (1 − γ 2 ) + N ) − γ P
√
√
,
γ P (2 + γ P )
5

5

[12]

cation,” in Proc. IEEE International Symposium on Information
Theory, St Petersburg, Russia, August 2011.
P. Grover and A. Sahai, “Witsenhausen’s counterexample as
assisted interference suppression,” submitted to International
Journal of Systems, Control and Communications.
H. Witsenhausen, “A counterexample in stochastic optimum
control,” SIAM J. Control, vol. 6, no. 1, pp. 131–147, February
1968.
P. Grover, A. Wagner, and A. Sahai, “Information embedding and
the triple role of control,” submitted to I.T. Transactions.
S. Verd´ , “Mismatched estimation and relative entropy,” IEEE
u
Trans. Inf. Theory, vol. 56, no. 8, pp. 3712–3720, August 2010.
D. Guo, Y. Wu, S. Shamai, and S. Verd´ , “Estimation in Gaussian
u
noise: Properties of the minimum mean-square error,” IEEE
Trans. Inf. Theory, vol. 57, no. 4, pp. 2371–2385, April 2011.
A. El Gamal and Y. H. Kim, Network Information Theory, 1st ed.
Cambridge University Press, 2011.
Y.-C. Huang and K. Narayanan, “Joint source-channel coding
with correlated interference,” in Proc. IEEE International Symposium on Information Theory, St. Petersburg, Russia, August
2011.
Y.-K. Chia, R. Soundararajan, and T. Weissman, “Estimation
with a helper who knows the interference,” submitted to I.T.
Transactions. Online at ArXiv: http://arxiv.org/abs/1203.4311.
S. H. Lim, P. Minero, and Y.-H. Kim, “Lossy communication of
correlated sources over multiple access channels,” in Proceedings
of the 48th Annual Allerton Conference on Communication,
Monticello, Illinois, September 2010, invited paper.
S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge
University Press, 2004.


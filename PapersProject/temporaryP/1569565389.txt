Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Fri May 18 18:27:23 2012
ModDate:        Tue Jun 19 12:54:12 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      441102 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565389

Shannon Entropy Convergence Results in the
Countable Inﬁnite Case
Patricio A. Parada

Jorge F. Silva

Department of Electrical Engineering
University of Chile
pparada@ing.uchile.cl

Department of Electrical Engineering
University of Chile
josilva@ing.uchile.cl

smaller or equal to > 0. This result shows that the ﬁnitesupport restriction is critical to bound the maximum deviation
of the entropy functional, since otherwise the expression is
unbounded; furthermore, they proved the continuity of entropy
for distributions deﬁned on a ﬁnite and known support [6, Theorem 6]. Complementing this continuity result, Harremoes [4,
Theorem 21] obtained convergence of the entropy by imposing
a power dominating condition [4, Def. 17] on the limiting
probability measure µ, for all the sequences {µn : n ≥ 0}
converging in reverse I-divergence to µ [7].
Another view of the problem was provided by Piera et al.
[10]. This work focused exclusively on the applications of
measure-theoretic convergence results [11], [12] to guarantee
the convergence of the entropy, instead of focusing on proving
continuity (in total variation or in any strong information topology [4]) in a subset of the space of probability measures. By
applying the dominated convergence theorem [11], [12], [13],
they showed sufﬁcient conditions for the entropy convergence
requiring that the limiting probability measure µ is neither
ﬁnitely supported nor power dominated, and only a ﬁnite
entropy for µ was assumed.
In this work we follow the approach presented in [10]. We
begin by revisiting the ﬁnite support scenario, where we stipulate necessary and sufﬁcient conditions for the convergence of
the entropy. This result is obtained for the case in which the
limiting distribution has a ﬁnite but unknown support, which,
to the best of our knowledge, has not been addressed before.
The results complement the fact that convergence in direct Idivergence is sufﬁcient for the entropy convergence [5], [6],
and the fact that controlling the support disagreement between
the limiting measure µ and the sequence {µn : n ≥ 0} is
critical to achieve limn→∞ H(µn ) = H(µ) [6]. For the
more challenging case where the limiting measure µ has an
inﬁnite and unknown support, we obtain new results that
provide sufﬁcient conditions for the entropy convergence, in
the unexplored scenario when µ
µn for all n. In the process,
we also obtain convergence of the distributions in direct and
reverse I-divergence. These results show that there is a stronger
interplay between Shannon entropy and reverse I-divergence
than between entropy and direct I-divergence, in terms of
the extra conditions needed to guarantee that distribution
convergence implies the convergence of the Shannon entropy
functional.

Abstract— The convergence of the Shannon entropy in the
countable inﬁnity case is revisited and extended in this work.
New results are presented that provide necessary and sufﬁcient
conditions for the convergence of the entropy in different settings,
including scenarios with both ﬁnitely and inﬁnitely supported
measures. These results show some connections between the
Shannon entropy convergence and the convergence in information divergence.

I. I NTRODUCTION
It is well known that the Shannon entropy in the ﬁnite
alphabet case is a continuous functional with respect to the
topology of the total variation in the space of probability
measures [1], [2]. In fact, this was one of the requirements
considered by Shannon in deﬁning this measure [3]. Surprisingly, the continuity does not hold if we move from the
ﬁnite alphabet to the countable inﬁnite alphabet case [4],
[5], [6]. In regard to this, recent results demonstrated the
discontinuity of entropy. In particular, Ho et al. [5, Theorem 2] showed concrete examples where convergence in χ2 divergence and in direct information divergence (I-divergence)
[7] (both stronger than total variational convergence [8], [9])
do not imply convergence in the entropy functional. These
constructions were extended to other information measures
in [5] (mutual information and their conditional versions)
proving their discontinuity with respect to the topology of
total variation [9]. Interestingly, these illuminating examples
consider distributions with ﬁnite support. On the other hand,
Harremeus [4] showed the discontinuity of the entropy with
respect to the reverse I-divergence [7], and consequently, with
respect to the total variational distance1 .
These results raised interest in studying sufﬁcient conditions
in a sequence of measures for which entropy convergence is
obtained in the countable inﬁnity alphabet case [4], [10], [6].
On this topic we can mention some recent contributions. The
ﬁrst is the work of Ho et al. [6], in which the interplay between
the entropy and the total variation distance was studied in
detail. This work reﬁned the results presented in [5], concerning the discontinuity of the entropy, but also found tight
bounds to control the entropy difference, |H(P ) − H(Q)|,
when the distributions P and Q have a total variation distance
1 The distinction between reverse and direct I-divergence was pointed out
in the work of Barron et al. [7], in the context of distribution estimation.

1

For D(µ||v) to be well-deﬁned, it is necessary that µ
v (or
equivalently that Aµ ⊂ Av ). It is well known that D(µ||v) ≥
0, and that D(µ||v) = 0 if, and only if, µ = v. Pinsker‘s
inequality offers a relationship between the I-divergence and
the total variational distance [19], [18], [8]. More precisely,
∀µ, v ∈ P(X),

II. P RELIMINARIES
Let X be a countable inﬁnite space, without loss of generality the integers, and let (X, B(X)) denote the measurable
space, in which the σ-algebra B(X) is the power set of X. Let
P(X) be the collection of probability measures in (X, B(X)).
Let µ and v be in P(X), if µ is absolutely continuous with
respect to v 2 , denoted by µ
v, then dµ (x) denotes the
dv
Radon-Nikodym (RN) derivative of µ with respect to v 3 [13],
[14]. In this context AC(X|v) denote the collection of µ ∈
P(X) where µ
v. Every µ ∈ P(X) is absolutely continuous
with respect to the counting measure λ in X (or the Lebesgue
measure)4 , where its RN derivate fµ (x) ≡ dµ (x) = µ ({x})
dλ
is the probability mass function (pmf). For any µ ∈ P(X) we
denote its support by Aµ ≡ supp(µ) ≡ {x ∈ X : fµ (x) > 0},
and we denote by
F(X) = {µ ∈ P(X) : λ(Aµ ) < ∞}

2 ln 2 · V (µ, v)2 ≤ D(µ, v).

Finally for any given v ∈ P(X), let H(X|v) denote the
collection of probability measures where the I-divergence with
respect to v is well deﬁned, i.e.,
H(X|v) =

H(X) ≡

f ∈ M(X) :

|f (x)| dµ(x) < ∞ ,

H(µ) ≡ −

|g(x)| dµ(x) =

|g(x)| fµ (x).

(3)

Let µ and v in P(X), the total variation distance of µ and
v is given by [9]

n→∞

(6)

X

A stronger notion of similarity between distributions was
proposed by Kullback and Leibler [16] (see also [17], [18],
[14], [8]) as an indicator of discrimination information. The
Kullback-Leibler divergence or I-divergence of µ with respect
to v is given by:
dµ
(x)dµ(x) =
D(µ||v) ≡
log
dv
X

x∈Aµ

fµ (x) log fµ (x) < ∞,
x∈Aµ

f (x)dµn =
X

f (x)dµ.

(12)

X

The standard notation for this convergence is µn ⇒ µ 5 . It is
simple to verify that µn ⇒ µ is equivalent to the point-wise
convergence of the measures (or the weak topology in P(X))
given by: limn→∞ µn ({x}) = µ({x}) for all x ∈ X [10].
Deﬁnition 2: A sequence {µn : n ∈ N} ⊂ P(X) is said to
converge in total variation (or variation) to µ ∈ P(X), if

From Sheffe‘s identity [15], this metric can be expressed in
terms of the L1 norm of the involved pmf‘s, i.e.,
|fµ (x) − fv (x)| dλ(x).

dµ
(x)·dµ(x) = −
dλ

lim

(5)

A∈B(X)

1
2

(10)

In the following, we denote the space of bounded realvalued functions form X to R by Mb (X).
Deﬁnition 1: A sequence {µn : n ∈ N} ⊂ P(X) is said to
converge weakly to µ ∈ P(X), if ∀f ∈ Mb (X),

A. Total Variation and Kullback-Leibler Divergence

V (µ, v) =

dµ
(x) ∈ l1 (µ) .
dλ

III. C ONVERGENCE N OTIONS

(4)

x∈Aµ

V (µ, v) ≡ sup |v(A) − µ(A)| .

µ ∈ P(X) : log

(11)
to be the Shannon entropy of µ [1], [14], [20]. Hence H(X) is
the space of ﬁnite entropy distributions, where it is simple to
verify that: F(X) ⊂ H(X); P(X) = H(X); and that there
are inﬁnite-supported distributions with ﬁnite entropy, i.e.,
F(X)c ∩ H(X) = ∅ [1], [14].

the space of µ integrable functions, where the L1 -norm of
g ∈ l1 (µ) is given by:

X

log
X

X

||g||L1 (µ) ≡

(9)

Then ∀µ ∈ H(X), we can properly deﬁne

Let M(X) denote the collection of functions from X to R.
Then for every µ ∈ P(X) let us deﬁne by
l1 (µ) ≡

dµ
(x) ∈ l1 (µ) .
dv

Let us consider the following collection of measures

(1)

(2)

x∈Aµ

µ ∈ AC(X|v) : log

B. Shannon Entropy

the collection of probability measures with ﬁnite support.
Then, it is simple to verify the following:
Proposition 1: µ belongs to F(X), if and only if,
mµ ≡ inf fµ (x) > 0.

(8)

lim V (µn , µ) = 0.
(13)
By (5) the convergence in total variation implies the uniform
convergence of the measures, i..e,
n→∞

lim sup |µn ({x}) − µ({x})| = 0.

n→∞ x∈X

fµ (x)
fµ (x) log
. (7)
fv (x)

(14)

It is well-known that the convergence in total variations is
stronger than the weak convergence, however when X is
countable these two notions are equivalent [10, Lemma 3].

2 A measure µ is absolutely continuous with respect to a measure v, if
v(A) = 0 implies that µ(A) = 0 for any event A ∈ B(X).
3 If µ
v then ∀A ∈ B(X), µ(A) = A dµ dv.
dv
4 The counting measure is given by λ(A) = |A| for all A ∈ B(X).

5 Note that the integrals in (12) are well-deﬁned since the measures involved
are ﬁnite (i.e., Mb (X) ⊂ l1 (µ), for all µ ∈ P(X)).

2

Finally, the last notion to mention is the convergence in
I-divergence introduces by Barron et al. [7].
Deﬁnition 3: Let {µn : n ∈ N} be a sequence in H(X|µ)
with µ ∈ P(X). We say that {µn : n ∈ N} converges to µ in
reverse I-divergence if,

Note that H (µn (·|Aµn \ Aµ )) ≤ log |Aµn \ Aµ | [1], then
we can state the following corollary.
Corollary 1: Under the setting of Theorem 1 if

lim D(µn ||µ) = 0.

then limn→∞ H(µn ) = H(µ).
Remark 2: From µn
⇒
µ, we have that
limn→∞ µn (Aµn \ Aµ ) = 0 (see proof of Theorem 1),
then if Aµn \ Aµ is uniformly in n bounded by a ﬁnite set,
i.e., ∃M > 0, ∃B ⊂ X a ﬁnite set such that Aµn \ Aµ ⊂ B
for all n > M , then we have the entropy convergence from
Corollary 1.
Theorem 1 also implies the entropy convergence result
stated at the beginning of this section.
Corollary 2: Under the assumptions of Theorem 1 if we
add the condition that µn
µ, then

n→∞

lim µn (Aµn \ Aµ ) · log |Aµn \ Aµ | = 0,

n→∞

(15)

Alternatively, let {µn : n ∈ N} be in P(X) and µ ∈ P(X)
such that µ
µn , for all n. Then {µn : n ∈ N} is said to
converge to µ in direct I-divergence if,
lim D(µ||µn ) = 0.
(16)
n→∞
From Pinsker‘s inequality in (8), the convergence in direct
and reverse I-divergence is stronger than total variational
convergence. The reciprocal result is not true [4], [8].
The following two sections will be devoted to ﬁnding conditions in a sequence {µn : n ∈ N} and its limiting measure
µ (in total variation) that guarantee the convergence of the
Shannon entropy functional, i.e., limn→∞ H(µn ) = H(µ).

lim H(µn ) = H(µ)

n→∞

and the following three convergence criteria are equivalent: limn→∞ V (µn , µ) = 0, limn→∞ D(µn ||µ) =
0 and limn→∞ D(µ||µn ) = 0.
The proof follows from Theorem 1.

IV. F INITELY S UPPORTED D ISTRIBUTIONS
We start with the scenario where µ ∈ F(X), i.e., the support
of the limiting measure is ﬁnite but unknown. In this context
if µn
µ, then µn ⇒ µ sufﬁces to obtain the convergence of
the entropy and convergence of the distribution in the reverse
I-divergence sense, as stated in [10, Sec. 5].
The next result does not impose either that µ
µn for all n,
or that µn
µ for all n. Furthermore, we obtain necessary
and sufﬁcient conditions for the convergence of the entropy
reﬁning the previously mentioned result.
THEOREM 1: Let µ ∈ F(X), {µn : n ∈ N} ⊂ F(X) and
let us assume that µn ⇒ µ. Then we have that:
i) There exists N > 0 such that µ
µn (Aµ ⊂ Aµn )
∀n ≥ N , and furthermore
lim D(µ||µn ) = 0,

n→∞

A. Proof of Theorem 1:
If µn
µ ∈ F(X) then D(µn ||µ) < ∞ and we can use
the following identity:
(fµn (x)−fµ (x)) log fµ (x)+D(µn ||µ).

H(µ)−H(µn ) =
x∈Aµ

(19)
However, considering that µn is not absolutely continuous with
respect to µ and vice versa, we have that:
H(µ) − H(µn ) =

[fµn (x) − fµ (x)] log fµ (x)
x∈Aµ ∩Aµn

(17)
+

ii) limn→∞ H(µn ) = H(µ), if and only if,

fµn (x) log
x∈Aµ ∩Aµn

lim µn (Aµn \ Aµ ) · H (µn (·|Aµn \ Aµ )) = 0. (18)

+

n→∞

fµ (x) log
x∈Aµ \Aµn

µ(·|B) denote the conditional probability with respect to the
event B ⊂ X.
Remark 1: 1) First note that in this ﬁnite-supported case,
µ is eventually absolutely continuous with respect to µn ,
and then, total variational convergence is equivalent to the
convergence in direct I-divergence (from i)). 2) The sufﬁcient
and necessary condition needed in ii) has to do with ensuring
a vanishing expression for the tail component of the entropy
of µn , restricted to the support disagrement Aµn \ Aµ . 3)
The convergence in direct I-divergence is not sufﬁcient to
guarantee the convergence of the entropy. Concrete examples
demonstrating this point have been presented in [5] for the
ﬁnitely supported scenario. 4) From the examples in [5],
controlling the support discrepancy Aµn \ Aµ seems to be a
critical ingredient for achieving the convergence of the entropy.
This fact is formalized in Theorem 1 point ii).

−

1
fµ (x)

fµn (x) log
x∈Aµn \Aµ

fµn (x)
fµ (x)

1
.
fµn (x)

(20)

The ﬁrst term of the RHS of (20) is upper bounded by Mµ ·
V (µn , µ), where
Mµ ≡ sup |log µ({x})| = log
x∈Aµ

1
< ∞,
mµ

(21)

which vanishes as n goes to inﬁnity. The second term tends to
zero as by the uniform convergence of µn to µ, we have that
fµn (x)
limn→∞ supx∈Aµ ∩Aµn fµ (x) − 1 = 0. For the third term,
note that by the point-wise convergence and the fact that |Aµ |
is ﬁnite, it is simple to verify that there exists N < ∞ such
that ∀n ≥ N , Aµ ⊂ Aµn (i.e., µ
µn eventually). Hence
Aµ \ Aµn is the empty set eventually, which implies that the

3

third term is zero eventually and, consequently, zero in the
asymptotic regime. Consequently we have that,
lim H(µn ) − H(µ) = lim

n→∞

n→∞

fµn (x) log
x∈Aµn \Aµ

V. I NFINITELY S UPPORTED D ISTRIBUTIONS
Here we consider the scenario where the support of µ could
be inﬁnite and unknown, i.e., |Aµ | = ∞. In this context Piera
et al. [10] introduced the following result.
THEOREM 2: [10, Theorem 4] Let µ ∈ H(X) and let us
consider that {µn : n ≥ 0} ⊂ AC(X|µ). If µn ⇒ µ and

1
.
fµn (x)
(22)

To conclude, the expression in the RHS of (22) is equivalent to
(18) from deﬁnition of conditional probability and the fact that
limn→∞ µn (Aµn \ Aµ ) · log (1/µn (Aµn \ Aµ )) = 0. Finally
(17) is taken directly from the uniform convergence of the
sequence to µ and the fact µ
µn eventually in n.

M ≡ sup sup
n≥1 x∈Aµ

lim D(µn ||µ) = 0 and lim H(µn ) = H(µ).
n→∞
Interpreting the result, to obtain the convergence of the
entropy without imposing a ﬁnite support assumption on the
limiting measure µ, a uniform bounding condition (UBC), µalmost surely, was added in (25). This UBC allows the use of
the dominated convergence theorem, and it is strictly needed
in that sense [12], [13], [11] 6 . As a consequence of adding the
UBC, the convergence on reverse I-divergence is also obtained,
reafﬁrming the relationship reported in Corollary 2 between
entropy convergence and convergence in reverse I-divergence
in the regime when µn
µ for all n.
Concerning the reciprocal case when µ
µn for all n, we
state the following result.
THEOREM 3: Let µ ∈ H(X) and a sequence of measures
{µn : n ≥ 1} ⊂ H(X), such that µ
µn for all n ≥ 1. If
i) µn ⇒ µ,
fµn (x)
ii) supn≥1 supx∈Aµ log fµ (x) < ∞
iii) limn→∞ x∈Aµn \Aµ fµn (x) log fµ 1(x) = 0,
n
then, µ ∈ H(X|µn ) for all n ≥ 1, and
n→∞

The proof of the discontinuity of the entropy in the countable inﬁnity case presented in [5, Theorem 2], was based on
the analysis of the following construction:
1−

α
α
α
,
, ..,
, 0, . . .
logn n log n
n log n

∈ F(X), (23)

with α > 0 and n ≥ 1. In fact, Ho et al.[5, Sec. III] showed
that limn→∞ D(µo ||µn ) = 0 for µo ≡ {1, 0, . . .} ∈ F(X),
where limn→∞ H(µn ) = α > H(µo ) = 0. From this example, the proof of the discontinuity of the entropy functional
at all probability measures was derived [5, Appendix A, pp.
5372]. Then it is of interest to revisit this key example in light
of the presented ﬁnite-supported results.
It is a simple exercise to verify that this discontinuity derives directly from Theorem 1, as we are in the ﬁnite support scenario where µn ⇒ µo = {1, 0, . . .} and
limn→∞ µn (Aµn \ Aµo ) · H (µn (·|Aµn \ Aµo )) = α > 0.
Furthermore from Theorem 1, we can stipulate necessary and
sufﬁcient conditions for the entropy convergence (or nonconvergence) for a more general construction that includes
(23) as a particular case. In particular, let us consider the
sequence of measures
µn (an , kn ) ≡

1 − an ,

an
an
, .., , 0, . . .
kn
kn

∈ F(X),

(25)

then, µn ∈ H(X) ∩ H(X|µ) for all n and

B. Revisiting Examples of Entropy Discontinuity and Convergence

µn =

fµn (x)
< ∞,
fµ (x)

lim D(µ||µn ) = 0 and lim H(µn ) = H(µ).

n→∞

n→∞

A. Proof of Theorem 3:
We use the following identity: H(µn ) − H(µ) =
(fµ (x) − fµn (x)) log fµn (x).

D(µ||µn ) +

(26)

x∈Aµn

Note that D(µ||µn ) < ∞ from the UBC in ii). Let us
f (x)
write D(µ||µn ) by X φn (x)dµ(x) with φn (x) ≡ log fµµ (x) .
n
Note that φn (x) is bounded µ-almost everywhere from ii),
and φn (x) → 0 point-wise from i). Then by the bounded
convergence theorem [12, Th. 1.4], limn→∞ D(µ||µn ) = 0.
Concerning the second term in (26),

(24)

function of (an )n≥1 , a sequence in (0, 1) being o(1), and
(kn )n≥1 , a sequence in N \ {0} with kn → ∞ (or (1/kn )
being o(1)). Note that limn→∞ µn (an , kn ) = µo , H(µn |Aµn \
Aµo ) = log kn , and µn (Aµn \ Aµo ) = an for all n ≥ 1.
Hence from Theorem 1, for the convergence of the entropy
it is necessary and sufﬁcient that limn→∞ an · log kn = 0
(equivalent to say that log(kn ) is o(1/an )). In particular, for
K
an = nτ and kn = nθ with K > 0, τ > 0 and θ > 0 the
α
convergence is obtained, while for an = log n and kn = nθ
with α > 0 and θ ∈ N the convergence is not obtained, which
1
is the case in (23). Finally for an = nβ and kn = γ n for
β > 0 and γ > 0, the convergence is obtained if and only
if β > 1, this scenario was constructed and analyzed in [5,
Sec. VII] using the marginal distributions of a Markov chain,
obtaining a congruent conclusion.

(fµ (x) − fµn (x)) log fµn (x) =
x∈Aµn

(fµ (x) − fµn (x)) log fµn (x)

(27)

x∈Aµn ∩Aµ

+

fµn (x) log
x∈Aµn \Aµ

1
.
fµn (x)

(28)

6 The proof was omitted in [10] as the authors claim that the argument
derived from the convergence result obtained for the differential entropy in
[10, Th.3, pp. 87-89]. We believe that this proof deserves an independent
argument, which is not presented here for the space constraints.

4

VI. O NGOING W ORK

The term in (27) can be expressed by:
log

fµ (x)
(fµ (x) − fµ (x))
fµn (x) n

(29)

We are currently working on the application of these ideas
and results into the problems of entropy estimation and estimation of distributions, consistently in information divergence.

log

1
(fµ (x) − fµ (x)).
fµ (x) n

(30)

VII. ACKNOWLEDGMENT

x∈Aµ

+
x∈Aµ

This material is based upon work supported by awards of
CONICYT-Chile, Fondecyt 1110145. We thanks the anonymous reviewer for valuable comments and suggestions.

˜1
The expression in (29) can be written as X fn (x)dµ(x) where
fµ (x)
1
˜
fn (x) ≡ 1Aµ (x) · log fµ (x) · (fµn (x)/fµn (x) − 1) ∈ l1 (µ).
n

f (x)

A PPENDIX I
B OUNDED C ONVERGENCE T HEOREM

From ii) both terms log fµµ (x) and (fµn (x)/fµn (x) − 1) are
n
˜1
upper bounded µ-almost everywhere. Then as fn (x) → 0
point-wise from i), this integral tends to zero by the bounded
convergence theorem.
˜2
On the other hand, (30) can be written as X fn (x)dµ(x)
˜2 (x) ≡ 1A (x) · log 1 · (fµ (x)/fµ (x) − 1) ∈
where fn
n
n
µ
fµ (x)
˜
l1 (µ). Hence from ii) there exists M < ∞ such that
˜2
fn (x) ≤ log

1
˜
· M , µ − almost everywhere.
fµ (x)

THEOREM 4: [12, Th. 1.4, pp. 10] Let {fn } be a sequence of measurable functions uniformly bounded with respect to a probability measure µ, if fn converges to f in
measure7 , then limn→∞ fn dµ = f µ.
R EFERENCES
[1] T. M. Cover and J. A. Thomas, Elements of Information Theory, Wiley
Interscience, New York, 1991.
[2] Robert B. Ash, Information Theory, Dover Publications, 1965.
[3] C. E. Shannon, “A mathematical theory of communication,” Bell System
Tech, J., vol. 27, pp. 379–423; 623–656, 1948.
[4] P. Herremoes, Entropy. Search, Complexity, vol. 16 of Bolyai Society of
Mathematical Studies, chapter Information topologies with applications,
pp. 113–150, Sprimger, 2007.
[5] Siu-Wai Ho and Raymond W. Yeung, “On the discontinuity of the
Shannon information measures,” IEEE Transactions on Information
Theory, vol. 55, no. 12, pp. 5362–5374, December 2009.
[6] Siu-Wai Ho and Raymond W. Yeung, “The interplay between entropy
and variational distance,” IEEE Transactions on Information Theory,
vol. 56, no. 12, pp. 5906–5929, December 2010.
[7] Andrew Barron, L. Gy¨ rﬁ, and Edward C. van der Meulen, “Distribution
o
estimation consistent in total variation and in two types of information
divergence,” IEEE Transactions on Information Theory, vol. 38, no. 5,
pp. 1437–1454, 1992.
[8] I. Csisz´ r and Paul C. Shields, Information theory and Statistics: A
a
tutorial, Now Inc., 2004.
[9] L. Devroye and G. Lugosi, Combinatorial Methods in Density Estimation, Springer - Verlag, New York, 2001.
[10] Francisco Piera and Patricio Parada, “On convergence properties of
Shannon entropy,” Problems of Information Transmission, vol. 45, no.
2, pp. 75–94, 2009.
[11] P. R. Halmos, Measure Theory, Van Nostrand, New York, 1950.
[12] S.R.S. Varadhan, Probability Theory, American Mathematical Society,
2001.
[13] Leo Breiman, Probability, Addison-Wesley, 1968.
[14] R. M. Gray, Entropy and Information Theory, Springer - Verlag, New
York, 1990.
[15] H. Scheff´ , “A useful convergence theorem for probability distribution,”
e
Ann. Math. Statist., vol. 18, pp. 434–458, 1947.
[16] S. Kullback and R. Leibler, “On information and sufﬁciency,” Ann.
Math. Statist., vol. 22, pp. 79–86, 1951.
[17] S. Kullback, Information theory and Statistics, New York: Wiley, 1958.
[18] I. Csisz´ r, “Information-type measures of difference of probability
a
distributions and indirect observations,” Studia Scient. Mathe. Hung.,
vol. 2, pp. 299–318, 1967.
[19] S. Kullback, “A lower bound for discrimiantion in terms of variation,”
IEEE Transactions on Information Theory, vol. 13, pp. 126–127, 1967.
[20] J. Beirlant, E. Dudewics, L. Gy¨ rﬁ, and Edward C. van der Meulen,
o
“Nonparametric entropy estimation: An overview,” Int. Math. Statist.
Sci., vol. 6, pp. 17–39, 1997.

(31)

˜2
Given that µ ∈ H(X) and the fact that fn (x) → 0 point-wise
from i), the integral in (30) tends to zero by the dominated
convergence theorem [12, Th. 1.8]. Then (27) tends to zero
and integrating the results,
lim H(µn ) − H(µ) = lim

n→∞

n→∞

fµn (x) log
x∈Aµn \Aµ

1
,
fµn (x)

which concludes the proof from iii).
Remark 3: It is important to mention that from i) and ii), we
have that µ ∈ H(X|µn ) ∀n, and that limn→∞ D(µ||µn ) = 0.
Hence, the tail vanishing entropy conditions in iii) is needed
to achieve the entropy convergence emphasizing, as in the
ﬁnite-supported result in Theorem 1, the non-sufﬁciency of the
convergence in direct I-divergence for the entropy convergence
in the regime when µ
µn for all n.
Proposition 2: Given the weak convergence µn ⇒ µ, a
sufﬁcient conditions to obtain iii) in Theorem 3 is that there
exists a ﬁnite set B and N > 0 such that Aµn \ Aµ ⊂ B for
all n ≥ N . (The proof is omitted for the space constraints.)
Then in the inﬁnite-supported scenario for µ, being able
to control the support disagreement is critical to obtain the
convergence of the entropy. A direct corollary of this result is
the following:
Corollary 3: Under the general assumption of Theorem 3,
if Aµ = X (consequently Aµn = X for all n), then under
the conditions i) and ii) of Theorem 3, limn→∞ D(µn ||µ) =
0, limn→∞ D(µ||µn ) = 0 and limn→∞ H(µn ) = H(µ).
Proof: The important thing to notice is that the UBC
of Theorem 3 (condition iii)) is stronger than the UBC of
Theorem 2 in (25), so then the convergence in reverse Idivergence is obtained from Theorem 2.
Finally note that in this last scenario, there is no support disagreement between µ and µn , for any ﬁxed n, which implies
a strong relationship between convergence in I-divergence and
entropy convergence.

7 The point-wise convergence of f to f implies, by deﬁnition, the almost
n
sure convergence of fn to f (with respect to µ), and consequently, the
convergence in measure stated in the theorem.

5


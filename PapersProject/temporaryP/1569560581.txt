Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Sat May 12 16:16:56 2012
ModDate:        Tue Jun 19 12:55:53 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      401617 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569560581

On Linear Index Coding for Random Graphs
Ishay Haviv

Michael Langberg

School of Computer Science
The Academic College of Tel Aviv-Yaffo, Israel

Department of Mathematics and Computer Science
The Open University of Israel

every client misses some of the information. At this step, the
clients have side information on the transmitted information,
and the network is interested in minimizing the broadcast
length in a way that enables the clients to decode their target
(see, e.g., [22]). The study of index coding is also motivated
by the more general problem of network coding, introduced
by Ahlswede et al. [1]. El Rouayheb et al. showed in [10] that
network coding instances can be efﬁciently reduced to index
coding instances. Hence, understanding the computational
complexity of solving the index coding problem (exactly or
approximately) has implications on that of network coding
(see [14], [15]).
For a graph G and a ﬁeld F we denote by β1 ( G ) the
minimum length of an index code for G over F. This graph
parameter is well-known to be related to several classical
graph parameters. Indeed, for an undirected graph G, β1 ( G )
is bounded from below by α ( G ), the maximum size of
an independent set in G, as follows from the fact that an
independent set in G corresponds to a set of receivers with
no mutual information. On the other hand, β1 ( G ) is bounded
from above by χ( G ), the clique cover number of G, as
follows from broadcasting the sum over F of the characters
corresponding to the vertices in every clique in an optimal
clique cover.1
In this paper we focus on linear index coding schemes, i.e.,
coding schemes in which the encoding function is linear. BarYossef et al. [5] proved that the minimum length of a linear
index code for a graph G over F equals the minimum rank
over F of a matrix that has nonzero values from F in the
diagonal and zeros in the entries that correspond to non-edges
(and arbitrary values from F in the other entries). This graph
parameter is called minrank and is denoted by minrkF ( G ).
Clearly, minrkF ( G ) bounds β1 ( G ) from above for every F.
Interestingly, it was proven in [5] that this upper bound is
tight and is equal to β1 ( G ) for several graph families for the
binary ﬁeld F2 . This includes directed acyclic graphs, perfect
graphs, odd holes (undirected odd-length cycles of length at
least 5) and odd anti-holes (complements of odd holes). These
results raised the question whether the minrank parameter
characterizes the minimum length of general index codes.
This question was answered in the negative by Lubetzky and
Stav [17], who showed that for any ε > 0 and a sufﬁciently
large n there is an n vertex graph G with β1 ( G )
nε and

Abstract—In the index coding problem, the goal is to transmit
an n character word over a ﬁeld F to n receivers (one character
per receiver), where the receivers have side information represented by a graph G. The objective is to minimize the length of a
codeword broadcasted to all receivers which allows each receiver
to learn its character. For linear index coding, the minimum
possible length is known to be equal to the minrank parameter.
In this paper we initiate the study of the typical minimum
length of a linear index code for the random graph G (n, p) over
a ﬁeld F. First, we prove that for every constant size ﬁeld F
and a constant p, the minimum length of a linear index code
√
for G (n, p) over F is almost surely Ω( n). Second, we introduce
and study two special models of index coding and study their
typical minimum length: Locally decodable index codes in which
the receivers are required to query at most q characters from
the encoded message (such codes naturally correspond to efﬁcient
decoding); and low density index codes in which every character
of the broadcasted word affects at most q characters in the
encoded message (such codes naturally correspond to efﬁcient
encoding procedures). We present enhanced results for these
special models.

I. I NTRODUCTION
In the index coding problem, a sender wishes to broadcast
an n character word x ∈ Fn (for a ﬁnite ﬁeld F) to n receivers
R1 , . . . , Rn in a way that enables every Ri to retrieve the ith
character xi . Every receiver has some side information on x.
The side information is represented by a directed graph G
on the vertex set [n] = {1, 2, . . . , n} in which a vertex i is
connected to a vertex j if and only if the receiver Ri knows
x j . Given a side information graph G, the goal is to ﬁnd a
coding scheme of minimum length, by which every receiver
Ri is able to retrieve xi given the encoded message and the
side information that it has on x according to G. The settings
are naturally extended to undirected graphs in which an edge
{i, j} means that Ri knows x j and R j knows xi .
For example, assume that every receiver Ri knows x j for
every j ∈ [n] \ {i }. The corresponding side information graph
is the complete graph on the vertex set [n]. In this case,
broadcasting the sum ∑i ∈ [n] xi over F enables every receiver
Ri to retrieve xi , and hence the minimum message length
required here is 1.
The study of index coding was initiated by Birk and Kol
in [6] and further developed by Bar-Yossef, Birk, Jayram and
Kol in [5]. This research is motivated by applications, such as
video on demand and wireless networking, in which a network
transmits information to clients, and during the transmission
This work was supported in part by ISF grant 480/08, the Open University
of Israel’s research fund (grant no. 46114), and the Adams Fellowship Program
of the Israel Academy of Sciences and Humanities. Work of Ishay Haviv was
done while at Tel Aviv University and the Open University of Israel.

1 A clique cover of G of size k is a partition of the vertex set of G into k
cliques. The chromatic number χ( G ) of the complement graph G equals the
minimum size of a clique cover of G.

1

minrkF2 ( G ) n1−ε (see [3] for additional counterexamples).
We note that the proof in [17] uses a property of the minrank
(see also [12]), saying that for every ﬁeld F and an n vertex
undirected graph G,
minrkF ( G ) · minrkF ( G )

n.

1
(and hence for any p
2 as well). To see this, notice that if G
is distributed according to G (n, 1 ) then so is its complement,
2
√
and hence the probability that minrkF ( G )
n is at
√
least 1 . We note, though, that any ω( n√lower bound on
)
2
the expectation above would imply an ω( n) lower bound
which holds almost surely, as follows from the large deviation
inequality for vertex exposure martingales (see, e.g., [4],
Chapter 7). Understanding the true value of minrkF ( G (n, p))
and, more speciﬁcally, the question whether one can show an
√
ω( n) lower bound on it, are the driving force of this work.

(1)

The ﬁrst to deﬁne the minrank parameter was Haemers [11],
[12], who related it to what is known as the Shannon capacity of graphs introduced in [21]. Haemers showed that for
every ﬁeld F and an undirected graph G, α ( G )
c( G )
minrkF ( G ), where c( G ) stands for the Shannon capacity of
G. He also showed that there are graphs for which the minrank
upper bound on the Shannon capacity is tighter than the one
given by the well-known Lov´ sz ϑ-function introduced in [16].
a
We note that calculating the minrank of a given input graph
is known to be computationally hard [14], [20], as opposed to
the efﬁciently computable Lov´ sz ϑ-function.
a
The following theorem summarizes some of the bounds
mentioned above.

A. Our Contribution
In the current paper we study the typical minimum length of
a linear index code for the random graph G (n, p) over a ﬁeld
√
F. We start by showing that an Ω( n) lower bound holds with
probability that (exponentially) tends to 1 as n tends to inﬁnity
(and not only in expectation). In addition, the bound holds for
every constant size ﬁeld F and a constant edge probability p.2
Theorem 2. For every constant size ﬁeld F and a constant
√
p ∈ (0, 1), almost surely minrkF ( G (n, p)) = Ω( n).

Theorem 1.[ [5], [11], [12]] For every ﬁeld F and an undirected
graph G, α ( G ) β1 ( G ) minrkF ( G ) χ( G ).

Observe that Theorem 2 implies that the random graph
1
G (n, 2 ) almost surely has an exponential gap between its
independence number and its minrank over any constant size
ﬁeld. In [2], Alon conjectured that the Shannon capacity of
1
G (n, 1 ) satisﬁes c( G (n, 2 )) = O(log n) almost surely. This,
2
if true, would imply an exponential gap between the Shannon
capacity and the minrank upper bound of Haemers [12] on it
for a typical graph G (n, 1 ).
2
In the attempt to understand √
where the minrank of G (n, p)
n
exactly lies in the range from n to log n we introduce and
study two natural special models of index coding.

All the inequalities in the above statement are known to be
strict for certain graphs. This makes the task of understanding
β1 ( G ) challenging. A fundamental parameter to study in this
context is the typical value of β1 ( G ) for random graphs G.
This question was raised by Lubetzky and Stav in [17] for the
well-known random graph G (n, 1 ), where G (n, p) denotes the
2
random undirected graph with n vertices in which every edge
is chosen to exist independently with probability p. In this
paper we focus on linear index codes and study the following
question:
What is the typical minimum length of a linear index code
for the random graph G (n, p) over F?

Locally decodable index coding. In our ﬁrst model we study
index codes in which the decoders are allowed to query a
limited number of characters from the encoded message. More
precisely, these are index codes in which the sender maps
x ∈ Fn to an encoded message, and each of the receivers
should be able to recover xi using at most q queries to the
encoded message and the information that the receiver has on
x according to the side information graph. Locally decodable
index codes naturally correspond to efﬁcient decoding, and
typically, as q grows smaller the minimum length of a locally
decodable index code for a given graph increases. The following theorem says that every linear locally decodable index
√
code for G (n, p) over F with q signiﬁcantly smaller than n
√
almost surely has length much higher than n. The Ω notation
is used to hide factors which are logarithmic in n.

Equivalently, we are asking for the typical minrank over F of
the random graph G (n, p).
Let us start with some bounds yielded by Theorem 1. Both
the independence number and the clique cover number of
G (n, p) are well understood (see [9] for the former and [8],
[18] for the latter). For a constant edge probability p, we obtain
that almost surely (i.e., with probability that tends to 1 as n
tends to inﬁnity),

(1 ± o(1)) ·

2 log n
1
log 1− p

minrkF ( G (n, p))

(1 ± o(1)) ·

n log

1
p

2 log ((1 − p)n)

.

Theorem 3. For every constant size ﬁeld F and a constant
p ∈ (0, 1), if there exists a linear index code of length for
G (n, p) over F, such that every decoding function queries at
1
most q = Ω(n 3 ) characters from the encoded message, then
almost surely = Ω( n ).
q

In short, for a constant p, almost surely, Ω(log n)
n
minrkF ( G (n, p)) O( log n ). The gap between these lower
and upper bounds is exponential, and, surprisingly, no better
bounds are known to hold almost surely for G (n, p). Yet, it is
plausible to expect the minrank of G (n, p) to be much higher
than the Ω(log n) lower bound, since the bound in (1) implies
√
1
that the expected minrank of G (n, p) is Ω( n) for p = 2

2 In fact, our proof provides a lower bound also for the case that |F| and p
depend on n. For the full statement of this result see Theorem 11.

2

Low density index coding. The second model we study
consists of linear index codes in which every character of
the word x (that the sender wishes to broadcast) affects a
limited number, say q, of characters in the encoded message.
Such codes are generated by generator matrices in which
every row has at most q nonzero entries, thus we call them
low density generator matrix index codes (or, in short, low
density index codes). Complementary to locally decodable
index codes, low density index codes correspond to efﬁcient
encoding procedures. Again, as q grows smaller the minimum
length of a low density index code for a given graph may
increase.
Low density (generator matrix) codes are usually not so
useful in coding theory. The reason is that such codes have
minimum distance at most q, whereas, in most applications,
one desires codes of large minimum distance. However, for our
purposes such codes turn to play a major role. More specif√
ically, our next theorem says that improving the n lower
bound on the length of low density index codes for G (n, p)
will imply such an improvement on the length of linear index
codes for G (n, p) in general. This is quite surprising since low
density index codes intuitively seem signiﬁcantly weaker than
general linear index codes. We state this result here informally,
and the formal statement can be found in Section V.
Theorem 4.[informal] Assume that every linear index code for
G (n, p) over F, with at most q = ω(1) nonzero entries in
√
a row of its generator matrix, has length ω( n) with √
high
probability. Then, almost surely, minrkF ( G (n, p)) = ω( n).

B. Outline
In Section II we provide some background preliminaries
needed throughout the paper. In Section III we prove the
√
Ω( n) lower bound given in Theorem 2. In Section IV we
prove our result on locally decodable index codes, and in
Section V we prove our results on low density index codes.
The ﬁnal Section VI discusses some concluding remarks and
open questions. Due to space limitations, our assertions appear
without detailed proofs. Complete proofs can be found in the
full version of the paper available online [13].
II. P RELIMINARIES
In the index coding problem a sender wishes to broadcast
a word x ∈ Fn (for a ﬁnite ﬁeld F) to n receivers R1 , . . . , Rn .
Every receiver Ri knows some ﬁxed subset of the characters
of x and is interested solely in the character xi . An -index
code for this setting is a length code over F, which enables
Ri to recover xi for every x ∈ Fn and i ∈ [n].
The index coding problem can be stated as a graph param+
eter. For a directed graph G and a vertex v let NG (v) denote
n and S ⊆ [n ]
the set of out-neighbors of v in G, and for x ∈ F
let x| S denote the restriction of x to the coordinates of S. The
setting of the deﬁnition of an index code is characterized by
the directed side information graph G on the vertex set [n]
where (i, j) is an edge if and only if the receiver Ri knows
x j . An -index code for G over F is a function E : Fn → F
and functions D1 , . . . , Dn , so that for all i ∈ [n] and x ∈ Fn ,
Di ( E( x), x| N + (i) ) = xi . The deﬁnition of an index code is
G
naturally extended to undirected graphs by replacing every
undirected edge by two oppositely directed edges. We say that
the index code is linear if the encoding function E is linear
(and thus is represented by an n × matrix). Bar-Yossef et
al. [5] showed that the minimum length of a linear index code
for G over F equals minrkF ( G ), a graph parameter deﬁned
as follows.

Theorem 4 motivates studying lower bounds on the length
of low density index codes for G (n, p). Observe that the
minimum length of a low density index code with q = 1 for a
graph G equals the clique cover number χ( G ). This implies a
n
tight lower bound of Ω( log n ) for q = 1. We are also able to
√
prove ω( n) lower bounds for low density index codes for
q = 2 and q = 3, as stated below. However, the analysis for
larger q (especially q = ω(1)) remains open.
Theorem 5. For every constant size ﬁeld F and sufﬁciently
small constants ε, p > 0, every q-low density index code for
G (n, p) over F almost surely has length at least n1−ε for q = 2
2
and n 3 −ε for q = 3.

Deﬁnition 6. Let A = ( ai j ) be an n by n matrix over some
ﬁeld F. We say that A represents an n vertex graph G over F if
aii = 0 for all i, and ai j = 0 whenever i = j and (i, j) is not an
edge in G. The minrank of a graph G over F is deﬁned as
minrkF ( G ) = min{rankF ( A) | A represents G over F}.

G (n, p) versus G (n, p). Although the results above are
stated for the undirected random graph G (n, p), it turns
out that the probability analysis is simpler if one were to
consider the directed random graph G (n, p). This is due to
the independence between the (outgoing) neighborhoods of
different vertices.3 We thus start by reducing the study of
the random undirected graph to that of the random directed
graph. Intuitively, this reduction is possible as the directed
graph G (n, p) essentially contains a copy of the undirected
graph G (n, p2 ) and is contained in the undirected graph
G (n, 1 − (1 − p)2 ) (due to space limitations, details of this
reduction are omitted and appear in the full version of this
work [13]).

We need the following simple claim, in which we use Bn (r)
to denote the set of vectors in Fn of Hamming weight (i.e.,
number of nonzero entries) at most r. The claim can be proven
in a greedy manner, details appear in our full version [13].
Claim 7. For every ﬁeld F, n, , r ∈ N, and a basis E ∈ Fn× ,
the number of indices of coordinates that are nonzero in at least
one vector in span( E) ∩ Bn (r) is at most r · . Here, span( E)
is the column span of E.
Let G (n, p), resp. G (n, p), denote the random undirected,
resp. directed, graph with n vertices in which every edge is
chosen to exist independently with probability p. We say that
G (n, p), resp. G (n, p), satisﬁes a graph property almost surely
if the probability that G (n, p), resp. G (n, p), satisﬁes this
property tends to 1 as n tends to inﬁnity.

3 The random graph G ( n, p ) does not have this property since a vertex i is
a neighbor of a vertex j if and only if j is a neighbor of i.

3

Lemma 10. For every ﬁeld F, a graph G, and a linear -index
code for G over F, at most n vertices in G are satisﬁed by
2
n
vectors of Hamming weight at most 2 .

Throughout the paper we ignore ﬂoors and ceilings whenever appropriate as this does not affect the asymptotic nature
of our results.
√
III. T HE Ω( n) L OWER B OUND
√
In this section we prove that minrkF ( G (n, p)) Ω( n)
almost surely. As noted, it sufﬁces to prove the lower bound
for the directed random graph G (n, p). We start with some
intuition. Fix a √
linear -index code generated by E ∈ Fn× for
certain = O( n). Our goal is to show that the probability
that E is an index code for G (n, p) is exponentially small, so
that applying the union bound over all the codes E will give
us the result. It is not hard to see that E is an index code for a
graph G if and only if for each vertex i there exists a vector in
the column span span( E) = span(e1 , . . . , e ) that is nonzero
in the ith entry and is zero in all the entries that correspond to
non-neighbors of i (see, e.g., [5, Claim 10]). This motivates
the following deﬁnition which will be useful throughout the
paper.
Deﬁnition 8. For a (directed) graph G on the vertex set [n], a
vector v ∈ Fn satisﬁes a vertex i ∈ [n] if vi = 0 and v j = 0 for
every j ∈ [n] \ {i } such that i is not connected to j in G.

Proof: Let E ∈ Fn× be a generator matrix of a linear
-index code for G over F. By Claim 7, the number of
indices of coordinates that are nonzero in at least one vector
n
in span( E) ∩ Bn ( 2 ) is at most n . Recall that a vector which
2
satisﬁes a vertex i must have the ith entry nonzero. Hence, the
number of vertices that can be satisﬁed by vectors in span( E)
n
of Hamming weight at most 2 is at most n .
2
√
The Ω( n) lower bound follows from combining Lemmas 9 and 10.
Theorem 11. For every ﬁeld F and p ∈ (0, 1), almost surely


1
log p 
√
minrkF ( G (n, p)) = Ω  n ·
.
log |F|
IV. L OCALLY D ECODABLE I NDEX C ODES

It now follows that any vector in span( E) of Hamming
weight r, whose ith entry is nonzero, satisﬁes a vertex i in
G (n, p) with probability pr−1 . Using this, we show that the
probability that at least n vertices are satisﬁed by vectors of
2
“high” Hamming weight is small (Lemma 9). On the other
hand, by Claim 7 we show that at most n vertices can be
2
satisﬁed by vectors of “low” Hamming weight (Lemma 10).
Thus, with high probability there exists a vertex in the graph
which is not satisﬁed by any vector in span( E), and hence
with such probability, E is not an index code for the graph.
The following lemma bounds from above the probability
that the graph G (n, p) has an index code for which many
vertices are satisﬁed by vectors of “high” Hamming weight.
Lemma 9. For every ﬁeld F and n, r, s ∈ N, the probability that
there exist a linear -index code E ∈ Fn× for G (n, p) over
F and s vertices, each of which is satisﬁed by a vector in
s
span( E) \ Bn (r), is at most (n) · |F|n · |F| · pr .
s

In this section we study locally decodable index codes
deﬁned as follows.
Deﬁnition 12. A (q, )-locally decodable index code is an index code in which the query complexity of the decoding is at
most q. This means that for every i the decoding function Di of
the ith receiver queries at most q characters from the encoded
message.
Remark 13. For every graph G, the minimum for which there
is a (1, )-locally decodable index code for G over F is the
clique cover number χ( G ) of G.
The following theorem shows a lower bound on the length
of a linear locally decodable index code for G (n, p) over F.
Although more involved, its proof follows the nature of the
proof given for Theorem 11 and can be found in [13].
Theorem 14. For every constant size ﬁeld F and a constant
p ∈ (0, 1), there exist constants c1 , c2 > 0 such that if
2/3
n
c1 · n 1/3 and q c2 · ·log then almost surely there is no

Proof: Fix a linear -index code E for G (n, p) over F
and a set S ⊆ [n] of s vertices. The probability that a vertex
i is satisﬁed by a ﬁxed vector y ∈ span( E) \ Bn (r) is at
most pr . To see this, notice that every vertex (except i) which
corresponds to a nonzero entry of y must be a neighbor of
i, and this happens independently with probability p. Taking
the union bound over all the vectors in span( E) \ Bn (r),
we get that the probability that a vertex is satisﬁed by a
vector in span( E) \ Bn (r) is at most |F| · pr . Hence, by
the independence of the edges in G (n, p), the probability that
every vertex in S is satisﬁed by a vector in span( E) \ Bn (r)

(log n)

linear (q, )-locally decodable index code for G (n, p) over F.
V. L OW D ENSITY G ENERATOR M ATRIX I NDEX C ODES
In this section we study low density generator matrix index
codes (or, in short, low density index codes). To obtain our
lower bounds (in Section V-B) we use proof techniques that
differ signiﬁcantly from those previously presented. A formal
deﬁnition of low density index codes follows.

s

is at most |F| · pr . Now, apply the union bound over all
the matrices E and sets S to get the desired bound.
Now we turn to deal with vertices which are satisﬁed by
vectors of “low” Hamming weight and to bound from above
their number.

Deﬁnition 15. A (q, )-low density index code is a linear index code in which every character of the broadcasted word
affects at most q characters in the encoded message. Equivalently, it is a linear -index code whose generator matrix has at
most q nonzero entries in a row.

4

A. The Reduction to q = ω(1)

the number of queries affects the length of locally decodable
index codes for G (n, p). We hope that the new notion of low
density index codes and Theorem 16 will be found useful in
understanding the minrank of G (n, p) over F.
Another challenging research direction is to study the vector
capacity of the random graph G (n, p) (see [3], [7], [17]). Here,
the sender wishes to broadcast a word x of n blocks, each
of t bits, to n receivers. The ith receiver is interested in the
ith block and has side information consisting of a subset of
the other blocks according to G (n, p). Denoting by βt the
minimum number of bits that has to be transmitted, we are
interested in limt→∞ βt . This limit represents the average
t
communication cost per bit in each block (for long blocks),
and it will be very interesting to compare it to β1 of a typical
random graph.

The following theorem shows that in order to prove an
√
ω( n) lower bound on the minimum length of a linear index
code for G (n, p) over a ﬁeld F, it sufﬁces to prove such a
lower bound on the length of a low density code for G (n, p)
over F for some q = ω(1). Full proof can be found in [13].
Theorem 16. For every ﬁeld F and p ∈ (0, 1), if the probability
that G (n, p) has a (q, )-low density index code over F is
√
2−ω(n) for some q = ω(1) and
= ω( n), then the
minimum length √ a linear index code for G (n, p) over F is
of
almost surely ω( n).
B. The Lower Bounds for q ∈ {2, 3}
The following theorem says that every index code for
G (n, p) whose generator √
matrix has at most 3 nonzero entries
in a row has length ω( n). Roughly speaking, our proof
is based on the following overview. Consider a ﬁxed linear
index code for G (n, p) with generator matrix E ∈ Fn× , and
assume that every row of E consists of at most 3 nonzero
entries. To utilize the low density assumption under study we
tie the notion of satisfaction from Deﬁnition 8 to the rows
of E instead of the column space (as in the previous proof
technique). More speciﬁcally, we show that a vertex i ∈ [n] is
satisﬁed by some vector in span( E) if and only if the ith row
of E cannot be written as a linear combination of the rows
corresponding to non-neighbors of i. Using the fact that the
rows of E are sparse, we analyze the probability that a vertex
i is satisﬁed by some vector in span( E). Our proof employs
a result of Naor and Verstra¨ te [19] addressing the maximum
e
possible size of a set of sparse vectors with no small linearly
dependent subsets. The proof can be found in [13].

R EFERENCES
[1] R. Ahlswede, N. Cai, S.-Y. R. Li, and R. W. Yeung. Network information
ﬂow. IEEE Trans. Inform. Theory, 46(4):1204–1216, 2000.
[2] N. Alon. The shannon capacity of a union. Combinatorica, 18:301–310,
1998.
[3] N. Alon, E. Lubetzky, U. Stav, A. Weinstein, and A. Hassidim. Broadcasting with side information. In FOCS, pages 823–832, 2008.
[4] N. Alon and J. H. Spencer. The probabilistic method. Wiley-Interscience
Series in Discrete Mathematics and Optimization. Wiley-Interscience
[John Wiley & Sons], New York, second edition, 2000.
[5] Z. Bar-Yossef, Y. Birk, T. S. Jayram, and T. Kol. Index coding with
side information. In FOCS, pages 197–206, 2006.
[6] Y. Birk and T. Kol. Coding on demand by an informed source (ISCOD)
for efﬁcient broadcast of different supplemental data to caching clients.
IEEE Trans. Inform. Theory, 52(6):2825–2830, 2006. An earlier version
appeared in INFOCOM 1998.
[7] A. Blasiak, R. D. Kleinberg, and E. Lubetzky. Index coding via linear
programming. CoRR, abs/1004.1379, 2010.
[8] B. Bollob´ s. The chromatic number of random graphs. Combinatorica,
a
8(1):49–55, 1988.
[9] B. Bollob´ s and P. Erd˝ s. Cliques in random graphs. Math. Proc.
a
o
Cambridge Philos. Soc., 80(3):419–427, 1976.
[10] S. El Rouayheb, A. Sprintson, and C. Georghiades. On the relation
between the index coding and the network coding problems. In
proceedings of IEEE International Symposium on Information Theory,
pages 1823–1827. IEEE Press, 2008.
[11] W. Haemers. On some problems of Lov´ sz concerning the Shannon
a
capacity of a graph. IEEE Trans. Inform. Theory, 25(2):231–232, 1979.
[12] W. Haemers. An upper bound for the Shannon capacity of a graph. In
Algebraic methods in graph theory, Vol. I, II (Szeged, 1978), volume 25
of Colloq. Math. Soc. J´ nos Bolyai, pages 267–272. North-Holland,
a
Amsterdam, 1981.
[13] I. Haviv and M. Langberg. On Linear Index Coding for Random Graphs.
ArXiv e-prints, 2011.
[14] M. Langberg and A. Sprintson. On the hardness of approximating
the network coding capacity. In proceedings of IEEE International
Symposium on Information Theory, pages 315–319. IEEE Press, 2008.
[15] A. R. Lehman and E. Lehman. Complexity classiﬁcation of network
information ﬂow problems. In SODA, pages 142–150, 2004.
[16] L. Lov´ sz. On the Shannon capacity of a graph. IEEE Trans. Inform.
a
Theory, 25(1):1–7, 1979.
[17] E. Lubetzky and U. Stav. Nonlinear index coding outperforming the
linear optimum. IEEE Trans. Inform. Theory, 55(8):3544–3551, 2009.
An earlier version appeared in FOCS 2007.
[18] T. Luczak. The chromatic number of random graphs. Combinatorica,
11(1):45–54, 1991.
[19] A. Naor and J. Verstra¨ te. Parity check matrices and product repree
sentations of squares. Combinatorica, 28(2):163–185, 2008. An earlier
version appeared in ISIT 2005.
[20] R. Peeters. Orthogonal representations over ﬁnite ﬁelds and the chromatic number of graphs. Combinatorica, 16(3):417–431, 1996.
[21] C. E. Shannon. The zero error capacity of a noisy channel. Institute of Radio Engineers, Transactions on Information Theory,, IT2(September):8–19, 1956.
[22] R. W. Yeung and Z. Zhang. Distributed source coding for satellite
communications. IEEE Trans. Inform. Theory, 45(4):1111–1120, 1999.

Theorem 17. For every ﬁeld F and a sufﬁciently small ε > 0
there exists a p = p (|F|, ε) > 0 such that for any p ∈ (0, p )
the following holds almost surely.
1) If there is a (2, )-low density index code for G (n, p)
over F then
n1−ε .
2) If there is a (3, )-low density index code for G (n, p)
2
over F then
n 3 −ε .
VI. C ONCLUDING R EMARKS AND O PEN Q UESTIONS
In this paper we initiated the study of index coding for
the random graph G (n, p) over a ﬁeld F and introduced
two new models of index coding – locally decodable index
coding and low density index coding. We proved several
lower bounds on the length of linear index codes for G (n, p)
(Theorems 11, 14, 17) and showed that in order to improve the
√
Ω( n) lower bound it sufﬁces to improve it for low density
index codes (Theorem 16).
The main task left for further work is to obtain tighter
bounds on the minimum length of index codes for the random
graph G (n, p) over a ﬁeld F. More speciﬁcally, it is not known
if there exists an index code for G (n, p) (linear or not) shorter
than the one achieved by the clique cover. It is interesting if
our lower bounds can be extended to general (non-linear) index
codes. It would be nice to understand better how the limit on

5


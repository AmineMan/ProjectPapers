Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Tue May 15 12:13:42 2012
ModDate:        Tue Jun 19 12:56:39 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      516811 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569566709

Action Dependent Strictly Causal State
Communication
Chiranjib Choudhuri and Urbashi Mitra
modifying its plan as new information from viewing the object
becomes available.

Abstract—Channels with action-dependent states are considered: given the message to be communicated, the transmitter
chooses an action sequence that affects the formation of the
channel states, and then creates the channel input sequence based
on the observed state sequence. The capacity–distortion tradeoff
of such a channel is characterized for the case when the state
information is available strictly causally at the channel encoder.
The problem setting extends the action dependent framework of
[1] and as a special case recovers the results of few previously
considered joint communication and estimation scenarios in [2],
[3], [4]. The scenario when the action is also allowed to depend on
the past observed states is also considered and it has been shown
that such adaptive action helps in achiveing a better capacity–
distortion function.

Channel with action-dependent states was introduced in [1]
and the capacity of such a channel both for the case where
the channel inputs are allowed to depend non-causally on the
state sequence, and that where they are restricted to causal
dependence are also characterized in [1]. We underscore that
our goals herein not only subsumes those of [1], where the
determination of channel capacity was the focus, but since
there is a natural tension between sending pure information
(message) and revealing the channel state, a different approach
is required to characterize the tradeoff as the coding strategy
optimal for achiveing capacity may not be a good code for
state estimation.

I. I NTRODUCTION
In this work, we consider a communication system where
encoding is in two parts: given the message, an action sequence is created. The actions affect the formation of the channel states, which are accessible to the transmitter in strictly
causal manner when producing the channel input sequence.
A channel with action-dependent states then is characterized
by the distribution of state given an action p(s|a) and the
distribution of the channel output given the input and state
p(y|x, s). We are interested in the scenario when in addition
to communicating pure information across the channel, the
transmitter also wishes to help reveal the channel state to the
receiver. We characterize the tradeoff between the independent
information rate and the accuracy of estimation of the channel state via the capacity-distortion function (ﬁrst introduced
in [2]). Our problem formulation is motivated is motivated
by a wide array of applications: active classiﬁcation [5],
underwater path planning [6], [7], data storage over memory
with defects [8], [9], dynamic spectrum access systems [10],
just to name a few. Each of these problems can be expressed
as a problem of conveying action dependent state to the
destination. For example, an autonomous vehicle performing
a active classiﬁcation task has control over how it views the
environment or state S. The vehicle could take actions such
as change its position, modify parameters on its sensor, or
even manipulate the environment to improve its view. Also
the autonomous vehicle is able to take actions adaptively by

Our problem framework can also be thought of as an
extension of [11], [2], [3], [4], as conditioned on the action
sequence the channel is equivalent to the one they study. So
the role of the action sequence in our framework is not only to
communicate the message, but also to setup a good communication channel for both pure information transmission and
state estimation. In fact we show that a two stage encoding
scheme is optimal, where in the ﬁrst stage the message is
communicated through the action sequence and then conditioned on the action sequence, a block Markov strategy similar
to the one in [3] is capacity–distortion optimal. We showed
that although strictly causal CSI is not useful to increase the
capacity, but it helps the receiver to get a better estimate of the
channel state. We also quantitatively characterize the beneﬁts
of feedback of the past states at the action stage. Beyond
merely generalizing previously considered problems involving
coding with states known at the transmitter, we show that
this adaptive action framework captures scenarios considered
in [12], [13] pertaining to multiple access channels (MAC)
with states.
The rest of this paper is organized as follows. Section II
describes the basic channel model with discrete alphabets,
characterizes the capacity–distortion function, establishes its
achievability and proves the converse part of the theorem.
Section III extends the results to the adaptive action setting,
wherein we allow the feedback from the past states to the
action encoder. Section IV illustrates our results with few
examples. Finally, Section V concludes the paper.

Chiranjib Choudhuri (cchoudhu@usc.edu) and Urbashi Mitra
(ubli@usc.edu) are with the Ming Hsieh Department of Electrical
Engineering, University of Southern California, University Park, Los
Angeles, CA 90089, USA.
This research has been funded in part by the following grants and organizations: ONR N00014-09-1-0700, NSF CNS-0832186, NSF CNS-0821750
(MRI), NSF CCF-0917343, NSF CCF-1117896 and DOT CA-26-7084-00.

Throughout the paper, we closely follow the notation in
[14]. In particular, For X ∼ p(x) and ∈ (0, 1), we deﬁne the
set of -typical n-sequences xn (or the typical set in short) [15]

1

as

We characterize this optimal tradeoff between information
transmission rate (capacity C) and state estimation (distortion
D) as follows.
Theorem 1: The capacity–distortion function for strictly
causal action dependent state communication is

|{i : xi = x}|
− p(x) ≤ p(x), ∀x ∈ X .
n
Finally, C(x) = (1/2) log(1 + x) denotes the Gaussian
capacity function.
T (n) (X) = xn :

A
CSC (D) = max I(U, A, X; Y ) − I(U, X; S |A) ,

II. P ROBLEM S ETUP AND M AIN R ESULT

where the maximum is over all conditional pmfs
p(a)p(x|a)p(u|x, s, a) and function s(u, x, a, y) such
ˆ
ˆ
that E(d(S, S)) ≤ D and I(U, X; Y |A) − I(U, X; S|A) ≥ 0.
Remark 1: It might sometimes be natural to consider channels of the form p(y|s, x, a). The capacity–distortion expression remains unchanged for this more general channel model.
This follows directly by deﬁning a new state S = (S, A) and
applying the above characterization.
Remark 2: When both the sender and the receiver is oblivious of the channel state, the capacity–distortion function for
action dependent state communication can be obtained by
choosing U = ∅ and is given by,

We assume a discrete memoryless channel (DMC)
with discrete memoryless state (DMS) model (X × S ×
A, p(y|x, s)p(s|a), Y) that consists of a ﬁnite input alphabet
X , a ﬁnite output alphabet Y, a ﬁnite state alphabet S, a
ﬁnite action alphabet A and a collection of conditional pmfs
p(y|x, s) on Y. The channel is memoryless in the sense that,
n
without feedback, p(y n |xn , sn ) = i=1 pY |X,S (yi |xi , si ), and
given the action sequence, the state is memoryless in the sense
that (S1 , S2 , . . .) is independent and identically distributed
(i.i.d.) with Si ∼ pS (si |ai ).

C A (D) = max I(X, A; Y ),

Fig. 1.

where the maximum is over all conditional pmfs p(a)p(x) and
ˆ
function s(x, a, y) such that E(d(S, S)) ≤ D.
ˆ
Before proving the Theorem 1, we recall a lemma and
A
summarize a few useful properties of CSC (D) (similar to
the [3, Corollary 1]).
Lemma 1: Suppose Z → V → W form a Markov chain
and d(z, z ) is a distortion measure. Then for every reconstrucˆ
tion function z (v, w), there exists a reconstruction function
ˆ
z ∗ (v) such that
ˆ

Strictly causal action dependent state communication.

A (2nR , n) code for strictly causal action dependent state
communication consists of
• a message set [1 : 2nR ],
• an action encoder that assigns an action sequence
an (m) ∈ An to each message m ∈ [1 : 2nR ]
• a channel encoder that assigns a symbol xi (m, si−1 ) ∈ X
to each message m ∈ [1 : 2nR ] and past state sequence
si−1 ∈ S i−1 for i ∈ [1 : n], and
• a decoder that assigns a message estimate m ∈ [1 : 2nR ]
ˆ
(or an error message e) and a state sequence estimate
ˆ
sn ∈ S n to each received sequence y n ∈ Y n .
ˆ
We assume that M is uniformly distributed over the message
(n)
set. The average probability of error is deﬁned as Pe =
ˆ
P{M = M }. The ﬁdelity of the state estimate is measured by
the expected distortion
1
ˆ
E(d(S n , S n )) =
n

E d(Z, z ∗ (V )) ≤ E d(Z, z (V, W )) .
ˆ
ˆ
This lemma traces back to Blackwell’s notion of channel
ordering [16], [17] and can be interpreted as a data processing
inequality for estimation.
A
Corollary 1: The capacity-distortion function CSC (D) in
Theorem 1 has the following properties:
A
(1) CSC (D) is a non-decreasing concave function of D for all
D ≥ D∗ ,
A
(2) CSC (D) is a continuous function of D for all D > D∗ ,
A
(3) CSC (D∗ ) = 0 if D∗ = 0 and CSC (D∗ ) ≥ 0 if D∗ = 0,
where D∗ is the minimum distortion with strictly causal
channel state at the sender akin to the zero rate case in [3].
In the following two subsections, we prove Theorem 1.

n

ˆ
E(d(Si , Si )),

A. Sketch of Achievability:

i=1

We use b transmission blocks, each consisting of n symbols.
The channel encoder uses rate-splitting technique, whereby in
block j, it appropriately allocates it’s rate between cooperative
transmission of common message mj and a description of the
state sequence S n (j − 1) in block j − 1.

ˆ
where d : S × S → [0, ∞) is a distortion measure between
ˆ
a state symbol s ∈ S and a reconstruction symbol s ∈ S.
ˆ
Without loss of generality, we assume that for every symbol
ˆ
s ∈ S there exists a reconstruction symbol s ∈ S such
ˆ
that d(s, s) = 0. A rate–distortion pair is said to be achievˆ
able if there exists a sequence of (2nR , n) codes such that
(n)
ˆ
limn→∞ Pe = 0 and lim supn→∞ E d(S n , S n ) ≤ D. The
A
capacity–distortion function CSC (D) is the supremum of the
rates R such that (R, D) is achievable.

Codebook
generation.
Fix
a
conditional
pmf
p(a)p(x|a)p(u|x, s, a) and function s(u, x, y, a) that attain
ˆ
A
CSC (D/(1 + )), where D is the desired distortion, and
let p(u|x, a) =
s p(s|a)p(u|x, s, a). For each j ∈ [1 : b],

2

randomly and independently generate 2nR sequences an (mj ),
n
mj ∈ [1 : 2nR ], each according to i=1 pA (ai ) and for
n
nRS
each a (mj ), generate 2
sequences xn (mj , lj−1 ),
nR
mj ∈ [1 : 2 ], lj−1 ∈ [1 : 2nRS ], each according to
n
nR
], lj−1 ∈ [1 : 2nRS ],
i=1 pX|A (xi |ai ). For each mj ∈ [1 : 2
˜
randomly and conditionally independently generate 2nRS
˜
n
nR S
sequences u (kj |mj , lj−1 ), kj ∈ [1 : 2
], each according
n
to
pU |X,A (ui |xi (mj , lj−1 ), ai (mj )). Partition the
i=1
˜
set of indices kj ∈ [1 : 2nRS ] into equal-size bins
˜
˜
B(lj ) = [(lj −1)2n(RS −RS ) +1 : lj 2n(RS −RS ) ], lj ∈ [1 : 2nRS ].
The codebook is revealed to the both encoder and the decoder.

(n)

i=1
n

(c)

(d)

(I(Ui , Xi , Ai ; Yi ) − I(Ui , Xi ; Si |Ai )) + n n ,
i=1

where (a) can be shown by Fano’s inequality [18, Theorem
7.7.1], (b) follows from the Csis´ar sum identity [14, Sec. 2.3]
z
and since An is a function of M , (c) follows from the fact
that given Ai , (M, S i−1 , An\i ) is independent of Si , and (d)
is true as Xi is a function of (M, S i−1 ). Similarly, for this
choice of Ui ,

.

n

n
n
I(M, S i−1 , Yi+1 , An\i , Xi ; Si |Ai )

I(Ui , Xi ; Si |Ai ) =
i=1

i=1
n
n
I(Yi+1 ; Si |M, S i−1 , An )

=
(b)

i=1
n
n
I(S i−1 ; Yi |M, Yi+1 , An )

=

i=1
n
n
I(M, S i−1 , Yi+1 , An\i ; Yi |Ai )

≤
(d)

i=1
n

I(Ui , Xi ; Yi |Ai ).

=

i=1

So now we have
R≤
(a)

≤
(b)

1
n
1
n

n

n

I(Ui , Xi , Ai ; Yi ) −
i=1
n

I(Ui , Xi ; Si |Ai ) + n

n

i=1
A
CSC (E(d(Si , si (Ui , Xi , Ai , Yi )))) + n
ˆ

n

i=1

A
≤ CSC

1
n

n

E(d(Si , si (Ui , Xi , Ai , Yi ))) + n
ˆ

n

i=1

(c)

A
≤ CSC (D),

where (a) follows from the deﬁnition of capacity-distortion
A
function, (b) follows by the concavity of CSC (D) (see Property
1 of Corollary 1), and (c) can be shown using Lemma 1 and
Corollary 1. This completes the proof of Theorem 1.

(a)

n

n
n

III. A DAPTIVE ACTION

i=1
n
n

i=1
n
n
n
(I(M, Yi+1 , S i−1 ; Yi ) − I(S i−1 ; Yi |M, Yi+1 )) + n

=

n

i=1
n

=

nR = H(M )

n
I(M, Yi+1 ; Yi ) + n

n
I(M, S i−1 , Yi+1 , An\i ; Si |Ai ) + n

−

We need to show that given any sequence of (2nR , n)-codes
(n)
ˆ
with limn→∞ Pe = 0 and E(d(S n , S n )) ≤ D, we must
A
have R ≤ CSC (D). We identify the auxiliary random variables
n
Ui := (M, S i−1 , Yi+1 , An\i ), i ∈ [1 : n] with (S0 , Yn+1 ) =
(∅, ∅). Note that, as desired, (Ui , Ai ) → (Xi , Si ) → Yi form
a Markov chain. Consider

≤

n
I(M, Yi+1 , S i−1 , An ; Yi )
i=1
n

B. Proof of the Converse

n
I(M ; Yi |Yi+1 ) + n

n

i=1
n

=

Decoding. Let
>
. At the end of block j + 1,
the receiver ﬁnds the unique index mj+1 , ˆj such that
ˆ
l
(n)
(xn (mj+1 , ˆj ), y n (j + 1), an (mj+1 )) ∈ T . It then looks
ˆ
l
ˆ
ˆ
for the unique compression index kj ∈ B(ˆj ) such that
l
n ˆ
ˆj−1 ), xn (mj , ˆj−1 ), an (mj ), y n (j)) ∈ T (n) and
(u (kj |mj , l
ˆ
ˆ l
ˆ
ˆ
kj ∈ B(ˆj ). Finally it computes the reconstruction sequence
l
ˆ ˆ l
as si (j) = s(ui (kj |mj , ˆj−1 ), xi (mj , ˆj−1 ), ai (mj ), yi (j)) for
ˆ
ˆ
ˆ l
ˆ
i ∈ [1 : n].
Following the analysis of capacity–distortion function in [3],
it can be easily shown that the scheme can achieve any rate
up to the capacity-distortion function given in Theorem 1.

=

n
I(Yi+1 ; Si |M, S i−1 , An ) + n

−

If there is more than one such index, it selects one of them
uniformly at random. If there is no such index, it selects an
˜
index from [1 : 2nRS ] uniformly at random. In block j + 1, the
action encoder chooses the action sequence an (mj+1 ), where
mj+1 is the new message index to be sent in block j + 1. Let
sn (j + 1) be the channel state sequence generated in response
to the action sequence. The channel encoder then transmits
xn (mj+1 , lj ) over the state dependent channel in block j + 1,
where lj is the bin index of kj .

≤ I(M ; Y n ) + n

n
I(M, Yi+1 , S i−1 , An ; Yi )

=

Encoding. By convention, let l0 = 1. At the end of block j,
the sender ﬁnds an index kj such that
(sn (j), un (kj |mj , lj−1 ), xn (mj , lj−1 ), an (mj )) ∈ T

n

(b)

n

i=1

3

It is natural to wonder whether “feedback” from the past
states at the action stage (ai (m, si−1 )) increases the capacitydistortion function or not. For an extreme example, consider
a channel for which p(y|s, x, a) = p(y|s, a). Clearly, the
capacity–distortion function for any such channel with only

message dependent non-adaptive action (an (m)) is same as
that of no CSI, since the action encoder is oblivious of the
channel state. But with adaptive action, the action encoder
can perform block Markov strategy to yield a potentially larger
capacity–distortion function, which is summarized below without proof.
Theorem 2: The capacity–distortion function for strictly
causal adaptive action dependent state communication is

can condition on it and can perform the usual block Markov
strategy on each subsequence associated with each action
symbol, achieving a rate of I(U, X; Y |A)−I(U, X; S|A). The
maximization is a search for the optimal tradeoff between the
amount of information that can be conveyed by the actions,
and the quality of the second stage channel that they induce.
B. Gaussian Channel with Additive Action Dependent State
Consider the Gaussian channel with additive action dependent state [1]

AA
CSC (D) = max I(U, A, X; Y ) − I(U, X; S |A) ,

where the maximum is over all conditional pmfs
p(a)p(x|a)p(u|x, s, a) and function s(u, x, a, y) such
ˆ
ˆ
that E(d(S, S)) ≤ D.
Note that the unconstrained capacity remains unchanged
even if we allow the actions to depend on the past states. But
AA
A
in general CSC (D) ≥ CSC (D) as the adaptive action helps
the receiver to get a better estimate of the state. Finally, by
setting A = ∅ in Theorem 2, we recover the result by [3] on the
capacity–distortion function when the i.i.d. state information
is available strictly causally at the encoder.
Remark 3: When the past states are available at both the
encoders, the encoders cooperate to send information consisting of the common message and a description of the state
in previous block (similar to sending a common message
over multiple access channel (MAC)), whereas in the nonadaptive action scenario, while the common message is sent
cooperatively, description of the state is a private message of
the channel encoder.
Remark 4: In the converse proof of Theorem 1,
we have used the following Markov chain condition
(M, S i−1 , An\i ) → Ai → Si , which need not
hold when allowing adaptive actions and hence the
converse proof for adaptive action necessitates different
deﬁnition of the key auxiliary random variable given by
n
Ui := (M, S i−1 , Yi+1 , Ai−1 ).

Y =X +S+Z
˜
= X + A + S + Z,
˜
where S ∼ N(0, Q) and the noise Z ∼ N(0, N ) are independent. Assume an expected average power constraint on both
the channel and action encoder
n

n

E(x2 (m, S i−1 ))
i
i=1

E(a2 ) ≤ nPA .
i

≤ nPX ,
i=1

We consider the squared error (quadratic) distortion measure
d(s, s) = (s − s)2 . When the action sequnce is only a function
ˆ
ˆ
of the message, using Theorem 1 we have the following.
Proposition 1: The capacity–distortion function of the
Gaussian channel with message dependent action is

A
0 ≤ D < Dmin ,
0,


A
P
1
A
A
Dmin ≤ D < Dmax ,
CSC (D) = 2 log QN/D ,
√
√

2

C ( PX + PA ) , D ≥ D
max .
Q+N
A
where Dmin =

QN
PX +Q+N ,

Dmax =

QN
Q+N

and P A = PX +

Q + N + PA + 2 PA (PX − ( QN − (Q + N ))).
D
When we allow the action encoder to observe the past states,
the capacity–distortion follows from Theorem 2 and it has the
A
similar form of Proposition 1, but P A and Dmin are replaced
AA
AA
AA
by P
and√ min , respectively, where P
D
= PX + Q +
AA
N + PA + 2 PA PX and Dmin = QN/P AA .

IV. I LLUSTRATIVE E XAMPLES
In the following subsections, we illustrate Theorem 1 and
Theorem 2 through simple examples.
A. Actions Seen by Decoder:
Consider the case where the decoder also has access to the
actions taken. Noting that this is a special case of our setting by
taking the pair (Y, A) as the new channel output, that U →
(X, S, A) → Y if and only if U → (X, S, A) → (Y, A),
we obtain that the capacity–distortion function for the case of
message depepdent action is given by
A
CSC (D) = max H(A) + I(U, X; Y |A) − I(U, X; S |A) ,

where the maximization is over the same set of distribution
and same feasible set as in Theorem 1. Similarly we can evaluate the capacity–distortion function for the case of adaptive
actions. This expression is quite intuitive: The amount of information per symbol that can be conveyed through the actions in
the ﬁrst stage is represented by the term H(A). In the second
stage, both encoder and decoder know the action sequence, so

Fig. 2.

Capacity–distortion function: adaptive vs. non-adaptive

The proof of the proposition is omitted here for brevity.
Note that since P AA ≥ P A , the capacity–distortion function
is larger in the adaptive action scenario (see Fig. 2). In fact,

4

the minimum distortion achievable with adaptive action is
smaller than that of non-adaptive action. But the unconstrained
capacity (capacity–distion function for D ≥ Dmax ) is same
in both the cases, which implies that adaptive action in useful
in estimation rather than in information transmission. Finally
by substituting PA = 0, both the capacity–distortion functions
converges to the one in [3].

Fig. 3.

V. C ONCLUSION
In [1], they have extended the study of channels with states
known at the transmitter to the case where the formation of
the states is affected by actions taken at the encoder and they
characterize the fundamental limits on reliable communication
for such channels. In this work, we have extended their framework to include the scenario when the receiver is not only
interested in decoding the message, but also in estimating the
channel state in distortion. We have characterized the capacity–
distortion function of such channels when the state is available
strictly causally at (a) only the channel encoder, and (b) both
the action encoder and channel encoder. By realizing that
conditioned on the action sequence our framework is similar
to the one in [3], we have shown that a two stage encoding
strategy is optimal. We have also shown that state-dependent
MAC with symmetric and asymmetric state information is a
special case of our channel model and using our results we
could recover common message capacity results of MAC with
strictly causal CSI (see [12], [13]).

State dependent MAC with strictly causal CSI at both encoders.

R EFERENCES

C. State dependent MAC
Consider communicating a common message over a memoryless state-dependent MAC (see Fig. 3) characterized by
p(y|s, x1 , x2 ), where the state sequence is known strictlycausally to both encoders. This problem can be seen as a
special case of our adaptive action setting via the following
associations:
A = X2 , X = X1 , p(s|a) = p(s), p(y|s, a, x) = p(y|s, x1 , x2 ).
Applying Theorem 2 to this case, keeping in mind the Remark 1 following the statement of the Theorem 2, about
channels of the form p(y|s, x, a), we get that the capacity–
distortion function is given by
S
CSC (D) = max I(U, X2 , X1 ; Y ) − I(U, X1 ; S |X2 ) ,

where the maximum is over p(x1 , x2 )p(u|x1 , s, x2 ) and funcˆ
tion s(u, x1 , x2 , y) such that E(d(S, S)) ≤ D. This setting
ˆ
was considered in [12], [13] and it recovers the common
message capacity results of [12], [13]. One can also consider
a scenario where the state sequence is known strictly-causally
to the ﬁrst encoder, but unknown at the second encoder and
at the receiver. This problem, motivated by multiterminal
communication scenarios involving transmitters with different degrees of channel state information, is a special case
of Theorem 1, we get that the capacity–distortion function
AS
S
(CSC (D)) is same as CSC (D) with additonal constraint of
I(U, X1 ; Y |X2 ) − I(U, X1 ; S|X2 ) ≥ 0 on the feasible distriS
AS
butions. Clearly CSC (D) ≥ CSC (D), since with symmetric
channel state information the encoders can jointly perform
both message and state cooperation as oppose to only message
cooperation when the state information is available at only one
of the encoders.

5

[1] T. Weissman, “Capacity of channels with action-dependent states,” IEEE
Trans. Inf. Theory, vol. 56, pp. 5396–5411, 2010.
[2] W. Zhang, S. Vedantam, and U. Mitra, “A constrained channel coding
approach to joint transmission and state estimation problem,” accepted
in IEEE Trans. Inf. Theory, Mar. 2011.
[3] C. Choudhuri, Y.-H. Kim, and U. Mitra, “Capacity-distortion trade-off in
channels with state,” in Proc. 48th Ann. Allerton Conf. Comm. Control
Comput., Allerton, IL, Sep. 2010, pp. 1311–1318.
[4] ——, “Causal state ampliﬁcation,” in Proc. IEEE Int. Symp. Inf. Theory,
St. Petersburg, Russia, Aug. 2011, pp. 2110–2114.
[5] M. Naghshvar and T. Javidi, “Active m-ary sequential hypothesis testing,” Austin, USA, Aug. 2010.
[6] I. Vasilescu, K. Kotay, D. Rus, M. Dunbabin, and P. Corke, “Data
collection, storage, and retrieval with an underwater sensor network,”
pp. 154–165, 2005.
[7] G. Hollinger, U. Mitra, and G. Sukhatme, “Active classiﬁcation: Theory and application to underwater inspection,” in Proc. International
Symposium on Robotics Research (ISRR), Flagstaff, AZ, Aug. 2011.
[8] A. V. Kusnetsov and B. S. Tsybakov, “Coding in a memory with
defective cells,” Probl. Control Inf. Theory, vol. 10, no. 2, pp. 52–60,
Apr. 1974.
[9] C. Heegard and A. El Gamal, “On the capacity of computer memories
with defects,” IEEE Trans. Inf. Theory, vol. 29, no. 5, pp. 731–739,
1983.
[10] S. Haykin, “Cognitive radio: brain-empowered wireless communications,” IEEE J. Select. Areas Comm., vol. 23, no. 2, pp. 201–220, Feb.
2005.
[11] A. Sutivong, M. Chiang, T. M. Cover, and Y.-H. Kim, “Channel capacity
and state estimation for state-dependent Gaussian channels,” IEEE Trans.
Inf. Theory, vol. 51, no. 4, pp. 1486–1495, Apr. 2005.
[12] A. Lapidoth and Y. Steinberg, “The multiple access channel with
causal and strictly causal side information at the encoders,” in Proc.
International Zurich Seminar on Communications, Mar. 2010.
[13] M. Li, O. Simeone, and A. Yener, “Multiple access channels with states
causally known at transmitters,” arXiv:1011.6639, 2011.
[14] A. El Gamal and Y.-H. Kim, Network Information Theory. Cambridge
University Press, 2012.
[15] A. Orlitsky and J. R. Roche, “Coding for computing,” IEEE Trans. Inf.
Theory, vol. 47, no. 3, pp. 903–917, Mar. 2001.
[16] D. Blackwell, “Equivalent comparisons of experiments,” Ann. Math.
Statits., vol. 24, pp. 265–272, 1953.
[17] M. Raginsky, “Shannon meets Blackwell and Le Cam: channels, codes,
and statistical experiments,” in Proc. IEEE Int. Symp. Inf. Theory, St.
Petersburg, Russia, Aug. 2011, pp. 1220–1224.
[18] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed.
New York: Wiley, 2006.


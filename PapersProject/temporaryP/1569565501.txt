Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Wed May  2 22:40:05 2012
ModDate:        Tue Jun 19 12:54:39 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      452238 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565501

An Upper Bound on Relaying over Capacity
Feng Xue
Qualcomm Research, 5665 Morehouse Drive, San Diego, CA 92121
Email: fengxue@ieee.org

to the destination Y is lossless with rate R0 . That is, in a
transmission over n slots, an index from {1, 2, · · · , 2nR0 },
called “color” in the paper, can be sent to Y without error. Note
the cut-set bound is maxp(x) min{I(X; Y )+R0 , I(X; Y, Z)}.

Abstract—The upper bound on the capacity of a 3-node
discrete memoryless relay channel is considered, where a source
X wants to send information to destination Y with the help of
a relay Z. Y and Z are independent given X, and the channel
is statistically degraded in the sense that XY Z or XZY can be
re-described statistically as a Markov chain. The link from Z to
Y is lossless with rate R0 . A new method is introduced to bound
the capacity when the encoding rate is beyond both individual
links XY and XZ. It generalizes the well-known blowing-up
lemma and links it with conditional entropy. The new bound
is explicitly computable and strictly better than the well-known
cut-set bound when the latter is CXY + R0 . The binary erasure
channel is analyzed in detail as an example, and generalization
to more cases is also discussed.

Z
R0

X

I. I NTRODUCTION

Fig. 1.

The relay channel model was ﬁrst formulated by Van-der
Meulen [1] in 1971, consisting a source X, a relay Z, and a
destination Y . The relay transmits a signal X1 based on its
observation to help Y . As a basic building block of the general
wireless networks, it has since then attracted much research
interests; see e.g. [2] and references therein.
A set of achievability results were introduced in Cover and
El Gamal [3]. Decode-forward and compress-forward are two
basic achievability methods. Several capacity results were also
established for cases such as degraded, reverse degraded [3],
and semi-deterministic channels [5]. They are all based on the
well-known cut-set bound; see e.g. Chapter 14 of [4].
In general, however, it is hard to establish capacity. The
cut-set bound is the typical tool, but it seems not tight in
many cases. Some progress of improving the cut-set bound
was made by Zhang in 1988 [12] for the channel depicted in
Figure 1. When the link from the relay to the destination is
lossless with ﬁxed rate R0 , it is shown that the cut-set bound
cannot be tight when Y and Z are conditionally independent
given X and X-Z-Y can be re-described as a Markov chain.
However, the paper does not provide a new bound. In [7], a
class of modulo additive noise relay channels is considered,
in which the relay observes a corrupted version of the noise.
The capacity is established and is strictly lower than the cutset bound. On the other hand, the cut-set bound is shown to
be tight when the relay output Z is a deterministic function
of X and Y [6].
In this paper, we consider a discrete memoryless channel
depicted in Figure 1 as introduced in [12], characterized by
the conditional distribution p(y, z|x). Y and Z are assumed
to be independent given X, and the channel is statistically
degraded in the sense that XY Z or XZY can be re-described
statistically as a Markov chain. The link from the relay Z

Y

Relay Network with lossless relay-destination link

We introduce a new method to improve the cut-set bound
for when the encoding rate is beyond the capacities of both the
XZ and XY links. Note that in this regime, decode-forward
obviously does not work.
We explain the main idea as follows by focusing on the
case when Y and Z are i.i.d. given X. Suppose otherwise
that the rate I(X; Y ) + R0 is achievable, then the relay Z
must fully utilize the wired link to Y . It turns out that the
sender X is almost sure about the color the relay will send
to Y , even though it is at the “other side” of a channel.
Mathematically it is that, given X n , the conditional entropy
of the message from Z to Y is diminishing. Since Y n has
the same distribution as Z n given X n , Y can guess the color
correctly with high conﬁdence. This means there is no need
of the relay, leading to a contradiction. In general, the sender
X must have certain control on the color of Z’s observation,
being represented by the conditional entropy of the color given
the transmitted code word. Denote S(Z n ) as the set of z n ’s
with the same color of the received signal Z n at the relay.
By generalizing the well-known blowing-up lemma [8], one
can show that Y n will be within a certain Hamming distance
d∗ of S(Z n ) with non-diminishing probability. Thus the node
Y can randomly pick a point and ﬁnd its color in the ball of
radius d∗ around Y n . Then for this color and Y n , it applies
the well-known decoding function to decode the code word,
as if it was the message from the relay. Overall, this procedure
leads to certain computable decoding probability. On the other
hand, note that the procedure is based on Y n only. There has
been well-known upper bound on the decoding probability for
rate exceeding the channel capacity [9]. Thus the decoding
probability obtained by our procedure must be smaller, leading
to the desired bound.

1

symbol ω on top of a random variable solely for the
ˆ
coloring.
We now quote the following known result on decoding
probability [9].

The above idea can be readily generalized to the cases when
XY Z or XZY is statistically degraded.
Based on this new method, we show an explicitly computable bound which is strictly lower than maxp(x) I(X; Y ) +
R0 . This directly improves the result in [12]. As an illustration,
we take the binary erasure channel for detailed derivation. We
also discuss how the method could be applied to cases when
the relay-destination link is no longer lossless.
The rest of the paper is organized as follows. Section II
introduces the basic deﬁnitions, notations and a well-known
bound on decoding probability. Section III generalizes the
blowing up lemma and links to conditional entropy. Section
IV applies it to show an explicit bound for the case when Y
and Z are i.i.d. given X. Section V subsequently generalizes
to the case when the channel is statistically degraded. The
binary erasure channel is analyzed in detail. Finally, Section
VI suggests a way generalizing to cases when the relaydestination link is lossy.

Theorem 1. Suppose that a discrete memoryless channel with
an input alphabet of K letters {a1 , · · · , aK } and an output
alphabet of J letters {b1 , · · · , bJ } is described by transition
probabilities Pjk = p(bj |ak ). Then, for any block length n,
any number of codewords M = 2nR , and any selection of
codes, the probability of decoding satisﬁes
P r(Decoding) ≤ 2−n(−ρR+minp Φ0 (ρ,p)) ,
where p represents a distribution and

J

1/(1+ρ)

pk Pjk
j=1


.

k=1

In the paper, we deﬁne the “best” exponent as
E(R) :=

max (−ρR + min Φ0 (ρ, p))
p

ρ∈[−1, 0)

ON DECODING PROBABILITY

(2)

Remark 1. ([9]) E(R) > 0 for any R > CXY . So the
decoding probability diminishes exponentially when the coding
rate is beyond capacity.

The memoryless relay channel we consider consists of three
nodes, sender X, relay Z and destination Y , deﬁned by the
conditional distribution p(y, z|x). Given X, Y and Z are
independent, i.e., p(y, z|x) = p(y|x)p(z|x). The values of X,
Y and Z are from ﬁnite spaces ΩX , ΩY and ΩZ respectively.
Correspondingly, for a transmission of length n, the code word
xn is chosen from Ωn , deﬁned as the product space of ΩX ,
X
and the received observations are y n ∈ Ωn and z n ∈ Ωn
Y
Z
respectively. The link from Z to Y is a lossless link with rate
R0 . Namely, for a transmission of length n, a number from
{1, 2, · · · , 2nR0 } can be send to Y without error.
A coding strategy of rate R for n channel uses is deﬁned by
n
a 3-tuple (Cn , gn (z n ), fn (zˆ , y n )). Set Cn := {xn (m), m =
nR
1, · · · , 2 } is the code book at the source X. It chooses
one codeword uniformly from the set and transmits to the
channel. Function gn (z n ) is the encoding function at the relay
Z, which is a function mapping an observation z n to a “color”
n
j in {1, 2, · · · , 2nR0 }. In this paper, we also use zˆ to denote
this function, and call the set {1, 2, · · · , 2nR0 } the color set.
n
Function fn (zˆ , y n ) is the decoding function at the destination
Y , mapping the color from the relay and the observation y n
to a code word in Cn . All Cn , gn (·) and fn (·) are well-known
at all nodes.

III. G ENERALIZING THE B LOWING U P L EMMA
The well-known blowing-up lemma [8], [10] states that if
an event A in a product probability space Qn has probability
diminishing slower than exponential, then the event consisting
all points that are within a small Hamming distance of A will
have a probability going to one.
In this section, we generalize to the case when the event
probability may vary arbitrarily. Following almost exactly
Marton’s proof [10] and the summary in El Gamal’s slides
[13], we have
Lemma 1. (Generalized blowing-up Lemma) Suppose Qn ∼
PQn = Πn PQi andPQn (A) ≥ 2−ncn for cn ≥ 0. Then
i=1
for any λ > 1, PQn (Γnλ√cn (A)) ≥ 1 − 1/λ > 0, where
Γl (A) := {xn : minyn ∈A dH (xn , y n ) ≤ l}.
Remark 2. The proof is in the appendix. This lemma is very
similar to the original blowing up lemma. If an event A has
certain probability, then A and its “neighborhood” within
certain distance will have a non-diminishing probability.
A connection to the conditional entropy is shown in the
following (See appendix for proof).

Deﬁnition 1. Rate R is achievable if there exists a sequence
n
of coding strategies of rate R, {(Cn , gn (z n ), fn (zˆ , y n )), n =
1, 2, · · · }, such that the successful decoding probability approaches one as n goes to inﬁnity. That is,
ˆ
limn P r(fn (Z n , Y n ) = X n ) = 1.

ˆ
Theorem 2. Assume that H(Z n |X n ) = nan . Then for all λ >
1, there exists a set of codewords C1 satisfying: i) P r(X n ∈
C1 ) ≥ 1 − 1/λ; ii) For each code word xn in C1 , there is
ˆ
a set of colors S(xn ) ⊆ {1, · · · , 2nR0 } such that P r(Z n ∈
n
n
n
S(x )|X = x ) ≥ 1 − 1/λ. Furthermore, for each j of
S(xn ), we have
ˆ
P r(Z n ∈ Γnλ3/2 √a (Aj )|xn ) ≥ 1 − 1/λ > 0,

We introduce several notations here.

•

(1+ρ)

K

Φ0 (ρ, p) := − log 

II. D EFINITIONS , NOTATIONS AND A WELL - KNOWN BOUND

•

∀ρ ∈ [−1, 0), (1)

CXY and CXZ are the channel capacities from the
channels X-Y and X-Z, respectively.
dH (xn , xn ) denotes the Hamming distance of two points.
1
2
log is with base 2. Also, we reserve the use of the hat

n

n

where Aj := {z ∈

2

Ωn
Z

n
: zˆ = j}.

ˆ
ˆ
ˆ
= P r(f (Z n , Y n ) = X n , ω n = Z n ,

IV. U PPERBOUND WHEN Y AND Z ARE CONDITIONALLY
I.I.D. GIVEN X

ˆ
Y n ∈ Γnλ3/2 √an (Aj )|xn , Z n = j)

In this section, we consider the case when Y and Z are
conditionally i.i.d. given X. Denote ΩY = ΩZ := Ω.

ˆ
ˆ
= P r(f (Z n , Y n ) = X n , Y n ∈ Γnλ3/2 √an (Aj )|xn , Z n = j)
ˆ
ˆ
ˆ
·P r(ω n = Z n |f (Z n , Y n ) = X n ,

Deﬁnition 2. A ball of radius r centered at a point in a space
Ωn is denoted as Ball(r), and is deﬁned as the set of points
which are within Hamming distance r of the point.

ˆ
Y n ∈ Γnλ3/2 √an (Aj ), X n = xn , Z n = j)
ˆ
ˆ
≥ P r(f (Z n , Y n ) = X n , Y n ∈ Γnλ3/2 √an (Aj )|xn , Z n = j)
1
·
,
√
|Ball(nλ3/2 an )|

It is easy to show the following on the volume based on
Stiring’s approximation. The proof is omitted.

where the last inequality is because one picks a point uniRemark 3. For all ρ ∈ [0, 1], we have |Ball(nρ)| ≤
formly within the ball. For the ﬁrst term, we know
n
1
ρn
ρn |Ω| , and n log |Ball(nρ)| ≤ ρ log |Ω| + H2 (ρ) + o(1),
where the o(1) is only a function of n, and H2 (ρ) is the binary P r(f (Z n , Y n ) = X n , Y n ∈ Γ 3/2 √ (Aj )|xn , Z n = j)
ˆ
ˆ
an
nλ
entropy function −ρ log ρ − (1 − ρ) log(1 − ρ).
n
n
= P r(Y ∈ Γnλ3/2 √an (Aj )|x , j)
We now prove the following.
ˆ
−P r(f (Z n , Y n ) = X n , Y n ∈ Γnλ3/2 √a (Aj )|xn , j)
n

Theorem 3. Assume that Y and Z are i.i.d. given X, and ≥ P r(Y n ∈ Γ 3/2 √ (Aj )|xn ) − P r(f (Z n , Y n ) = X n |xn , j),
ˆ
an
nλ
ˆ
H(Z n |X n ) = nan . Also assume that rate R > CXY is
achievable. Then for all λ > 1, there exists δn going to zero, noting that Y n is independent of Z n given X n . Because of
determined by n and λ only, and integer N1 , such that for (3), we thus know
√
1
n ≥ N1 , n log |Ball(nλ3/2 an )| + δn ≥ E(R), where E(R)
ˆ
ˆ
P r(f (ω n , Y n ) = X n |xn , Z n = j)
(5)
is for the XY channel as deﬁned in (2).
1
n
n n
ˆn
Proof: By Theorem 2, for any λ > 1, there exist a set of ≥ p0 − P r(f (Z , Y ) = X |x , j) · |Ball(nλ3/2 √an )| .
code words C1 and constant p0 := 1 − 1/λ > 0 such that: i)
We notice that
P r(X n ∈ C1 ) ≥ p0 ; and ii) for each xn ∈ C1 , there exists a
ˆ
set of colors S(xn ) such that P r(Z n ∈ S(xn )|xn ) ≥ p0 , and
ˆ
P r(X n ∈ C2 , Z n ∈ S(X n ))
for each color j ∈ S(xn ),
n
ˆ
≥ P r(X ∈ C2 ) · P r(Z n ∈ S(X n )|X n ∈ C2 ) ≥ p2 /2.
0
n
n
(3)
P r(Y ∈ Γnλ3/2 √an (Aj )|x ) ≥ p0 ,
Thus, combining with (5), we get
n
n
n
ˆ = j}. Note we use Y n in
where Aj := {z ∈ Ω : z
ˆ
P r(f (ω n , Y n ) = X n )
(3) instead of Z n because Y n and Z n are i.i.d. given X n .
√
ˆ
=
P r(X n = xn , Z n = j)
In other words, in the ball of radius nλ3/2 an around an
xn ,j
independently drawn Y n , with probability at least p0 a point
will have color j, assuming the code word sent is from C1 .
ˆ
ˆ
·P r(f (ω n , Y n ) = X n |X n = xn , Z n = j)
Based on this, the following procedure can be applied to
ˆ
≥
P r(X n = xn , Z n = j)
decode X n based on Y n only. Randomly and uniformly
xn ∈C2 ,j∈S(xn )
ˆ
pick a point ω n in the ball. Assume its color is ω n . Apply
ˆ
ˆ
·P r(f (ω n , Y n ) = X n |X n = xn , Z n = j)
ˆ
the decoding function f (ω n , Y n ) to map to a code word.
1
Announce it to be the codeword sent.
ˆ
≥
·
P r(X n = xn , Z n = j)
√
|Ball(nλ3/2 an )| n
Now we can calculate the decoding probability. By
n)
x ∈C2 ,j∈S(x
assumption, since the code book is feasible, we have
ˆn , Y n ) = X n |xn , j)
ˆ
· p0 − P r(f (Z
P r(f (Z n , Y n ) = X n ) → 0. So there exists an integer N1 > 0
such that at least half of the code words in C1 satisﬁes
1
ˆ
=
· P r(X n ∈ C2 , Z n ∈ S(X n )) · p0
√
n
n
n
n
|Ball(nλ3/2 an )|
ˆ
P r(f (Z n , Y ) = X |X = x ) ≤ p0 , and
ˆ
ˆ
ˆ
−P r(f (Z n , Y n ) = X n , X n ∈ C2 , Z n ∈ S(X n ))
P r(f (Z n , Y n ) = X n ) ≤ p3 /4,
(4)
0
1
ˆ
· p3 /2 − P r(f (Z n , Y n ) = X n )
√
0
|Ball(nλ3/2 an )|
1
≥
· p3 /4,
√
|Ball(nλ3/2 an )| 0

when n > N1 . Denote these code words as C2 .
For an xn ∈ C2 and a j ∈ S(xn ), with n ≥ N1 ,

≥

ˆ
ˆ
P r(f (ω n , Y n ) = X n |xn , Z n = j)
n , Y n ) = X n , ωn = Z n ,
ˆ
ˆ
ˆ
≥ P r(f (ω

where the last inequality is because of (4).
√
1
By (1), we must have n log |Ball(nλ3/2 an )| + δn ≥
E(R), where δn is a function of n and λ.

ˆ
Y n ∈ Γnλ3/2 √an (Aj )|xn , Z n = j)

3

Based on this theorem and applying Remark 3 with ρ :=
√
λ3/2 an , we have

Theorem 5. Suppose that XY Z is statistically degraded.
ˆ
Denote H(Z n |X n ) = nan . Then for all λ > 1, there
exists δn → 0, determined by n and λ only, such that
1
3/2 √
an )| + δn ≥ EY (R), for R > CXY .
n log |BallΩZ (nλ
Here EY (R) is for the XY channel as deﬁned in (2).

Corollary 1. Assume that Y and Z are i.i.d. given X,
ˆ
and H(Z n |X n ) = nan . Then for all λ > 1, there exists
√
(2)
δn = δ (2) (n, λ) → 0 such that E(R) ≤ H2 (λ3/2 an ) +
√
(2)
λ3/2 an log |Ω| + δn , where deﬁne H2 (ρ) = 0 for ρ > 1.

Theorem 6. Under the same condition in Theorem 5, there
exists a ∈ [0, R0 ] such that any achievable rate R larger than
√
CXY satisﬁes: R − CXY ≤ R0 − a and EY (R) ≤ H2 ( a) +
√
a log |ΩZ |.

We now prove the following lemma.
ˆ
Lemma 2. Assume that H(Z n |X n ) = nan . For any achievable rate R, it satisﬁes that R ≤ CXY + R0 − an + o(1).

B. When XZY is Statistically Degraded

Proof: Since the code book is achievable, we have
ˆ
H(X n ) = nR and, by Fano’s lemma, H(X n |Y n , Z n ) =
ˆ
n · o(1). So n(R + o(1)) = I(X n ; Y n , Z n )
ˆ
ˆ
= I(X n ; Y n ) + H(Z n |Y n ) − H(Z n |X n )
ˆ
≤ nCXY + nR0 − H(Z n |X n ).

The upper bound for this case can be derived by considering
the decoding probability when Z tries to decode X n based on
Z n as follows.

(6)

~
Z

Combining the above two gives the following main theorem.

R0

~
Y

Z
R0

Theorem 4. Assume that Y and Z are i.i.d. given X. Then
there exists a ∈ [0, R0 ] such that any achievable rate R larger
than CXY satisﬁes:
√
√
R − CXY ≤ R0 − a and E(R) ≤ H2 ( a) + a log |Ω|.

X
Fig. 2.

ˆ
Proof: Assume that H(Z n |X n )/n = an . From Corollary
1 and Lemma 2, we know R − CXY ≤ R0 − an + o(1)
√
√
and E(n) ≤ H2 (λ3/2 an ) + λ3/2 an log |Ω| + o(1). Suppose
lim sup an = a, which exists because an ∈ [0, R0 ].√
Then R −
√
CXY ≤ R0 − a and E(R) ≤ H2 (λ3/2 a) + λ3/2 a log |Ω|.
Because this is valid for any λ > 1, we know the theorem is
true.
√
√
Noticing Remark 1 and that H2 ( a) + a log |Ω| is continuous in a and is zero at a = 0, the following is obvious.

Y
Augmented Network

We build a new channel based on the relay channel XY Z as
˜
depicted in Figure 2. First, add a new random variable Z which
is independent of others given X and has the same distribution
˜
as Z given X. Then add another random variable Y based on
˜
Z as follows. Whenever Z is received, node Z generates Y
˜ Z.
˜
according to q2 (y|z). Thus we have a new channel XZ Y
˜
˜
Finally add a lossless link of rate R0 from Z to Y . Since
˜ Z and XY Z are equivalent statistically, any
˜
the channels X Y
rate achievable by the XY Z channel must be achievable by
˜˜
˜
the channel XZ Y Z. More speciﬁcally, given observation Z n ,
ˆ n based on the same mapping from
˜
˜
node Z maps to a color Z
ˆ
Z n to Z n . For any feasible code rate R, node Z invokes the
ˆ ˜
˜
associated decoding function f (Z n , Y n ) to decode X n .
˜ Z. Now node Z can guess
˜
Consider the channel XZ Y
X n based on Z n only by the following procedure. Assume
ˆ
˜
H(Z n |X n ) = nan , and ﬁx a constant λ > 1. Node Z draws
√
˜
a ball of radius nλ3/2 an around Z n . Because Z and Z
are i.i.d. given X, as shown in the proof for Theorem 4,
ˆ
˜
the color Z n is contained in the ball with non-diminishing
probability. Randomly pick a point ω n in the ball, node Z
ˆ ˜
announces f (ω n , Y n ) as the code word. By similar argument,
the following is true.

Corollary 2. The maximum achievable rate is strictly less
than CXY + R0 .
V. W HEN THE C HANNEL IS S TATISTICALLY D EGRADED
In this section, we consider the case when the channel
is statistically degraded. We say XY Z is degraded if there
exists a transition probability distribution q1 (z|y) such that
p(z|x) =
y q1 (z|y)p(y|x). Similarly, XZY is degraded
if there exists a probability distribution q2 (y|z) such that
p(y|x) = z q2 (y|z)p(z|x). Note that [12] considers the case
when Z is better than Y .
A. When XY Z is Statistically Degraded
The following procedure can be employed by Y to decode X n based on observation Y n only. For any received
˜
observation Y , it generates a random variable Z based on the
transition probability q1 (z|y); thus for the observed Y n , an
˜
Z n is generated. Now consider the relay channel formed by
˜
˜
X, Z, and Z. It is obvious that Z and Z are i.i.d. given X.
˜
The same procedure in Section IV, namely the method for Z
to guess X n and the derivation on the decoding probability,
can be applied. This leads to the following.

Theorem 7. Assume XZY is statistically degraded. Denote
ˆ
H(Z n |X n ) = nan . Then for all λ > 1, there exists δn going
to zero, determined by n and λ only, such that
√
1
log |BallΩZ (nλ3/2 an )| + δn ≥ EZ (R).
n
Theorem 8. Assume the same condition in Theorem 7. Then
there exists a ∈ [0, R0 ] such that any achievable rate R larger

4

than√CXZ √
satisﬁes: R − CXY ≤ R0 − a and EZ (R) ≤
H2 ( a) + a log |ΩZ |.

[10] K. Marton, “A simple proof of the Blowing-Up Lemma,” IEEE Trans.
on Information Theory, IT-32, pp. 445-446, 1986.
[11] D. Omstein, Ergodic Theory, Randomness and Dynamical Systems, Yale
Univ. Press: New Haven, 1974.
[12] Z. Zhang, “Partial converse for a relay channel,” IEEE Trans. Info.
Theory, Vol. 34, No. 5, pp. 1106-1110, September 1988.
[13] A. El Gamal, Talk on Katalin Marton’s work at the third
WITHITS Annual Event at ISIT 2010. http://isl.stanford.edu/ abbas/presentations/Marton.pdf

We now take an example for detailed derivation.
Example: Binary Erasure Channel. Suppose both XY
and XZ are conditionally i.i.d. binary erasure channels with
erasure probability . We determine E(R). For input distribution such that P r(X = 0) = p, by deﬁnition
Φ0 (ρ, p)

= − log (p(1+ρ) + (1 − p)(1+ρ) )(1 − ) +

A PPENDIX

.

A. Proof for Lemma 1

One can show that

The proof applies the following lemma from [10]. Recall
that the KL-divergence between two distributions P1 and P2
is deﬁned as D(P1 ||P2 ) := i P1 (i) log(P1 (i)/P2 (i)).
n
Lemma 1 of [10]: Let Qn ∼
i=1 PQi , i.e. Qi ’s are
¯ n ∼ PQn . Then, there exists a joint
independent, and Q
¯
probability measure PQn ,Qn with these given marginals such
¯
1/2
n
1
1
n ¯n
.
that n E(dH (Q , Q )) ≤ n D PQn || i=1 PQi
¯

x

E(R) = max (Rx − log[2 (1 − ) + ]) =
x∈(0,1]
R
(1− )(1−R)

R
R log
− log[ 1−R + ], R ∈ (1 − , 1 −
R − log(2 − ), R ≥ 1 − 2− .

2−

);

Thus one can calculate the bound numerically based on
Theorem 4. It is nevertheless very close to the cut-set bound.

Now deﬁne PQn (xn ) as PQn (xn ) = PQn |A (xn ) =
¯
¯
PQn (xn )/PQn (A), ∀xn ∈ A, and PQn (xn ) = 0, otherwise.
¯
n
Then, D(PQn || i=1 PQi ) = − log PQn (A) ≤ ncn .
¯
By the lemma, we know there exists a joint distribution such
√
¯
that EdH (Qn , Qn ) ≤ n cn . By the Markov inequality,
√
cn
¯
PQn ,Qn {dH (Qn , Qn ) ≤ nδn } ≥ 1 −
.
¯
δn
√
If choose δn = λ cn with λ > 1, we therefore have

VI. F URTHER D ISCUSSIONS
The same idea for upper-bounding the capacity could be
generalized to more general cases, where the relay-destination
link may not be lossless. For example, suppose the relay
channel is now represented by four random variables X, Y ,
X1 and Z with joint distribution p(x, x1 , y, z), such that:
n
n
• The relay observes Z and sends out X1 ;
• The relay’s observation Z is independent of Y and X1
given X, i.e. p(z|x, y, x1 ) = p(z|x);
• Y
is independent of Z given X and X1 , i.e.,
p(y|x, x1 , z) = p(y|x, x1 ).
To upper-bound the capacity using the tools developed, one
can replace the X1 Y channel by a link with deterministic rate
R0 = min{log |ΩX1 |, log |ΩY |}. This new relay channel has
greater capacity because any encoding/decoding process for
the original one can be considered as a special case. If the
cardinality of the space ΩX1 or ΩY is small, one can achieve
a upper bound better than the cut-set bound. One extreme case
is when: 1) Y = (Y1 , Y2 ), and 2) X1 Y1 represents the relaydestination link independent of all other random variables.

PQn (Γnδn (A))
= PQn ,Qn (Γnδn (A) × A) + PQn ,Qn (Γnδn (A) × Ac )
¯
¯
= PQn ,Qn (Γnδn (A) × A)
¯
¯
= PQn ,Qn (dH (Qn , Qn ) ≤ nδn ) ≥ 1 − 1/λ.
¯

(7)
(8)

where (7) and (8) follow from the fact that PQn ,Qn (xn , xn ) =
¯
¯
0 when xn ∈ A.
¯
B. Proof for Theorem 2
We present two lemmas ﬁrst. Due to page limit, we omit
ˆ
the proofs. Recall that Z n is the “coloring” function on Z n .
ˆ
Lemma 3. Suppose H(Z n |X n ) = nan . Then for all µ > 1,
ˆ
P r(X n ∈ {xn : H(Z n |X n = xn ) ≤ nµan }) ≥ 1 − 1/µ > 0.

R EFERENCES
[1] E. C. Van der Meulen, “Three-terminal communication channel,” Adr.
Appl. Proh.. vol. 3, pp. 120-154, 1971.
[2] A. El Gamal, “Coding for noisy networks,” plenary talk at the 2010
IEEE International Symposium on Information Theory, Austin, USA
[3] T.M. Cover and A. El Gamal, “Capacity theorems for the relay channel,”
IEEE Trans. Inform. Theory, vol. 25, no. 5, pp. 572-584, Sept. 1979.
[4] T.M. Cover and J.S. Thomas, Elements of Information Theory. New
York: Wiley, 1991.
[5] A. El Gamal and M. Aref, “The capacity of the semideterministic relay
channel,” IEEE Trans. Inf. Theory, vol. IT-28, no. 3, p. 536, May 1982.
[6] Y. H. Kim, “Capacity of a class of deterministic relay channels,” in
IEEE Trans. on Information Theory, 2008.
[7] M. Aleksic, P. Razaghi, and W. Yu, “Capacity of a class of modulo-sum
relay channels,” IEEE Trans. Inf. Theory, vol. 55, pp. 921-930, 2009.
[8] R. Ahlswede, P. Gacs, and J. Korner, “Bounds on conditional probabilities with application in multi-user communication,”, Z. Wuhrschernlrchkertstheorie uenv. Gehiete, vol. 34, pp. 157-177, 1976.
[9] S. Arimoto, “On the converse to the coding theorem for discrete
memoryless channels,” IEEE Trans. Info. Theory, Vol. 19, No. 3, pp.357359, May 1973

2nR0

Lemma 4. If j=1 −pj log pj ≤ nbn , then for any α > 1,
those colors S := {j : pj ≥ 2−nbn α } have a total probability
no less than 1 − 1/α.
Now, by Lemma 3, we know that for any µ > 1,
ˆ
P r({xn : H(Z n |xn ) ≤ nµan }) ≥ 1 − 1/µ.
ˆ
Deﬁne C1 := {xn : H(Z n |xn ) ≤ nµan }. For each xn in
2nR0
C1 , by deﬁnition, we have k=1 −pk log pk ≤ nµan , where
ˆ
pk := P r(Z n = k|xn ). Then by Lemma 4, we know for any
ˆ
α > 1 there exists a set of colors S such that: 1) P r(Z n ∈
S|xn ) ≥ 1 − 1/α, and 2) for each color j ∈ S, pj ≥ 2−nµan α .
For such an xn and color j, by Lemma 1, we know for any
ˆ
λ > 1, P r(Z n ∈ Γnλ√µan α (Aj )|X n = xn ) ≥ 1 − 1/λ.
Now the theorem is proved by letting µ = α = λ.

5


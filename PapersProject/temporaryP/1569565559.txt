Creator:        TeX
Producer:       Appligent StampPDF Batch, version 5.1
CreationDate:   Mon May  7 14:47:32 2012
ModDate:        Tue Jun 19 12:55:02 2012
Tagged:         no
Pages:          5
Encrypted:      no
Page size:      612 x 792 pts (letter)
File size:      463980 bytes
Optimized:      no
PDF version:    1.6
ISIT'2012 1569565559

Compressive Principal Component Pursuit
John Wright∗

Arvind Ganesh†

Kerui Min†

Yi Ma†‡

∗ Dept.

of Electrical Engineering † Dept. of Electrical and Computer Engineering
Columbia University
University of Illinois at Urbana-Champaign

where Q ⊆ Rm×n is a linear subspace, and PQ is the projection operator onto Q. The natural convex program becomes
min L

In recent years, there has been tremendous interest in
recovering low-dimensional structure in high-dimensional signal or data spaces. This interest has been fueled by the
striking discovery that efﬁcient techniques based on convex
programming can accurately recover low-complexity signals
such as sparse vectors or low-rank matrices from severely
compressive, incomplete, or even corrupted observations.
One representative example arises in Robust Principal Component Analysis (RPCA). There, the goal is to recover a lowrank matrix L0 ∈ Rm×n from grossly corrupted observations.
For example, suppose we observe M = L0 + S 0 , where
S 0 ∈ Rm×n is a sparse error. Under mild conditions, the following convex program, called Principal Component Pursuit
(PCP) [1], [2]:
+λ S

1

s.t. L + S = M ,

∗

+λ S

1

s.t. PQ [L + S] = D.

(3)

Following [1], in this paper we refer to this convex program
as Compressive Principal Component Pursuit (CPCP).
The adjective compressive is suggestive of one application
of this tool. The low-rank and sparse model captures properties
of many signals of interest, including videos [1], [5], [6],
structured textures [7], hyperspectral datacubes [8], [9] and
more. The ability to recover low-rank and sparse models
from small sets of measurements D could be very useful
for developing new sensing architectures for such signals [8],
[10]. CPCP (3) also arises in other computational problems,
including transformation-invariant low-rank recovery [7], [5].
The fundamental question is whether we can simultaneously
recover the low-rank and sparse components from highly
compressive measurements via CPCP? While this question is
largely open, there is good reason to believe the answer may
be positive. For example, [1], [11] have studied the “robust
matrix completion” problem, with PQ = PΩ , where Ω is a
small subset of the entries of the matrix. When PQ = PΩ ,
it is impossible to exactly recover S 0 (many of the entries
are simply not observed!), but the low-rank term L0 can be
recovered from near-minimal sets of samples [11]. However, in
many applications the sparse term S 0 is actually the quantity
of interest: for example, in visual surveillance, S 0 might
capture moving foreground objects. To recover both L0 and
S 0 , we must require measurements Q that are incoherent with
both the low-rank and the sparse term.
In this paper, we investigate the performance of (3) when Q
is a randomly chosen subspace. A similar recovery problem
was recently considered by [8], with the goal of designing
sensing strategies capable of recovering both L0 and S 0 . We
will discuss the results of [8] and other related works in more
detail in Section III, after we have stated our main results.
Our results will show that if the number of measurements
q is large enough, (3) will correctly recover L0 and S 0 with
high probability. Clearly, to ensure a unique solution, this q
should at least as large as the number of intrinsic degrees of
freedom in (L0 , S 0 ). Since a rank r matrix has (m + n −
r)r degrees of freedom, the number of continuous degrees of

I. I NTRODUCTION

∗

Computing Group
Microsoft Research Asia

could be extravagantly large compared to the number degrees
of freedom in the unknowns L0 , S 0 . Is it possible to recover
L0 and S 0 from smaller sets of linear measurements?
In this work, we consider more general observations
.
D = PQ [M ] = PQ [L0 + S 0 ],
(2)

Abstract—We consider the problem of recovering a target
matrix that is a superposition of low-rank and sparse components, from a small set of linear measurements. This problem
arises in compressed sensing of structured high-dimensional
signals such as videos and hyperspectral images, as well as
in the analysis of transformation invariant low-rank recovery.
We analyze the performance of the natural convex heuristic for
solving this problem, under the assumption that measurements
are chosen uniformly at random. We prove that this heuristic
exactly recovers low-rank and sparse terms, provided the number
of observations exceeds the number of intrinsic degrees of
freedom of the component signals by a polylogarithmic factor.
Our analysis introduces several ideas that may be of independent
interest for the more general problem of compressive sensing of
superpositions of structured signals.1

min L

‡ Visual

(1)

precisely recovers L0 and S 0 . In (1), · ∗ is the matrix nuclear
norm (sum of singular values) and · 1 is the 1 norm (sum of
magnitudes). For data analysis applications, this suggests that
a low-rank matrix L0 can be recovered from the observation
M despite large-magnitude sparse errors.
The conditions under which recovery is known to occur
are broad: provided the low-rank term satisﬁes a technical
incoherence condition, correct recovery can occur even when
rank(L0 ) is almost proportional n, and the number of nonzero
entries in S 0 is proportional to mn [1]. On the other hand,
in many applications of interest, the rank may actually be
signiﬁcantly smaller than dimension (say 3 [3], or 9 [4]).
Moreover, cardinality of the sparse term may also be quite
small. In such a situation our number mn of observations
1 This work was partially supported by the funding of ONR N00014-09-10230, NSF CCF 09-64215, NSF IIS 11-16012, and DARPA KECoM 10036100471. JW was also supported by startup funding from Columbia University.

1

freedom in the pair (L0 , S 0 ) is (m + n − r)r + S 0 0 , where
· 0 is the number of nonzero entries in a matrix. We will
show that when the measurements are Gaussian, (L0 , S 0 ) can
be exactly recovered from a number of measurements that is
merely within an O(log2 m) factor of this lower bound.
Our analysis actually pertains to a much more general class
of problems of decomposing a given observation into multiple
incoherent components:
min

λi X i

(i)

s.t.

i

Xi = M .

distributed according to the Haar measure on the Grassmannian G(Rm×n , q). This means that Q is equal in distribution
to the linear span of a collection of q independent iid N (0, 1)
matrices. Under this setting, the following theorem gives a
tight bound on the number of (random) measurements required
to correctly recover the pair (L0 , S 0 ) from PQ [M ] via CPCP:
Theorem II.1 (Compressive PCP Recovery). Let L0 , S 0 ∈
Rm×n , with m ≥ n, and suppose that L0 = 0 is a rank-r, µincoherent matrix with r ≤ cr n(µ log2 m)−1 , and sign (S 0 )
is iid Bernoulli-Rademacher with nonzero probability ρ < cρ .
Let Q ⊂ Rm×n be a random subspace of dimension

(4)

i

Here, · (i) are (decomposable) norms that encourage various types of low-complexity structure. Principal Component
Pursuit [1], [2], Outlier Pursuit [12], [13] and Morphological
Component Analysis [14] are all special cases of this general
problem. Our analysis will suggest that if (4) succeeds in
recovering all the components {X i } from M , one should
also expect to recover them from the highly compressive
measurements PQ [M ]. The number of measurements required
is again governed by the intrinsic degrees of freedom {X i }
times at most a polylog(m) factor. Thus, we believe the results
in this paper will be applicable to a broad class of source
separation or signal decomposition problems that may arise in
signal processing, communications, and pattern recognition.

dim(Q) ≥ CQ · (ρmn + mr) · log2 m

distributed according to the Haar measure, probabilistically
independent of sign(S 0 ). Then with probability at least √ −
1
Cm−9 in (sign(S 0 ), Q), the solution to (3) with λ = 1/ m
is unique, and equal to (L0 , S 0 ). Above, cr , cρ , CQ , C are
positive numerical constants.
Here, the magnitudes of the nonzeros in S 0 are arbitrary,
and no randomness is assumed in L0 . The randomness is in
the sign and support pattern of S 0 and the measurements Q.
We note in passing that the randomness in the signs of S 0 can
be removed using the techniques of [1] Sections 2.1-2.2.
We also note that the bounds on r and ρ essentially match
those of [1], possibly with different constants. So, again, r
and S 0 0 can be rather large. On the other hand, when these
quantities are small, the bound on dim(Q) ensures that the
number of measurements needed for accurate recovery is also
commensurately small. We will compare our results to other
works from the literature in the next section.

II. M ODELS AND M AIN R ESULTS
We ﬁrst recall conditions under which M = L0 +S 0 can be
exactly separated into its constituents by PCP. Intuitively, we
should not expect to recover all possible low-rank pairs and
sparse pairs (L0 , S 0 ). Indeed, imagine the case when M is
rank-one and one-sparse (i.e., M = ei e∗ for some i, j). In this
j
situation the answers (L = ei e∗ , S = 0) and (L = 0, S =
j
ei e∗ ) both seem reasonable – the problem is ambiguous!
j
To make the problem meaningful, we need conditions that
ensure that (i) the low-rank term L0 does not “look sparse”
and (ii) the sparse term S 0 does not “look low-rank.” One
popular way formalizing the ﬁrst intuition of doing this is
via the notion of incoherence introduced by [15]. If the lowrank matrix L0 has rank-reduced singular value decomposition
L0 = U ΣV ∗ , then we say that L0 is µ-incoherent if
∀ i, U ∗ ei
UV

2
2

∗
∞

≤ µr/m,
≤

∀ j, V ∗ ej

µr/mn.

2
2

(7)

III. R ELATIONSHIP TO THE L ITERATURE
As mentioned above, in recent years there has been a large
amount of work on matrix recovery and decomposition, for
example see [1], [2], [16], [17], [12], [13], [18], [19] and
references therein. The aforementioned works all pertain to
the case when the matrix M is fully observed, and hence
are not directly comparable to our result. Our analysis relies
on a tool for transforming a certiﬁcate of optimality for the
fully-observed problem into a certiﬁcate of optimality for the
compressive problem, which might potentially also be applied
in conjunction with the above analyses.
Compared to the fully observed problem, there is much
less dedicated work on low-rank and sparse recovery from
compressive measurements. Recently, motivated by applications in compressive foreground and background separation
and compressive hyperspectral image acquisition, Waters et.
al. [8] introduced a greedy algorithm for this problem, which
is similar in spirit to the CoSaMP algorithm [20], and performs
well on numerical examples. Analyzing its behavior and
proving performance guarantees is currently an open problem.
As the body of results on speciﬁc problems such as matrix
recovery grows, there has been an increasing interest in unifying or generalizing the insights obtained. For example, Negahban et. al. [21] give a geometric framework for analyzing

≤ µr/n, (5)
(6)

These conditions ensure that the singular vectors of L0 are
not too concentrated on only a few coordinates [15].
At the same time, we need to ensure that the sparse term
does not “look low-rank.” One appealing way of doing this is
via a random model: we assume that each (i, j) is an element
of supp (S 0 ) independently with probability ρ bounded by
some small constant. We assume that the signs of the nonzero
entries are independent symmetric ±1 random variables (i.e.,
Rademacher random variables). In stating our theorems, we
call such a distribution an “iid Bernoulli-Rademacher model.”
We will give a result for the case when the measurement
subspace Q is chosen uniformly at random from the set of
all q-dimensional subspaces of Rm×n . More precisely, Q is

2

Our result pertains to decomposable norms · (i) [21], [23].
This notion includes sparsity inducing norms such as the 1
norm and nuclear norm, as well as sums of block p norms.

low-complexity signal recovery. Agarwal et. al. [18] use this
framework to analyze sparse and low-rank recovery, obtaining
near-optimal rates for estimation in noise. [18] proceed under
weaker assumptions, which preclude exact recovery.
Chandrasekaran et. al. [22] have recently produced a very
general analysis of structured signal recovery with Gaussian
measurements. That work exploits the geometry of the norm
ball, relating the required number of measurements to the
Gaussian width of the tangent cone at the desired solution.
For our problem, the non-trivial analysis in [1], [2] can be
viewed as simply showing that the desired solution lies on the
boundary of the norm ball. Estimating the width of the tangent
cone seems to entail additional difﬁculty.
For Gaussian measurements, Cand` s and Recht [23] also
e
give simple bounds for exact recovery, under the assumption
that the regularizer (or norm) is decomposable. To apply
similar analysis to our problem, we would need to work with
.
the quotient norm on M : M = inf L+S=M L ∗ +λ S 1 .
This is the inﬁmal convolution of two decomposable norms.
Its subdifferential has a number of nice properties, but decomposability does not appear to be one of them. Nevertheless, our
results suggest we should expect the same type of compressive sensing results for this class of generalized norms for
superpositions of low-complexity components.

Deﬁnition IV.1. We say that a norm · is decomposable at
X if there exists a subspace T and a matrix S such that
∂ · (X) = { Λ | PT Λ = S, PT ⊥ Λ
∗

where ·
denotes the dual norm of
nonexpansive with respect to · ∗ .

λi X i

(i)

s.t.

Xi = M .

(8)

Deﬁnition IV.3. We call Λ an (α, β)-inexact certiﬁcate for
a putative solution (X 1, , . . . , X τ, ) to (9) with parameters
(λ1 , . . . , λτ ) if for each i, PTi Λ − λi S i F ≤ α, and
PTi⊥ Λ ∗ < λi β.
(i)

(9)

i

The goal of this paper is not to study (9) per se, but rather
to understand what happens to it when we only observe the
projection of M onto a much lower dimensional subspace:
min

λi X i
i

(i)

s.t. PQ

X i = PQ M .

· , and PT ⊥ is

This condition implies that Λ lies in the subdifferential of
λi · (i) for each i. If we take Q = Rm×n in Lemma IV.2,
we obtain a sufﬁcient optimality condition for the original
decomposition problem (9). The condition given by Lemma
IV.2 is not so convenient, because it demands that Λ exactly
satisﬁes a set of equality constraints PTi Λ = λi S i . One very
useful device, due to Gross [25], is to trade off between the
equality constraints and the dual norm inequality constraints
PTi⊥ Λ ∗ < λi , tightening the latter while loosening the
(i)
former. The following deﬁnition gives this idea a name:

where each X i satisﬁes a low-complexity model such as
sparsity or rank-deﬁciency, possibly also including more exotic
types of structured sparsity [24]. For each type of structure, we
have a corresponding regularizer · (i) . The natural convex
heuristic for decomposing M into its components would solve
i

(11)

Lemma IV.2. Consider a feasible solution x
=
(X 1, , . . . , X τ, ) to (10). Suppose that each of the norms
· (i) is decomposable at X i, . If T1 , . . . , Tτ , Q⊥ are independent subspaces and there exists Λ satisfying PTi Λ = λi S i
and PTi⊥ Λ ∗ < λi for each i, and PQ⊥ Λ = 0, then x is
(i)
the unique optimal solution to (10).

In this section, we present the technical result used to
obtain Theorem II.1 above. As promised, this result will have
implications for compressive variants of a large number of
conceivable signal decomposition problems. Suppose that the
fully observed data M are given as a sum of structured terms:

min

≤ 1} ,

For example, the 1 norm satisﬁes this deﬁnition with T =
supp (X) and S = sign (X). This deﬁnition is completely
equivalent to that of [23]. It is also related to the deﬁnition
of [21], but not strictly equivalent to it. We assume that each
· (i) is decomposable at the target solution X i, , so per
the above deﬁnition we have a sequence of subspaces Ti and
matrices S i that deﬁne the subdifferentials of each of the regularizers · (i) . We will say that the subspaces T1 , . . . , Tτ are
independent if dim(T1 +· · ·+Tτ ) = dim(T1 )+· · ·+dim(Tτ ),
and state a simple sufﬁcient optimality condition for (10):

IV. G ENERAL C ERTIFICATE U PGRADES

M = X1 + X2 + · · · + Xτ ,

∗

Comparing to Lemma IV.2, we can see that this deﬁnition
is most meaningful when α is small, and β ≤ 1. Deﬁnition
IV.3 pertains to the decomposition problem (9), and does not
involve the measurement operator Q in any way. Adding one
more constraint, PQ⊥ Λ = 0, we obtain an inexact certiﬁcate
for the compressive problem (10):

(10)

i

Suppose we know that (9) correctly decomposes M into
X 1 , . . . , X τ . Does this imply that (10) can also recover
X 1 , . . . , X τ ? Theorem IV.6 below will imply that this is
true under broad circumstances. Provided we have proved
optimality for (9), we can move to optimality for (10), as long
as the number of measurements dim(Q) is sufﬁciently large. In
this sense, our analysis is modular: any technique can be used
to perform the analysis of the original decomposition problem,
provided it constructs an (approximate) dual certiﬁcate.

Deﬁnition IV.4. We call Λ an (α, β)-inexact certiﬁcate for
a putative solution (X 1, , . . . , X τ, ) to (10) with parameters
(λ1 , . . . , λτ ) if
(i) Λ is an (α, β) inexact certiﬁcate for (9), and
(ii) PQ⊥ Λ = 0.
As we will see, an inexact certiﬁcate is easier to produce
than the “exact” Λ demanded in the optimality condition

3

Lemma IV.2. Is it still sufﬁcient to certify optimality? The
answer is yes, provided α and β are small enough:

w.p. ≥ 1 − C2 · τ · m−9 in Q. Above, CQ , C1 and C2 are
positive numerical constants.

Lemma IV.5. Consider a feasible solution x
=
(X 1, , . . . , X τ, ) to the optimization problem (10). Suppose
that each of the norms · (i) is decomposable at X i, , and
that each of the · (i) majorizes the Frobenius norm. Then
if T1 , . . . , Tτ , Q⊥ are independent subspaces with
PTi PTj < (τ − 1)−1 ∀ i = j,
(12)
ˆ with
and there exists an (α, β)-inexact certiﬁcate Λ,
√
α τ (1 − PQ⊥ PT1 +···+Tτ 2 )−1
1
×
< 1, (13)
β+
minl λl
1 − (τ − 1) maxij PTi PTj

In the next two sections, we ﬁrst sketch how Theorem IV.6
implies Theorem II.1, and then sketch a proof of Theorem
IV.6. Complete proofs are given in the full version [26].
V. P ROOF S KETCH : T HEOREM II.1
Proof of Theorem II.1: From Lemma IV.5, to show
(L0 , S 0 ) is the unique solution to (3), it is enough to show
that (i) PT PΩ ≤ 1/2, (ii) there exists an (α , 1/2)-inexact
CPCP certiﬁcate ΛCPCP with
√
(18)
α < 1 − PQ⊥ PT ⊕Ω 2 /4 m.
A covering argument shows:

then x is the unique optimal solution.

Lemma V.1. If S ⊆ Rm×n is a ﬁxed subspace, A =
γ
j=1 H j H j , · , with (H j ) a sequence of independent i.i.d.
N (0, 1/mn) matrices, and R = range(A) ⊆ Rm×n , then if
γ ≥ C1 · dim(S), w.p., ≥ 1 − C2 exp (−cγ),

The technical condition that · (i) majorizes the Frobenius
norm (i.e., for all X, X (i) ≥ X F ) is immediately
satisﬁed by sparsity inducing norms such as the nuclear and
1
norms. In any case, it can always be ensured by rescaling.
Thus, to show that X 1 , . . . , X τ solve the compressive
decomposition problem (10), we just have to produce an
inexact certiﬁcate Λ following the speciﬁcation of Deﬁnition
IV.4 with (α, β) sufﬁciently small. This is fortuitous, since
many existing analyses of the original decomposition problem
(9) already give certiﬁcates for that problem. To prove that the
desired solution remains optimal even when we only see a few
measurements Q, we will show that a certiﬁcate for (9) can be
“upgraded” to a certiﬁcate for (10), with very high probability
in the random Q, and only a small loss in the parameters
(α, β). As it turns out, the loss in the dual norm · ∗ depends
(i)
on the expected dual norm of a standard Gaussian matrix:
.
νi = E G ∗ ,
G ∼iid N (0, 1) .
(14)
(i)

PS mn APS − PS ≤
γ

and PS mn PR PS − PS ≤
γ

1
16 .

We can use this to show that with high probability, 1 −
15
1
PQ⊥ PT ⊕Ω 2 ≥ 16 dim(Q) ≥ m . So it is enough to show an
mn
(α , 1/2) CPCP certiﬁcate with α < 1/4m3/2 . We construct
this by taking an inexact PCP certiﬁcate from [1], and then
applying Theorem IV.6 to upgrade it to a CPCP certiﬁcate.
Proposition V.2 (Dual Certiﬁcation for PCP [1]2 ). Under the conditions of Theorem II.1, w.p. ≥ 1 − Cm−10 ,
(i) PΩ PT ≤ 1/2, and (ii) there exists a (m−2 , 1/4)inexact PCP certiﬁcate ΛPCP for (L0 , S 0 ), which satisﬁes
ΛPCP F ≤ 4 rank(L0 ) + 2λ
S0 0.
After applying Theorem IV.6, we obtain ΛCPCP . Since
√
ΛPCP F < 6 n, (16) gives α ≤ m−2 + 6m−5/2 . Our
condition α < 1/4m3/2 is satiaﬁed. Furthermore, with high
probability√ ΛCPCP 2 ≤ C(r+ρn). The expected dual norms
F
√
are ν ≤ C m for · ∗ and ν ≤ C log m for · 1 . Hence,

Of course, upgrading should only be possible if the number
of measurements is sufﬁcient. Interestingly, however, the number of measurements does not need to be too much larger than
the number of degrees of freedom in x . More precisely, our
theorem will refer to the quantity dim(T1 + · · · + Tτ ). Indeed,
for the 1 norm, dim(Ti ) is the number of nonzero entries
in the solution X i . For the nuclear norm, one can check that
dim(Ti ) is the number of degrees of freedom a matrix whose
rank is equal to that of X i :

β ≤ β + C (mr + ρmn) log2 m/dim(Q)

1/2

.

Under the stated condition on dim(Q), β ≤ 1/2, and so
Lemma IV.5 shows that (L0 , S 0 ) are optimal for CPCP.
VI. P ROOF S KETCH : T HEOREM IV.6
ˆ
Proof of Theorem IV.6: Let S = T1 +· · ·+Tτ +span(Λ).
ˆ
Our goal is to generate Λ that is close to Λ on S, and s.t.,

Theorem IV.6 (Certiﬁcate Upgrade). Consider the general
decomposition problem (9), and suppose that each of the
norms
· (i) majorizes the Frobenius norm. Let x =
(X 1, , . . . , X τ, ) be feasible for (9), and suppose there exists
ˆ
an (α, β)-inexact certiﬁcate Λ for x for (9).
Then if Q ⊂ Rm×n is a Haar random subspace, with
dim(Q) ≥ CQ · dim(T1 + · · · + Tτ ) · log m,

1
2

PQ⊥ Λ = 0.

(19)

Set Λ0 = 0. Generate inductively a sequence (Λj )j=1,...,k for
appropriate k, such that with high probability Λ = Λk is the
desired certiﬁcate. Deﬁne the error at step j to be
ˆ
E j = PS [Λj ] − Λ ∈ S.
(20)

(15)

By orthogonal invariance, Q is equal in distribution to
the linear span of H 1 , . . . , H dim(Q) , where H j are independent iid N (0, 1/mn) random matrices. Choose from
{1, . . . , dim(Q)}, k = 3 log2 m disjoint subsets I1 , . . . , Ik

there exists an (α , β )-inexact certiﬁcate for x for the
compressive decomposition problem (10) with
ˆ
α ≤ α + m−3 Λ F ,
(16)
1/2
√
ˆ
Λ 2 log m
νi + log m
F
(17)
,
β ≤ β + C1 max
i
λi
dim(Q)

2 This proposition follows from the construction of [1]. An extra calculation,
given in the long version of this work, is needed to bound ΛPCP F .

4

of size γ = dim(Q)/k . Then 2−k ≤ m−3 . We will require
that γ ≥ C3 · dim(S). Since

λ−1
i

dim(Q) ≥ CQ · dim(T1 + · · · + Tτ ) · log(m),

For j = 1, . . . , k, let
j

= Λj−1 −

Λj

mn
γ Aj E j−1

mn
γ Ei

−

=

− 1.

i=1

Then by construction PQ⊥ Λj = 0, and
ˆ
= PS [Λj ] − Λ

Ej

PS (I −

=

mn
γ Aj )PS E j−1 .

Applying Lemma V.1, we can show that for each j, E j
ˆ
E j−1 F /2. Using E 0 = −Λ, this further gives

F

≤

k

Ek

F

ˆ
≤ 2−k Λ

Ej

and

F

F

ˆ
≤2 Λ

F.

(22)

j=0

It is left to calculate α and β . Write
Λk

= PS [Λk ] + PS ⊥ [Λk ],
k

ˆ
= Λ + Ek −

PS ⊥
j=1

mn
Aj PS E j−1 .
γ

From the deﬁnition, set α = maxi PTi Λk − λi S i
gives (19), via:
PTi Λk − λi S i

F

≤

λ−1
i

Λk

k
∗
(i)

ˆ
≤ Λ

∗
(i)

+ Ek

F.

i

We can take β = maxi=1,...,τ
the triangle inequality, we have
Λk

(23)

ˆ
PTi [Λ + E k ] − λi S i F ,
ˆ
ˆ
PT Λ − λi S i F + m−3 Λ

=

F

PS ⊥

+
j=1

∗
(i) .

This

F.

From (23) and

mn
Aj PS E j−1
γ

∗
(i) ,

Applying the following lemma controls the dual norm:
Lemma VI.1. Let S be any ﬁxed subspace, and M any ﬁxed
γ
matrix. Let A = l=1 H l H l , · be a random semideﬁnite
operator constructed from a sequence of independent iid
N (0, 1/mn) matrices (H l ), · any norm that majorizes the
Frobenius norm, and · ∗ its dual norm. Set ν = E [ G ∗ ],
with G iid N (0, 1). Then we have
√
mn
ν + log m
PS ⊥
APS M ∗ ≤ 10 PS M F
, (24)
√
γ
γ
with probability at least 1 − m−10 − exp − γ .
2
So, we have
Λk

∗
(i)

≤ λi β + 2−k E 0

and using γ ≥

F

+ 10

νi +

√
log m
√
γ

√

log m

λi

ˆ
Λ 2 log m
F
dim(Q)

1/2

.

R EFERENCES

i∈Ij

Notice that E [Aj ] =

≤ β + C5

νi +

This completes the proof of Theorem IV.6.

this is satisﬁed. Let Aj : Rm×n → Rm×n denote the operator
that acts via
H i H i, · .
(21)
Aj [·] =
γ
mn I.

∗
Λk (i)

k

E j−1

F

,

j=1

c·dim(Q)
log m ,

5

[1] E. Cand` s, X. Li, Y. Ma, and J. Wright, “Robust principal component
e
analysis?” Journal of the ACM, vol. 58, no. 7, May 2011.
[2] V. Chandrasekaran, S. Sanghavi, P. Parillo, and A. Wilsky, “Ranksparsity incoherence for matrix decomposition,” SIAM Journal on Optimization, vol. 21, no. 2, pp. 572–596, 2011.
[3] L. Wu, A. Ganesh, B. Shi, Y. Matsushita, Y. Wang, and Y. Ma, “Robust
photometric stereo via low-rank matrix completion and recovery,” in
Asian Conference on Computer Vision, 2010.
[4] R. Basri and D. Jacobs, “Lambertian reﬂectance and linear subspaces,”
IEEE Tr. PAMI, vol. 25, no. 2, pp. 218–233, 2003.
[5] A. Ganesh, Y. Peng, W. Xu, J. Wright, and Y. Ma, “RASL: Robust
alignment via sparse and low-rank decomposition,” IEEE Transactions
on Pattern Analysis and Machine Intelligence (PAMI), 2011.
[6] X. Shu and N. Ahuja, “Imaging via three-dimensional compressive
sampling (3DCS),” in International Conference on Computer Vision
ICCV, 2011.
[7] Z. Zhang, A. Ganesh, X. Liang, and Y. Ma, “TILT: Transform-invariant
low-rank textures,” Intl. Journal on Computer Vision (IJCV), 2011.
[8] A. Waters, A. Sankaranarayanan, and R. Baraniuk, “Sparcs: Recovering
low-rank and sparse matrices from compressive measurements,” in Proc.
Neural Information Processing Systems (NIPS), 2011.
[9] M. Golbabaee and P. Vandergheynst, “Hyperspectral image compressed
sensing via low-rank and joint sparse matrix recovery,” in Indernational
Conference on Acoustics, Speech and Signal Processing ICASSP, 2011.
[10] D. Donoho, “Compressive sensing,” IEEE Transactions on Information
Theory, vol. 52, no. 4, pp. 1289–1306, 2006.
[11] X. Li, “Compressed sensing and matrix completion with constant
constant proportion of corruptions,” 2011, available at http://arxiv.org/
abs/1104.1041.
[12] H. Xu, S. Sanghavi, and C. Caramanis, “Robust PCA via outlier pursuit,”
IEEE Transactions on Information Theory, 2011.
[13] M. McCoy and J. Tropp, “Two proposals for robust PCA using semidefinite programming,” Electronic Journal of Statistics, 2011.
[14] J. Bobin, Y. M. J. L. Starck, and M. Elad, “Sparsity and morphological
diversity and source separation,” IEEE Transactions on Image Processing, vol. 16, no. 11, pp. 2662–2674, 2007.
[15] E. Cand` s and B. Recht, “Exact matrix completion via convex optie
mzation,” Foundations of Computational Mathematics, vol. 9, no. 6, pp.
717–772, 2008.
[16] Z. Zhou, X. Li, J. Wright, E. Candes, and Y. Ma, “Stable principal
component pursuit,” in Proc. of International Symposium on Information
Theory, 2010.
[17] A. Ganesh, X. Li, J. Wright, E. Candes, and Y. Ma, “Dense error
correction for low-rank matrices via principal component pursuit,” in
Proc. of International Symposium on Information Theory, 2010.
[18] A. Agarwal, S. Negahban, and M. J. Wainwright, “Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions,”
preprint, 2011.
[19] D. Hsu, S. Kakade, and T. Zhang, “Robust matrix decomposition with
outliers,” IEEE Transactions on Information Theory, vol. 57, pp. 7221–
7234, 2011.
[20] D. Needell and J. Tropp, “CoSaMP: Iterative signal recovery from
incomplete and inaccurate samples,” Appl. Comp. Harmonic Anal.,
vol. 26, pp. 301–321, 2008.
[21] S. Negahban, P. Ravikumar, M. Wainwright, and B. Yu, “A uniﬁed
framework for analyzing m-estimators with decomposible regularizers,”
2010, available at http://arxiv.org/abs/1010.2731v1.
[22] V. Chandrasekaran, B. Recht, P. Parillo, and A. Wilsky, “The convex
geometry of linear inverse problems,” preprint, 2010.
[23] E. Cand` s and B. Recht, “Simple bounds for low-complexity model
e
reconstruction,” preprint, 2011.
[24] F. Bach, “Structured sparsity-inducing norms through submodular functions,” preprint, 2010.
[25] D. Gross, “Recovering low-rank matrices from few coefﬁcients in any
basis,” IEEE Transactions on Information Theory, vol. 57, no. 3, 2011.
[26] J. Wright, A. Ganesh, K. Min, and Y. Ma, “Compressive principal
component pursuit,” preprint, 2012.

